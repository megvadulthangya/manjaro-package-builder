
--- FILE: .github/scripts/checker.py ---
#!/usr/bin/env python3

import os
import sys
import py_compile
import yaml

def find_files_by_extension(root_dir, extensions):
    """Find all files with given extensions recursively"""
    matched_files = []
    for root, dirs, files in os.walk(root_dir):
        for file in files:
            if any(file.endswith(ext) for ext in extensions):
                matched_files.append(os.path.join(root, file))
    return matched_files

def check_python_file(file_path):
    """Check Python file syntax using py_compile"""
    try:
        py_compile.compile(file_path, doraise=True)
        print(f"[PASS] Python syntax: {file_path}")
        return True
    except py_compile.PyCompileError as e:
        print(f"[FAIL] Python syntax: {file_path} - {e}")
        return False
    except FileNotFoundError:
        print(f"[FAIL] Python syntax: {file_path} - File not found")
        return False
    except Exception as e:
        print(f"[FAIL] Python syntax: {file_path} - Unexpected error: {e}")
        return False

def check_yaml_file(file_path):
    """Basic YAML syntax check"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            yaml.safe_load(f)
        print(f"[PASS] YAML syntax: {file_path}")
        return True
    except yaml.YAMLError as e:
        print(f"[FAIL] YAML syntax: {file_path} - {e}")
        return False
    except FileNotFoundError:
        print(f"[FAIL] YAML syntax: {file_path} - File not found")
        return False
    except Exception as e:
        print(f"[FAIL] YAML syntax: {file_path} - Unexpected error: {e}")
        return False

def check_env_vars(vars_list):
    """Check that environment variables are not empty"""
    all_passed = True
    for var in vars_list:
        value = os.getenv(var, '')
        if value and value.strip():
            print(f"[PASS] ENV variable: {var}")
        else:
            print(f"[FAIL] ENV variable: {var} - Empty or not set")
            all_passed = False
    return all_passed

def main():
    print("=== Running Preflight Checker ===")
    
    # Track overall status
    all_checks_passed = True
    
    # Check Python files in .github/scripts/ and subdirectories
    scripts_dir = '.github/scripts'
    if os.path.exists(scripts_dir):
        python_files = find_files_by_extension(scripts_dir, ['.py'])
        
        if python_files:
            print(f"\nChecking {len(python_files)} Python file(s) in '{scripts_dir}' and subdirectories:")
            for py_file in python_files:
                if not check_python_file(py_file):
                    all_checks_passed = False
        else:
            print(f"[INFO] No Python files found in '{scripts_dir}'")
    else:
        print(f"[WARNING] Directory '{scripts_dir}' does not exist")
    
    # Check YAML files in .github/workflows/
    workflows_dir = '.github/workflows'
    if os.path.exists(workflows_dir):
        yaml_files = find_files_by_extension(workflows_dir, ['.yaml', '.yml', '.bckp'])
        
        if yaml_files:
            print(f"\nChecking {len(yaml_files)} YAML file(s) in '{workflows_dir}' and subdirectories:")
            for yaml_file in yaml_files:
                if not check_yaml_file(yaml_file):
                    all_checks_passed = False
        else:
            print(f"[INFO] No YAML files found in '{workflows_dir}'")
    else:
        print(f"[WARNING] Directory '{workflows_dir}' does not exist")
    
    # Check required environment variables
    print("\nChecking environment variables:")
    required_vars = ['VPS_USER', 'VPS_HOST', 'VPS_SSH_KEY', 'REPO_SERVER_URL']
    if not check_env_vars(required_vars):
        all_checks_passed = False
    
    print("\n" + "=" * 30)
    
    if all_checks_passed:
        print("âœ… All preflight checks passed")
        sys.exit(0)
    else:
        print("âŒ One or more preflight checks failed")
        sys.exit(1)

if __name__ == '__main__':
    main()
--- FILE: .github/scripts/modules/__init__.py ---
"""
Package Builder Modules
"""

# This file makes the modules directory a Python package
--- FILE: .github/scripts/modules/common/__init__.py ---
"""
Common utilities for the package builder system
"""

from .logging_utils import setup_logging, get_logger
from .shell_executor import ShellExecutor
from .environment import EnvironmentValidator
from .config_loader import ConfigLoader

__all__ = [
    'setup_logging',
    'get_logger',
    'ShellExecutor',
    'EnvironmentValidator',
    'ConfigLoader'
]
--- FILE: .github/scripts/modules/common/logging_utils.py ---
"""
Logging utilities for the package builder system
Handles logging configuration with support for debug mode
"""

import logging
import os
from typing import Optional


def setup_logging(debug_mode: bool = False, log_file: str = 'builder.log') -> None:
    """
    Configure logging for the application
    
    Args:
        debug_mode: If True, sets log level to DEBUG and adds more verbose output
        log_file: Path to log file
    """
    # Determine log level
    log_level = logging.DEBUG if debug_mode else logging.INFO
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    
    # Remove existing handlers to avoid duplication
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Create formatters
    console_formatter = logging.Formatter(
        '[%(asctime)s] %(levelname)s: %(message)s',
        datefmt='%H:%M:%S'
    )
    
    file_formatter = logging.Formatter(
        '[%(asctime)s] %(levelname)s - %(name)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler
    try:
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO if debug_mode else logging.WARNING)
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    except (IOError, PermissionError) as e:
        # Log but don't fail if we can't create log file
        root_logger.warning(f"Could not create log file {log_file}: {e}")
    
    # Suppress overly verbose logs from dependencies
    logging.getLogger('paramiko').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('git').setLevel(logging.WARNING)


def get_logger(name: Optional[str] = None) -> logging.Logger:
    """
    Get a logger instance
    
    Args:
        name: Logger name (usually __name__ of the calling module)
    
    Returns:
        Configured logger instance
    """
    return logging.getLogger(name)
--- FILE: .github/scripts/modules/common/shell_executor.py ---
"""
Shell command execution with comprehensive logging, timeout, and debug mode support
Extracted from PackageBuilder._run_cmd with enhanced features
"""

import os
import subprocess
import logging
from pathlib import Path
from typing import Dict, List, Optional, Union, Any


class ShellExecutor:
    """
    Executes shell commands with comprehensive logging, timeout handling,
    and optional debug mode for CI/CD output
    """
    
    def __init__(self, debug_mode: bool = False, default_timeout: int = 1800):
        """
        Initialize ShellExecutor
        
        Args:
            debug_mode: If True, bypass logger and print directly to stdout for CI/CD visibility
            default_timeout: Default command timeout in seconds
        """
        self.debug_mode = debug_mode
        self.default_timeout = default_timeout
        self.logger = logging.getLogger(__name__)
    
    def run(
        self,
        cmd: Union[str, List[str]],
        cwd: Optional[Union[str, Path]] = None,
        capture: bool = True,
        check: bool = True,
        shell: bool = True,
        user: Optional[str] = None,
        log_cmd: bool = False,
        timeout: Optional[int] = None,
        extra_env: Optional[Dict[str, str]] = None,
        **kwargs  # Accept any additional kwargs like 'text', 'capture_output', etc.
    ) -> subprocess.CompletedProcess:
        """
        Run command with comprehensive logging and timeout
        
        Args:
            cmd: Command to execute (string or list)
            cwd: Working directory
            capture: Capture stdout/stderr (alias for capture_output)
            check: Raise CalledProcessError on non-zero exit code
            shell: Use shell execution
            user: Run as specified user (requires sudo)
            log_cmd: Log command details
            timeout: Command timeout in seconds (defaults to self.default_timeout)
            extra_env: Additional environment variables
            **kwargs: Additional arguments passed to subprocess.run (text, capture_output, etc.)
        
        Returns:
            subprocess.CompletedProcess with decoded string output
        
        Raises:
            subprocess.TimeoutExpired: Command timed out
            subprocess.CalledProcessError: Command failed and check=True
        """
        if timeout is None:
            timeout = self.default_timeout
        
        # Convert cmd to string if it's a list (for logging)
        if isinstance(cmd, list):
            cmd_str = ' '.join(cmd)
            # For list commands, use shell=False unless explicitly overridden
            if 'shell' not in kwargs:
                shell = False
        else:
            cmd_str = cmd
            # For string commands, use shell=True by default
            if 'shell' not in kwargs:
                shell = True
        
        # Log command if requested
        if log_cmd or self.debug_mode:
            self._log_command(cmd_str, log_cmd)
        
        # Prepare working directory
        cwd_path = Path(cwd) if cwd else Path.cwd()
        
        # Prepare environment
        env = os.environ.copy()
        env['LC_ALL'] = 'C'  # Ensure consistent locale for command output
        
        if extra_env:
            env.update(extra_env)
        
        # Handle capture_output parameter (Python 3.7+)
        subprocess_kwargs = {
            'cwd': cwd_path,
            'shell': shell,
            'check': check,
            'env': env,
            'timeout': timeout,
            'text': True,  # Always return strings, not bytes
            'encoding': 'utf-8',
            'errors': 'ignore'
        }
        
        # Add any additional kwargs passed by caller
        subprocess_kwargs.update(kwargs)
        
        # Handle capture parameter (map to capture_output)
        if 'capture_output' not in subprocess_kwargs:
            subprocess_kwargs['capture_output'] = capture
        
        # Ensure we return text, not bytes
        if 'text' not in subprocess_kwargs or subprocess_kwargs.get('text') is False:
            subprocess_kwargs['text'] = True
            subprocess_kwargs['encoding'] = 'utf-8'
            subprocess_kwargs['errors'] = 'ignore'
        
        # Prepare command based on user
        if user:
            return self._run_as_user(cmd, user, subprocess_kwargs, log_cmd)
        else:
            return self._run_direct(cmd, subprocess_kwargs, log_cmd)
    
    def _log_command(self, cmd: str, log_cmd: bool) -> None:
        """Log command execution details"""
        if self.debug_mode:
            print(f"ðŸ”§ [DEBUG] RUNNING COMMAND: {cmd}", flush=True)
        elif log_cmd:
            self.logger.info(f"RUNNING COMMAND: {cmd}")
    
    def _log_output(self, result: subprocess.CompletedProcess, log_cmd: bool) -> None:
        """Log command output based on debug mode"""
        # Always decode to string if needed
        stdout = self._ensure_string(result.stdout)
        stderr = self._ensure_string(result.stderr)
        
        if self.debug_mode:
            if stdout:
                print(f"ðŸ”§ [DEBUG] STDOUT:\n{stdout}", flush=True)
            
            if stderr:
                print(f"ðŸ”§ [DEBUG] STDERR:\n{stderr}", flush=True)
            
            print(f"ðŸ”§ [DEBUG] EXIT CODE: {result.returncode}", flush=True)
        elif log_cmd:
            if stdout:
                self.logger.info(f"STDOUT: {stdout[:500]}")
            
            if stderr:
                self.logger.info(f"STDERR: {stderr[:500]}")
            
            self.logger.info(f"EXIT CODE: {result.returncode}")
        
        # Critical: If command failed and we're in debug mode, print full output
        if result.returncode != 0 and self.debug_mode:
            cmd_str = result.cmd if hasattr(result, 'cmd') else 'unknown'
            print(f"âŒ [DEBUG] COMMAND FAILED: {cmd_str}", flush=True)
            
            if stdout and len(stdout) > 500:
                print(f"âŒ [DEBUG] FULL STDOUT (truncated):\n{stdout[:2000]}", flush=True)
            
            if stderr and len(stderr) > 500:
                print(f"âŒ [DEBUG] FULL STDERR (truncated):\n{stderr[:2000]}", flush=True)
    
    def _ensure_string(self, value: Any) -> str:
        """Ensure value is a string, decoding bytes if necessary"""
        if value is None:
            return ""
        elif isinstance(value, str):
            # Clean string: strip whitespace and ensure no binary data
            cleaned = value.strip()
            # Remove any null bytes or control characters that might indicate binary data
            cleaned = ''.join(char for char in cleaned if ord(char) >= 32 or char == '\n' or char == '\t')
            return cleaned
        elif isinstance(value, bytes):
            try:
                decoded = value.decode('utf-8', errors='ignore')
                return decoded.strip()
            except:
                # If decoding fails, return safe string representation
                return str(value)[:1000].strip()
        else:
            return str(value).strip()
    
    def _run_as_user(
        self,
        cmd: Union[str, List[str]],
        user: str,
        subprocess_kwargs: Dict[str, Any],
        log_cmd: bool
    ) -> subprocess.CompletedProcess:
        """Run command as specified user"""
        # Set up environment for the user
        env = subprocess_kwargs.get('env', os.environ.copy())
        env['HOME'] = f'/home/{user}'
        env['USER'] = user
        subprocess_kwargs['env'] = env
        
        try:
            # Build sudo command
            shell = subprocess_kwargs.get('shell', True)
            
            if isinstance(cmd, list):
                # For list commands, build sudo command with list
                sudo_cmd = ['sudo', '-u', user] + cmd
                subprocess_kwargs['shell'] = False
            else:
                # For string commands, use shell execution
                sudo_cmd = ['sudo', '-u', user, 'bash', '-c', cmd]
                subprocess_kwargs['shell'] = True
            
            result = subprocess.run(sudo_cmd, **subprocess_kwargs)
            
            # Ensure stdout/stderr are clean strings
            if hasattr(result, 'stdout') and result.stdout is not None:
                result.stdout = self._ensure_string(result.stdout)
            
            if hasattr(result, 'stderr') and result.stderr is not None:
                result.stderr = self._ensure_string(result.stderr)
            
            if log_cmd or self.debug_mode:
                self._log_output(result, log_cmd)
            
            return result
            
        except subprocess.TimeoutExpired as e:
            error_msg = f"âš ï¸ Command timed out after {subprocess_kwargs.get('timeout', 1800)} seconds: {cmd}"
            if self.debug_mode:
                print(f"âŒ [DEBUG] {error_msg}", flush=True)
            self.logger.error(error_msg)
            raise
        except subprocess.CalledProcessError as e:
            # Ensure stdout/stderr are clean strings for the exception
            if hasattr(e, 'stdout') and e.stdout is not None:
                e.stdout = self._ensure_string(e.stdout)
            
            if hasattr(e, 'stderr') and e.stderr is not None:
                e.stderr = self._ensure_string(e.stderr)
            
            if log_cmd or self.debug_mode:
                error_msg = f"Command failed: {cmd}"
                if self.debug_mode:
                    print(f"âŒ [DEBUG] {error_msg}", flush=True)
                    if hasattr(e, 'stdout') and e.stdout:
                        print(f"âŒ [DEBUG] EXCEPTION STDOUT:\n{e.stdout}", flush=True)
                    if hasattr(e, 'stderr') and e.stderr:
                        print(f"âŒ [DEBUG] EXCEPTION STDERR:\n{e.stderr}", flush=True)
                else:
                    self.logger.error(error_msg)
            
            if subprocess_kwargs.get('check', True):
                raise
            
            # Create a CompletedProcess with decoded strings
            return subprocess.CompletedProcess(
                args=[],
                returncode=e.returncode,
                stdout=self._ensure_string(getattr(e, 'stdout', '')),
                stderr=self._ensure_string(getattr(e, 'stderr', ''))
            )
    
    def _run_direct(
        self,
        cmd: Union[str, List[str]],
        subprocess_kwargs: Dict[str, Any],
        log_cmd: bool
    ) -> subprocess.CompletedProcess:
        """Run command directly (no user switch)"""
        try:
            result = subprocess.run(cmd, **subprocess_kwargs)
            
            # Ensure stdout/stderr are clean strings
            if hasattr(result, 'stdout') and result.stdout is not None:
                result.stdout = self._ensure_string(result.stdout)
            
            if hasattr(result, 'stderr') and result.stderr is not None:
                result.stderr = self._ensure_string(result.stderr)
            
            if log_cmd or self.debug_mode:
                self._log_output(result, log_cmd)
            
            return result
            
        except subprocess.TimeoutExpired as e:
            error_msg = f"âš ï¸ Command timed out after {subprocess_kwargs.get('timeout', 1800)} seconds: {cmd}"
            if self.debug_mode:
                print(f"âŒ [DEBUG] {error_msg}", flush=True)
            self.logger.error(error_msg)
            raise
        except subprocess.CalledProcessError as e:
            # Ensure stdout/stderr are clean strings for the exception
            if hasattr(e, 'stdout') and e.stdout is not None:
                e.stdout = self._ensure_string(e.stdout)
            
            if hasattr(e, 'stderr') and e.stderr is not None:
                e.stderr = self._ensure_string(e.stderr)
            
            if log_cmd or self.debug_mode:
                error_msg = f"Command failed: {cmd}"
                if self.debug_mode:
                    print(f"âŒ [DEBUG] {error_msg}", flush=True)
                    if hasattr(e, 'stdout') and e.stdout:
                        print(f"âŒ [DEBUG] EXCEPTION STDOUT:\n{e.stdout}", flush=True)
                    if hasattr(e, 'stderr') and e.stderr:
                        print(f"âŒ [DEBUG] EXCEPTION STDERR:\n{e.stderr}", flush=True)
                else:
                    self.logger.error(error_msg)
            
            if subprocess_kwargs.get('check', True):
                raise
            
            # Create a CompletedProcess with decoded strings
            return subprocess.CompletedProcess(
                args=[],
                returncode=e.returncode,
                stdout=self._ensure_string(getattr(e, 'stdout', '')),
                stderr=self._ensure_string(getattr(e, 'stderr', ''))
            )
    
    def simple_run(self, cmd: str, check: bool = True, **kwargs) -> subprocess.CompletedProcess:
        """
        Simplified run command for common use cases
        
        Args:
            cmd: Command to execute
            check: Raise on error
            **kwargs: Additional arguments passed to run()
        
        Returns:
            subprocess.CompletedProcess
        """
        return self.run(cmd, check=check, log_cmd=False, **kwargs)
--- FILE: .github/scripts/modules/common/environment.py ---
"""
Environment validation and system path utilities
Extracted from PackageBuilder._validate_env and _get_repo_root
"""

import os
import re
import sys
import logging
from pathlib import Path
from typing import List, Tuple, Optional


class EnvironmentValidator:
    """Validates environment and provides system path utilities"""
    
    # Required environment variables
    REQUIRED_VARS = [
        'REPO_NAME',
        'VPS_HOST',
        'VPS_USER',
        'VPS_SSH_KEY',
        'REMOTE_DIR',
    ]
    
    # Optional but recommended variables
    OPTIONAL_VARS = [
        'REPO_SERVER_URL',
        'GPG_KEY_ID',
        'GPG_PRIVATE_KEY',
        'PACKAGER_ENV',
    ]
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        Initialize EnvironmentValidator
        
        Args:
            logger: Optional logger instance (creates one if not provided)
        """
        self.logger = logger or logging.getLogger(__name__)
    
    def validate(self) -> bool:
        """
        Comprehensive pre-flight environment validation
        
        Returns:
            True if validation passed, False otherwise
        
        Note:
            Exits with sys.exit(1) on critical failures
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("PRE-FLIGHT ENVIRONMENT VALIDATION")
        self.logger.info("=" * 60)
        
        # Check required variables
        missing_vars = []
        for var in self.REQUIRED_VARS:
            value = os.getenv(var)
            if not value or value.strip() == '':
                missing_vars.append(var)
                self.logger.error(f"[ERROR] Variable {var} is empty! Ensure it is set in GitHub Secrets.")
        
        if missing_vars:
            self.logger.error(f"Missing required variables: {', '.join(missing_vars)}")
            sys.exit(1)
        
        # Check optional variables and warn if missing
        for var in self.OPTIONAL_VARS:
            value = os.getenv(var)
            if not value or value.strip() == '':
                self.logger.warning(f"âš ï¸ Optional variable {var} is empty")
        
        # âœ… SECURITY FIX: DO NOT display secret information!
        self.logger.info("âœ… Environment validation passed:")
        for var in self.REQUIRED_VARS + self.OPTIONAL_VARS:
            value = os.getenv(var)
            if value and value.strip() != '':
                self.logger.info(f"   {var}: [LOADED]")
            else:
                self.logger.info(f"   {var}: [MISSING]")
        
        # Validate REPO_NAME for pacman.conf
        repo_name = os.getenv('REPO_NAME')
        if repo_name:
            if not re.match(r'^[a-zA-Z0-9_-]+$', repo_name):
                self.logger.error(f"[ERROR] Invalid REPO_NAME '{repo_name}'. Must contain only letters, numbers, hyphens, and underscores.")
                sys.exit(1)
            if len(repo_name) > 50:
                self.logger.error(f"[ERROR] REPO_NAME '{repo_name}' is too long (max 50 characters).")
                sys.exit(1)
        
        return True
    
    def get_repo_root(self) -> Path:
        """
        Get the repository root directory reliably
        
        Returns:
            Path to repository root
        """
        # Check GITHUB_WORKSPACE first (GitHub Actions)
        github_workspace = os.getenv('GITHUB_WORKSPACE')
        if github_workspace:
            workspace_path = Path(github_workspace)
            if workspace_path.exists():
                self.logger.info(f"Using GITHUB_WORKSPACE: {workspace_path}")
                return workspace_path
        
        # Check container workspace (Docker/container specific)
        container_workspace = Path('/__w/manjaro-awesome/manjaro-awesome')
        if container_workspace.exists():
            self.logger.info(f"Using container workspace: {container_workspace}")
            return container_workspace
        
        # Get script directory and go up to repo root
        script_path = Path(__file__).resolve()
        
        # Navigate up: modules/common/environment.py -> modules/common -> modules -> scripts -> .github -> repo root
        # That's 6 levels up from this file
        potential_roots = [
            script_path.parent.parent.parent.parent.parent.parent,  # modules/common -> repo root
            script_path.parent.parent.parent.parent.parent,          # Alternative path
            Path.cwd(),                                              # Current directory
        ]
        
        for repo_root in potential_roots:
            if repo_root.exists():
                # Check for typical repository markers
                if (repo_root / '.github').exists() or (repo_root / 'README.md').exists():
                    self.logger.info(f"Using repository root: {repo_root}")
                    return repo_root
        
        # Fallback to current directory
        current_dir = Path.cwd()
        self.logger.info(f"Using current directory: {current_dir}")
        return current_dir
    
    def get_required_env(self, var_name: str, default: Optional[str] = None) -> str:
        """
        Get required environment variable
        
        Args:
            var_name: Environment variable name
            default: Default value (if not required)
        
        Returns:
            Environment variable value
        
        Raises:
            ValueError: If variable is not set and no default provided
        """
        value = os.getenv(var_name)
        if value is None or value.strip() == '':
            if default is not None:
                return default
            raise ValueError(f"Required environment variable {var_name} is not set")
        return value
    
    def get_optional_env(self, var_name: str, default: str = '') -> str:
        """
        Get optional environment variable
        
        Args:
            var_name: Environment variable name
            default: Default value if not set
        
        Returns:
            Environment variable value or default
        """
        value = os.getenv(var_name)
        if value is None or value.strip() == '':
            return default
        return value
    
    def validate_repo_name(self, repo_name: str) -> bool:
        """
        Validate repository name format
        
        Args:
            repo_name: Repository name to validate
        
        Returns:
            True if valid, False otherwise
        """
        if not repo_name:
            return False
        
        # Check format
        if not re.match(r'^[a-zA-Z0-9_-]+$', repo_name):
            self.logger.error(f"Invalid REPO_NAME '{repo_name}'. Must contain only letters, numbers, hyphens, and underscores.")
            return False
        
        # Check length
        if len(repo_name) > 50:
            self.logger.error(f"REPO_NAME '{repo_name}' is too long (max 50 characters).")
            return False
        
        return True
    
    def check_environment(self) -> Tuple[bool, List[str]]:
        """
        Check environment without exiting
        
        Returns:
            Tuple of (is_valid, missing_vars)
        """
        missing_vars = []
        
        for var in self.REQUIRED_VARS:
            value = os.getenv(var)
            if not value or value.strip() == '':
                missing_vars.append(var)
        
        return (len(missing_vars) == 0, missing_vars)
--- FILE: .github/scripts/modules/common/config_loader.py ---
"""
Configuration loading from environment, config files, and package definitions
Extracted from PackageBuilder._load_config and get_package_lists
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging


class ConfigLoader:
    """
    Loads configuration from environment variables, config files,
    and package definitions
    """
    
    def __init__(self, repo_root: Path, logger: Optional[logging.Logger] = None):
        """
        Initialize ConfigLoader
        
        Args:
            repo_root: Repository root directory
            logger: Optional logger instance
        """
        self.repo_root = repo_root
        self.logger = logger or logging.getLogger(__name__)
        
        # Initialize config state
        self._config_cache: Dict[str, Any] = {}
        self._packages_cache: Optional[Tuple[List[str], List[str]]] = None
        
        # Try to import config files
        self.has_config_files = self._try_import_config()
    
    def _try_import_config(self) -> bool:
        """Try to import config and packages modules"""
        try:
            # Add the script directory to sys.path for imports
            script_dir = self.repo_root / ".github" / "scripts"
            if script_dir.exists():
                sys.path.insert(0, str(script_dir))
            
            # Try to import config
            try:
                import config
                self._config_cache['config_module'] = config
                self.logger.debug("âœ… Config module imported")
            except ImportError:
                self.logger.warning("âš ï¸ Could not import config module")
                self._config_cache['config_module'] = None
            
            # Try to import packages
            try:
                import packages
                self._config_cache['packages_module'] = packages
                self.logger.debug("âœ… Packages module imported")
            except ImportError:
                self.logger.warning("âš ï¸ Could not import packages module")
                self._config_cache['packages_module'] = None
            
            return bool(self._config_cache.get('config_module'))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error importing config files: {e}")
            return False
    
    def load_config(self) -> Dict[str, Any]:
        """
        Load configuration from environment and config files
        
        Returns:
            Dictionary with configuration values
        """
        config = {}
        
        # Required environment variables (secrets)
        config['vps_user'] = os.getenv('VPS_USER', '')
        config['vps_host'] = os.getenv('VPS_HOST', '')
        config['ssh_key'] = os.getenv('VPS_SSH_KEY', '')
        
        # Optional environment variables (overrides)
        config['repo_server_url'] = os.getenv('REPO_SERVER_URL', '')
        config['remote_dir'] = os.getenv('REMOTE_DIR', '')
        
        # Repository name from environment
        config['repo_name'] = os.getenv('REPO_NAME', '')
        
        # Load from config.py if available
        if self.has_config_files and self._config_cache.get('config_module'):
            config_module = self._config_cache['config_module']
            
            # Directories
            config['output_dir'] = getattr(config_module, 'OUTPUT_DIR', 'built_packages')
            config['build_tracking_dir'] = getattr(config_module, 'BUILD_TRACKING_DIR', '.build_tracking')
            config['mirror_temp_dir'] = Path(getattr(config_module, 'MIRROR_TEMP_DIR', '/tmp/repo_mirror'))
            config['sync_clone_dir'] = Path(getattr(config_module, 'SYNC_CLONE_DIR', '/tmp/manjaro-awesome-gitclone'))
            config['aur_build_dir'] = self.repo_root / getattr(config_module, 'AUR_BUILD_DIR', 'build_aur')
            
            # AUR and SSH
            config['aur_urls'] = getattr(config_module, 'AUR_URLS', [
                "https://aur.archlinux.org/{pkg_name}.git",
                "git://aur.archlinux.org/{pkg_name}.git"
            ])
            config['ssh_options'] = getattr(config_module, 'SSH_OPTIONS', [
                "-o", "StrictHostKeyChecking=no",
                "-o", "ConnectTimeout=30",
                "-o", "BatchMode=yes"
            ])
            
            # GitHub and packager
            config['github_repo'] = os.getenv('GITHUB_REPO', 
                                            getattr(config_module, 'GITHUB_REPO', 'megvadulthangya/manjaro-awesome.git'))
            config['packager_id'] = getattr(config_module, 'PACKAGER_ID', 'Maintainer <no-reply@gshoots.hu>')
            
            # Debug mode
            config['debug_mode'] = getattr(config_module, 'DEBUG_MODE', False)
        else:
            # Default values if config.py not available
            config['output_dir'] = 'built_packages'
            config['build_tracking_dir'] = '.build_tracking'
            config['mirror_temp_dir'] = Path('/tmp/repo_mirror')
            config['sync_clone_dir'] = Path('/tmp/manjaro-awesome-gitclone')
            config['aur_build_dir'] = self.repo_root / 'build_aur'
            config['aur_urls'] = [
                "https://aur.archlinux.org/{pkg_name}.git",
                "git://aur.archlinux.org/{pkg_name}.git"
            ]
            config['ssh_options'] = [
                "-o", "StrictHostKeyChecking=no",
                "-o", "ConnectTimeout=30",
                "-o", "BatchMode=yes"
            ]
            config['github_repo'] = os.getenv('GITHUB_REPO', 'megvadulthangya/manjaro-awesome.git')
            config['packager_id'] = 'Maintainer <no-reply@gshoots.hu>'
            config['debug_mode'] = False
        
        # Convert string paths to Path objects
        config['output_dir'] = self.repo_root / config['output_dir']
        config['build_tracking_dir'] = self.repo_root / config['build_tracking_dir']
        
        # Log configuration (without secrets)
        self.logger.info("ðŸ”§ Configuration loaded:")
        self.logger.info(f"   SSH user: {config['vps_user']}")
        self.logger.info(f"   VPS host: {config['vps_host']}")
        self.logger.info(f"   Remote directory: {config['remote_dir']}")
        self.logger.info(f"   Repository name: {config['repo_name']}")
        if config['repo_server_url']:
            self.logger.info(f"   Repository URL: {config['repo_server_url']}")
        self.logger.info(f"   Config files loaded: {self.has_config_files}")
        self.logger.info(f"   Debug mode: {config['debug_mode']}")
        
        return config
    
    def get_package_lists(self) -> Tuple[List[str], List[str]]:
        """
        Get package lists from packages.py or exit if not available
        
        Returns:
            Tuple of (local_packages_list, aur_packages_list)
        
        Raises:
            SystemExit: If package lists cannot be loaded
        """
        if self._packages_cache is not None:
            return self._packages_cache
        
        packages_module = self._config_cache.get('packages_module')
        
        if packages_module and hasattr(packages_module, 'LOCAL_PACKAGES') and hasattr(packages_module, 'AUR_PACKAGES'):
            self.logger.info("ðŸ“¦ Using package lists from packages.py")
            local_packages_list = packages_module.LOCAL_PACKAGES
            aur_packages_list = packages_module.AUR_PACKAGES
            
            total_packages = len(local_packages_list) + len(aur_packages_list)
            self.logger.debug(f">>> DEBUG: Found {total_packages} packages to check")
            
            self._packages_cache = (local_packages_list, aur_packages_list)
            return self._packages_cache
        else:
            self.logger.error("âŒ Cannot load package lists from packages.py")
            self.logger.error("Please ensure packages.py exists and contains LOCAL_PACKAGES and AUR_PACKAGES lists")
            sys.exit(1)
    
    def get_packager_id(self) -> str:
        """
        Get PACKAGER_ID from config or environment
        
        Returns:
            PACKAGER_ID string
        """
        # Try environment variable first
        packager_env = os.getenv('PACKAGER_ENV')
        if packager_env:
            return packager_env
        
        # Fall back to config
        if self.has_config_files and self._config_cache.get('config_module'):
            config_module = self._config_cache['config_module']
            return getattr(config_module, 'PACKAGER_ID', 'Maintainer <no-reply@gshoots.hu>')
        
        return 'Maintainer <no-reply@gshoots.hu>'
    
    def get_config_value(self, key: str, default: Any = None) -> Any:
        """
        Get a specific configuration value
        
        Args:
            key: Configuration key
            default: Default value if key not found
        
        Returns:
            Configuration value or default
        """
        # Try config module first
        if self.has_config_files and self._config_cache.get('config_module'):
            config_module = self._config_cache['config_module']
            if hasattr(config_module, key):
                return getattr(config_module, key)
        
        # Fall back to environment
        env_value = os.getenv(key.upper())
        if env_value is not None:
            return env_value
        
        return default
--- FILE: .github/scripts/modules/build/__init__.py ---
"""
Package building modules
"""

# Placeholder for build modules - will be implemented in Phase 2
--- FILE: .github/scripts/modules/build/version_manager.py ---
"""
Version management for package building
Handles version extraction, comparison, and SRCINFO parsing
Extracted from build_engine.py with enhanced functionality
"""

import re
import subprocess
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any


class VersionManager:
    """Manages package version extraction, comparison, and parsing"""
    
    def __init__(self, shell_executor, logger: Optional[logging.Logger] = None):
        """
        Initialize VersionManager
        
        Args:
            shell_executor: ShellExecutor instance for command execution
            logger: Optional logger instance
        """
        self.shell_executor = shell_executor
        self.logger = logger or logging.getLogger(__name__)
    
    def extract_version_from_srcinfo(self, pkg_dir: Path) -> Tuple[str, str, Optional[str]]:
        """Extract pkgver, pkgrel, and epoch from .SRCINFO or makepkg --printsrcinfo output"""
        srcinfo_path = pkg_dir / ".SRCINFO"
        
        # First try to read existing .SRCINFO
        if srcinfo_path.exists():
            try:
                with open(srcinfo_path, 'r') as f:
                    srcinfo_content = f.read()
                return self._parse_srcinfo_content(srcinfo_content)
            except Exception as e:
                self.logger.warning(f"Failed to parse existing .SRCINFO: {e}")
        
        # Generate .SRCINFO using makepkg --printsrcinfo
        try:
            result = self.shell_executor.run(
                ['makepkg', '--printsrcinfo'],
                cwd=pkg_dir,
                capture=True,
                text=True,
                check=False,
                log_cmd=False
            )
            
            if result.returncode == 0 and result.stdout:
                # Also write to .SRCINFO for future use
                with open(srcinfo_path, 'w') as f:
                    f.write(result.stdout)
                return self._parse_srcinfo_content(result.stdout)
            else:
                self.logger.warning(f"makepkg --printsrcinfo failed: {result.stderr}")
                raise RuntimeError(f"Failed to generate .SRCINFO: {result.stderr}")
                
        except Exception as e:
            self.logger.error(f"Error running makepkg --printsrcinfo: {e}")
            raise
    
    def _parse_srcinfo_content(self, srcinfo_content: str) -> Tuple[str, str, Optional[str]]:
        """Parse SRCINFO content to extract version information"""
        pkgver = None
        pkgrel = None
        epoch = None
        
        lines = srcinfo_content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Handle key-value pairs
            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip()
                
                if key == 'pkgver':
                    pkgver = value
                elif key == 'pkgrel':
                    pkgrel = value
                elif key == 'epoch':
                    epoch = value
        
        if not pkgver or not pkgrel:
            raise ValueError("Could not extract pkgver and pkgrel from .SRCINFO")
        
        return pkgver, pkgrel, epoch
    
    def get_full_version_string(self, pkgver: str, pkgrel: str, epoch: Optional[str]) -> str:
        """Construct full version string from components"""
        if epoch and epoch != '0':
            return f"{epoch}:{pkgver}-{pkgrel}"
        return f"{pkgver}-{pkgrel}"
    
    def compare_versions(self, remote_version: Optional[str], pkgver: str, pkgrel: str, 
                        epoch: Optional[str]) -> bool:
        """
        Compare versions using vercmp-style logic
        
        Returns:
            True if AUR_VERSION > REMOTE_VERSION (should build), False otherwise
        """
        # If no remote version exists, we should build
        if not remote_version:
            self.logger.debug(f"[DEBUG] Comparing Package: Remote(NONE) vs New({pkgver}-{pkgrel}) -> BUILD TRIGGERED (no remote)")
            return True
        
        # Parse remote version
        remote_epoch = None
        remote_pkgver = None
        remote_pkgrel = None
        
        # Check if remote has epoch
        if ':' in remote_version:
            remote_epoch_str, rest = remote_version.split(':', 1)
            remote_epoch = remote_epoch_str
            if '-' in rest:
                remote_pkgver, remote_pkgrel = rest.split('-', 1)
            else:
                remote_pkgver = rest
                remote_pkgrel = "1"
        else:
            if '-' in remote_version:
                remote_pkgver, remote_pkgrel = remote_version.split('-', 1)
            else:
                remote_pkgver = remote_version
                remote_pkgrel = "1"
        
        # Build version strings for comparison
        new_version_str = f"{epoch or '0'}:{pkgver}-{pkgrel}"
        remote_version_str = f"{remote_epoch or '0'}:{remote_pkgver}-{remote_pkgrel}"
        
        # Use vercmp for proper version comparison
        try:
            result = self.shell_executor.run(
                ['vercmp', new_version_str, remote_version_str], 
                capture=True,
                text=True,
                check=False,
                log_cmd=False
            )
            
            if result.returncode == 0:
                cmp_result = int(result.stdout.strip())
                
                if cmp_result > 0:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({pkgver}-{pkgrel}) -> BUILD TRIGGERED (new version is newer)")
                    return True
                elif cmp_result == 0:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({pkgver}-{pkgrel}) -> SKIP (versions identical)")
                    return False
                else:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({pkgver}-{pkgrel}) -> SKIP (remote version is newer)")
                    return False
            else:
                # Fallback to simple comparison if vercmp fails
                self.logger.warning("vercmp failed, using fallback comparison")
                return self._fallback_version_comparison(remote_version, pkgver, pkgrel, epoch)
                
        except Exception as e:
            self.logger.warning(f"vercmp comparison failed: {e}, using fallback")
            return self._fallback_version_comparison(remote_version, pkgver, pkgrel, epoch)
    
    def _fallback_version_comparison(self, remote_version: str, pkgver: str, pkgrel: str, 
                                   epoch: Optional[str]) -> bool:
        """Fallback version comparison when vercmp is not available"""
        # Parse remote version
        remote_epoch = None
        remote_pkgver = None
        remote_pkgrel = None
        
        if ':' in remote_version:
            remote_epoch_str, rest = remote_version.split(':', 1)
            remote_epoch = remote_epoch_str
            if '-' in rest:
                remote_pkgver, remote_pkgrel = rest.split('-', 1)
            else:
                remote_pkgver = rest
                remote_pkgrel = "1"
        else:
            if '-' in remote_version:
                remote_pkgver, remote_pkgrel = remote_version.split('-', 1)
            else:
                remote_pkgver = remote_version
                remote_pkgrel = "1"
        
        # Compare epochs first
        if epoch != remote_epoch:
            try:
                epoch_int = int(epoch or 0)
                remote_epoch_int = int(remote_epoch or 0)
                if epoch_int > remote_epoch_int:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> BUILD TRIGGERED (epoch {epoch_int} > {remote_epoch_int})")
                    return True
                else:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (epoch {epoch_int} <= {remote_epoch_int})")
                    return False
            except ValueError:
                if epoch != remote_epoch:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (epoch string mismatch)")
                    return False
        
        # Compare pkgver
        if pkgver != remote_pkgver:
            self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> BUILD TRIGGERED (pkgver different)")
            return True
        
        # Compare pkgrel
        try:
            remote_pkgrel_int = int(remote_pkgrel)
            pkgrel_int = int(pkgrel)
            if pkgrel_int > remote_pkgrel_int:
                self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> BUILD TRIGGERED (pkgrel {pkgrel_int} > {remote_pkgrel_int})")
                return True
            else:
                self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (pkgrel {pkgrel_int} <= {remote_pkgrel_int})")
                return False
        except ValueError:
            if pkgrel != remote_pkgrel:
                self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (pkgrel string mismatch)")
                return False
        
        # Versions are identical
        self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (versions identical)")
        return False
    
    def parse_package_filename(self, filename: str) -> Optional[Tuple[str, str]]:
        """Parse package filename to extract name and version"""
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            if len(parts) < 4:
                return None
            
            # Try to find where package name ends
            for i in range(len(parts) - 3, 0, -1):
                potential_name = '-'.join(parts[:i])
                remaining = parts[i:]
                
                if len(remaining) >= 3:
                    # Check for epoch format (e.g., "2-26.1.9-1")
                    if remaining[0].isdigit() and '-' in '-'.join(remaining[1:]):
                        epoch = remaining[0]
                        version_part = remaining[1]
                        release_part = remaining[2]
                        version_str = f"{epoch}:{version_part}-{release_part}"
                        return potential_name, version_str
                    # Standard format (e.g., "26.1.9-1")
                    elif any(c.isdigit() for c in remaining[0]) and remaining[1].isdigit():
                        version_part = remaining[0]
                        release_part = remaining[1]
                        version_str = f"{version_part}-{release_part}"
                        return potential_name, version_str
        
        except Exception as e:
            self.logger.debug(f"Could not parse filename {filename}: {e}")
        
        return None
--- FILE: .github/scripts/modules/build/aur_builder.py ---
"""
AUR package builder with SRCINFO-based version comparison and dependency fallback
Extracted from PackageBuilder._build_aur_package
"""

import os
import shutil
import time
import re
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List

from modules.common.shell_executor import ShellExecutor
from modules.build.version_manager import VersionManager


class AURBuilder:
    """Handles AUR package building with version comparison and dependency resolution"""
    
    def __init__(self, config: Dict[str, Any], shell_executor: ShellExecutor,
                 version_manager: VersionManager, version_tracker,
                 build_state, logger: Optional[logging.Logger] = None):
        """
        Initialize AURBuilder
        
        Args:
            config: Configuration dictionary
            shell_executor: ShellExecutor instance
            version_manager: VersionManager instance
            version_tracker: VersionTracker instance
            build_state: BuildState instance
            logger: Optional logger instance
        """
        self.config = config
        self.shell_executor = shell_executor
        self.version_manager = version_manager
        self.version_tracker = version_tracker
        self.build_state = build_state
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract configuration
        self.repo_root = Path(config.get('repo_root', '.'))
        self.output_dir = Path(config.get('output_dir', 'built_packages'))
        self.aur_build_dir = Path(config.get('aur_build_dir', 'build_aur'))
        self.aur_urls = config.get('aur_urls', [])
        self.packager_id = config.get('packager_id', 'Maintainer <no-reply@gshoots.hu>')
        self.repo_name = config.get('repo_name', '')
    
    def build(self, pkg_name: str, remote_version: Optional[str] = None) -> bool:
        """
        Build AUR package with SRCINFO-based version comparison and dependency fallback
        
        Args:
            pkg_name: Package name
            remote_version: Optional remote version for comparison
        
        Returns:
            True if build successful or skipped (up-to-date), False on failure
        """
        aur_dir = self.aur_build_dir
        aur_dir.mkdir(exist_ok=True)
        
        pkg_dir = aur_dir / pkg_name
        if pkg_dir.exists():
            shutil.rmtree(pkg_dir, ignore_errors=True)
        
        self.logger.info(f"Cloning {pkg_name} from AUR...")
        
        # Try different AUR URLs from config (ALWAYS FRESH CLONE)
        clone_success = False
        for aur_url_template in self.aur_urls:
            aur_url = aur_url_template.format(pkg_name=pkg_name)
            self.logger.info(f"Trying AUR URL: {aur_url}")
            result = self.shell_executor.run(
                f"git clone --depth 1 {aur_url} {pkg_dir}",
                check=False,
                log_cmd=False
            )
            if result and result.returncode == 0:
                clone_success = True
                self.logger.info(f"Successfully cloned {pkg_name} from {aur_url}")
                break
            else:
                if pkg_dir.exists():
                    shutil.rmtree(pkg_dir, ignore_errors=True)
                self.logger.warning(f"Failed to clone from {aur_url}")
        
        if not clone_success:
            self.logger.error(f"Failed to clone {pkg_name} from any AUR URL")
            return False
        
        # Set correct permissions
        self.shell_executor.run(f"chown -R builder:builder {pkg_dir}", check=False, log_cmd=False)
        
        pkgbuild = pkg_dir / "PKGBUILD"
        if not pkgbuild.exists():
            self.logger.error(f"No PKGBUILD found for {pkg_name}")
            shutil.rmtree(pkg_dir, ignore_errors=True)
            return False
        
        # Extract version info from SRCINFO (not regex)
        try:
            pkgver, pkgrel, epoch = self.version_manager.extract_version_from_srcinfo(pkg_dir)
            version = self.version_manager.get_full_version_string(pkgver, pkgrel, epoch)
            
            # DECISION LOGIC: Only build if AUR_VERSION > REMOTE_VERSION
            if remote_version and not self.version_manager.compare_versions(remote_version, pkgver, pkgrel, epoch):
                self.logger.info(f"âœ… {pkg_name} already up to date on server ({remote_version}) - skipping")
                self.build_state.add_skipped(pkg_name, version, is_aur=True, reason="up-to-date")
                
                # ZERO-RESIDUE FIX: Register the target version for this skipped package
                self.version_tracker.register_skipped_package(pkg_name, remote_version)
                
                shutil.rmtree(pkg_dir, ignore_errors=True)
                return True  # Skipped is considered successful for workflow
            
            if remote_version:
                self.logger.info(f"â„¹ï¸  {pkg_name}: remote has {remote_version}, building {version}")
                
                # PRE-BUILD PURGE: Remove old version files BEFORE building new version
                self._pre_build_purge(pkg_name, remote_version)
            else:
                self.logger.info(f"â„¹ï¸  {pkg_name}: not on server, building {version}")
                
        except Exception as e:
            self.logger.error(f"Failed to extract version for {pkg_name}: {e}")
            version = "unknown"
        
        # Attempt build
        build_success = self._execute_build(pkg_name, pkg_dir, version)
        
        if build_success:
            # ZERO-RESIDUE FIX: Register the target version for this built package
            self.version_tracker.register_built_package(pkg_name, version)
            self.build_state.add_built(pkg_name, version, is_aur=True)
        
        return build_success
    
    def _pre_build_purge(self, pkg_name: str, old_version: str):
        """Purge old version before building new one"""
        # This is a placeholder - actual cleanup is handled by CleanupManager
        self.logger.debug(f"Would purge old version {old_version} of {pkg_name}")
    
    def _execute_build(self, pkg_name: str, pkg_dir: Path, version: str) -> bool:
        """Execute the actual build process"""
        try:
            self.logger.info(f"Building {pkg_name} ({version})...")
            
            # Clean workspace before building
            self._clean_workspace(pkg_dir)
            
            self.logger.info("Downloading sources...")
            source_result = self.shell_executor.run(
                f"makepkg -od --noconfirm",
                cwd=pkg_dir,
                check=False,
                capture=True,
                timeout=600,
                extra_env={"PACKAGER": self.packager_id},
                log_cmd=False
            )
            
            if source_result.returncode != 0:
                self.logger.error(f"Failed to download sources for {pkg_name}")
                shutil.rmtree(pkg_dir, ignore_errors=True)
                return False
            
            # CRITICAL FIX: Install dependencies with AUR fallback
            self.logger.info("Installing dependencies with AUR fallback...")
            
            # First attempt: Standard makepkg -si
            self.logger.info("Building package (first attempt)...")
            build_result = self.shell_executor.run(
                f"makepkg -si --noconfirm --clean --nocheck",
                cwd=pkg_dir,
                capture=True,
                check=False,
                timeout=3600,
                extra_env={"PACKAGER": self.packager_id},
                log_cmd=True
            )
            
            # If first attempt fails, try yay fallback for missing dependencies
            if build_result.returncode != 0:
                self.logger.warning(f"First build attempt failed for {pkg_name}, trying AUR dependency fallback...")
                
                # Extract missing dependencies from error output
                error_output = build_result.stderr if build_result.stderr else build_result.stdout
                missing_deps = self._extract_missing_dependencies(error_output)
                
                if missing_deps:
                    self.logger.info(f"Found missing dependencies: {missing_deps}")
                    
                    # Try to install missing dependencies with yay
                    deps_str = ' '.join(missing_deps)
                    yay_cmd = f"LC_ALL=C yay -S --needed --noconfirm {deps_str}"
                    yay_result = self.shell_executor.run(
                        yay_cmd,
                        log_cmd=True,
                        check=False,
                        user="builder",
                        timeout=1800
                    )
                    
                    if yay_result.returncode == 0:
                        self.logger.info("âœ… Missing dependencies installed via yay, retrying build...")
                        
                        # Retry the build
                        build_result = self.shell_executor.run(
                            f"makepkg -si --noconfirm --clean --nocheck",
                            cwd=pkg_dir,
                            capture=True,
                            check=False,
                            timeout=3600,
                            extra_env={"PACKAGER": self.packager_id},
                            log_cmd=True
                        )
                    else:
                        self.logger.error(f"âŒ Failed to install missing dependencies with yay")
                        shutil.rmtree(pkg_dir, ignore_errors=True)
                        return False
            
            if build_result.returncode == 0:
                moved = False
                for pkg_file in pkg_dir.glob("*.pkg.tar.*"):
                    dest = self.output_dir / pkg_file.name
                    shutil.move(str(pkg_file), str(dest))
                    self.logger.info(f"âœ… Built: {pkg_file.name}")
                    moved = True
                
                shutil.rmtree(pkg_dir, ignore_errors=True)
                
                if moved:
                    return True
                else:
                    self.logger.error(f"No package files created for {pkg_name}")
                    return False
            else:
                self.logger.error(f"Failed to build {pkg_name}")
                shutil.rmtree(pkg_dir, ignore_errors=True)
                return False
                
        except Exception as e:
            self.logger.error(f"Error building {pkg_name}: {e}")
            if pkg_dir.exists():
                shutil.rmtree(pkg_dir, ignore_errors=True)
            return False
    
    def _clean_workspace(self, pkg_dir: Path):
        """Clean workspace before building to avoid contamination"""
        self.logger.info(f"ðŸ§¹ Cleaning workspace for {pkg_dir.name}...")
        
        # Clean src/ directory if exists
        src_dir = pkg_dir / "src"
        if src_dir.exists():
            try:
                shutil.rmtree(src_dir, ignore_errors=True)
                self.logger.info(f"  Cleaned src/ directory")
            except Exception as e:
                self.logger.warning(f"  Could not clean src/: {e}")
        
        # Clean pkg/ directory if exists
        pkg_build_dir = pkg_dir / "pkg"
        if pkg_build_dir.exists():
            try:
                shutil.rmtree(pkg_build_dir, ignore_errors=True)
                self.logger.info(f"  Cleaned pkg/ directory")
            except Exception as e:
                self.logger.warning(f"  Could not clean pkg/: {e}")
        
        # Clean any leftover .tar.* files
        for leftover in pkg_dir.glob("*.pkg.tar.*"):
            try:
                leftover.unlink()
                self.logger.info(f"  Removed leftover package: {leftover.name}")
            except Exception as e:
                self.logger.warning(f"  Could not remove {leftover}: {e}")
    
    def _extract_missing_dependencies(self, error_output: str) -> List[str]:
        """Extract missing dependencies from error output"""
        missing_deps = []
        
        # Look for patterns like "error: target not found: <package>"
        missing_patterns = [
            r"error: target not found: (\S+)",
            r"Could not find all required packages:",
            r":: Unable to find (\S+)",
        ]
        
        for pattern in missing_patterns:
            matches = re.findall(pattern, error_output)
            if matches:
                missing_deps.extend(matches)
        
        # Also look for specific makepkg dependency errors
        if "makepkg: cannot find the" in error_output:
            lines = error_output.split('\n')
            for line in lines:
                if "makepkg: cannot find the" in line:
                    # Extract package name from line like "makepkg: cannot find the 'gcc14' package"
                    dep_match = re.search(r"cannot find the '([^']+)'", line)
                    if dep_match:
                        missing_deps.append(dep_match.group(1))
        
        # Remove duplicates
        missing_deps = list(set(missing_deps))
        return missing_deps
--- FILE: .github/scripts/modules/build/local_builder.py ---
"""
Local package builder with SRCINFO-based version comparison and dependency fallback
Extracted from PackageBuilder._build_local_package
"""

import os
import shutil
import re
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List

from modules.common.shell_executor import ShellExecutor
from modules.build.version_manager import VersionManager


class LocalBuilder:
    """Handles local package building with version comparison and dependency resolution"""
    
    def __init__(self, config: Dict[str, Any], shell_executor: ShellExecutor,
                 version_manager: VersionManager, version_tracker,
                 build_state, logger: Optional[logging.Logger] = None):
        """
        Initialize LocalBuilder
        
        Args:
            config: Configuration dictionary
            shell_executor: ShellExecutor instance
            version_manager: VersionManager instance
            version_tracker: VersionTracker instance
            build_state: BuildState instance
            logger: Optional logger instance
        """
        self.config = config
        self.shell_executor = shell_executor
        self.version_manager = version_manager
        self.version_tracker = version_tracker
        self.build_state = build_state
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract configuration
        self.repo_root = Path(config.get('repo_root', '.'))
        self.output_dir = Path(config.get('output_dir', 'built_packages'))
        self.packager_id = config.get('packager_id', 'Maintainer <no-reply@gshoots.hu>')
        self.repo_name = config.get('repo_name', '')
        
        # HOKIBOT data collection
        self.hokibot_data = []
    
    def build(self, pkg_name: str, remote_version: Optional[str] = None) -> bool:
        """
        Build local package with SRCINFO-based version comparison and dependency fallback
        
        Args:
            pkg_name: Package name
            remote_version: Optional remote version for comparison
        
        Returns:
            True if build successful or skipped (up-to-date), False on failure
        """
        pkg_dir = self.repo_root / pkg_name
        if not pkg_dir.exists():
            self.logger.error(f"Package directory not found: {pkg_name}")
            return False
        
        pkgbuild = pkg_dir / "PKGBUILD"
        if not pkgbuild.exists():
            self.logger.error(f"No PKGBUILD found for {pkg_name}")
            return False
        
        # Extract version info from SRCINFO (not regex)
        try:
            pkgver, pkgrel, epoch = self.version_manager.extract_version_from_srcinfo(pkg_dir)
            version = self.version_manager.get_full_version_string(pkgver, pkgrel, epoch)
            
            # DECISION LOGIC: Only build if AUR_VERSION > REMOTE_VERSION
            if remote_version and not self.version_manager.compare_versions(remote_version, pkgver, pkgrel, epoch):
                self.logger.info(f"âœ… {pkg_name} already up to date on server ({remote_version}) - skipping")
                self.build_state.add_skipped(pkg_name, version, is_aur=False, reason="up-to-date")
                
                # ZERO-RESIDUE FIX: Register the target version for this skipped package
                self.version_tracker.register_skipped_package(pkg_name, remote_version)
                
                return True  # Skipped is considered successful for workflow
            
            if remote_version:
                self.logger.info(f"â„¹ï¸  {pkg_name}: remote has {remote_version}, building {version}")
                
                # PRE-BUILD PURGE: Remove old version files BEFORE building new version
                self._pre_build_purge(pkg_name, remote_version)
            else:
                self.logger.info(f"â„¹ï¸  {pkg_name}: not on server, building {version}")
                
        except Exception as e:
            self.logger.error(f"Failed to extract version for {pkg_name}: {e}")
            version = "unknown"
        
        # Attempt build
        build_success = self._execute_build(pkg_name, pkg_dir, version, pkgver, pkgrel, epoch)
        
        if build_success:
            # ZERO-RESIDUE FIX: Register the target version for this built package
            self.version_tracker.register_built_package(pkg_name, version)
            self.build_state.add_built(pkg_name, version, is_aur=False)
        
        return build_success
    
    def _pre_build_purge(self, pkg_name: str, old_version: str):
        """Purge old version before building new one"""
        # This is a placeholder - actual cleanup is handled by CleanupManager
        self.logger.debug(f"Would purge old version {old_version} of {pkg_name}")
    
    def _execute_build(self, pkg_name: str, pkg_dir: Path, version: str,
                      pkgver: str, pkgrel: str, epoch: Optional[str]) -> bool:
        """Execute the actual build process"""
        try:
            self.logger.info(f"Building {pkg_name} ({version})...")
            
            # Clean workspace before building
            self._clean_workspace(pkg_dir)
            
            self.logger.info("Downloading sources...")
            source_result = self.shell_executor.run(
                f"makepkg -od --noconfirm",
                cwd=pkg_dir,
                check=False,
                capture=True,
                timeout=600,
                extra_env={"PACKAGER": self.packager_id},
                log_cmd=False
            )
            
            if source_result.returncode != 0:
                self.logger.error(f"Failed to download sources for {pkg_name}")
                return False
            
            # CRITICAL FIX: Install dependencies with AUR fallback
            self.logger.info("Installing dependencies with AUR fallback...")
            
            # First attempt: Standard makepkg with appropriate flags
            makepkg_flags = "-si --noconfirm --clean"
            if pkg_name == "gtk2":
                makepkg_flags += " --nocheck"
                self.logger.info("GTK2: Skipping check step (long)")
            
            self.logger.info("Building package (first attempt)...")
            build_result = self.shell_executor.run(
                f"makepkg {makepkg_flags}",
                cwd=pkg_dir,
                capture=True,
                check=False,
                timeout=3600,
                extra_env={"PACKAGER": self.packager_id},
                log_cmd=True
            )
            
            # If first attempt fails, try yay fallback for missing dependencies
            if build_result.returncode != 0:
                self.logger.warning(f"First build attempt failed for {pkg_name}, trying AUR dependency fallback...")
                
                # Extract missing dependencies from error output
                error_output = build_result.stderr if build_result.stderr else build_result.stdout
                missing_deps = self._extract_missing_dependencies(error_output)
                
                if missing_deps:
                    self.logger.info(f"Found missing dependencies: {missing_deps}")
                    
                    # Try to install missing dependencies with yay
                    deps_str = ' '.join(missing_deps)
                    yay_cmd = f"LC_ALL=C yay -S --needed --noconfirm {deps_str}"
                    yay_result = self.shell_executor.run(
                        yay_cmd,
                        log_cmd=True,
                        check=False,
                        user="builder",
                        timeout=1800
                    )
                    
                    if yay_result.returncode == 0:
                        self.logger.info("âœ… Missing dependencies installed via yay, retrying build...")
                        
                        # Retry the build
                        build_result = self.shell_executor.run(
                            f"makepkg {makepkg_flags}",
                            cwd=pkg_dir,
                            capture=True,
                            check=False,
                            timeout=3600,
                            extra_env={"PACKAGER": self.packager_id},
                            log_cmd=True
                        )
                    else:
                        self.logger.error(f"âŒ Failed to install missing dependencies with yay")
                        return False
            
            if build_result.returncode == 0:
                moved = False
                built_files = []
                for pkg_file in pkg_dir.glob("*.pkg.tar.*"):
                    dest = self.output_dir / pkg_file.name
                    shutil.move(str(pkg_file), str(dest))
                    self.logger.info(f"âœ… Built: {pkg_file.name}")
                    moved = True
                    built_files.append(str(dest))
                
                if moved:
                    # Collect metadata for hokibot
                    if built_files:
                        # Simplified metadata extraction
                        self.hokibot_data.append({
                            'name': pkg_name,
                            'built_version': version,
                            'pkgver': pkgver,
                            'pkgrel': pkgrel,
                            'epoch': epoch
                        })
                        self.logger.info(f"ðŸ“ HOKIBOT observed: {pkg_name} -> {version}")
                    
                    return True
                else:
                    self.logger.error(f"No package files created for {pkg_name}")
                    return False
            else:
                self.logger.error(f"Failed to build {pkg_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"Error building {pkg_name}: {e}")
            return False
    
    def _clean_workspace(self, pkg_dir: Path):
        """Clean workspace before building to avoid contamination"""
        self.logger.info(f"ðŸ§¹ Cleaning workspace for {pkg_dir.name}...")
        
        # Clean src/ directory if exists
        src_dir = pkg_dir / "src"
        if src_dir.exists():
            try:
                shutil.rmtree(src_dir, ignore_errors=True)
                self.logger.info(f"  Cleaned src/ directory")
            except Exception as e:
                self.logger.warning(f"  Could not clean src/: {e}")
        
        # Clean pkg/ directory if exists
        pkg_build_dir = pkg_dir / "pkg"
        if pkg_build_dir.exists():
            try:
                shutil.rmtree(pkg_build_dir, ignore_errors=True)
                self.logger.info(f"  Cleaned pkg/ directory")
            except Exception as e:
                self.logger.warning(f"  Could not clean pkg/: {e}")
        
        # Clean any leftover .tar.* files
        for leftover in pkg_dir.glob("*.pkg.tar.*"):
            try:
                leftover.unlink()
                self.logger.info(f"  Removed leftover package: {leftover.name}")
            except Exception as e:
                self.logger.warning(f"  Could not remove {leftover}: {e}")
    
    def _extract_missing_dependencies(self, error_output: str) -> List[str]:
        """Extract missing dependencies from error output"""
        missing_deps = []
        
        # Look for patterns like "error: target not found: <package>"
        missing_patterns = [
            r"error: target not found: (\S+)",
            r"Could not find all required packages:",
            r":: Unable to find (\S+)",
        ]
        
        for pattern in missing_patterns:
            matches = re.findall(pattern, error_output)
            if matches:
                missing_deps.extend(matches)
        
        # Also look for specific makepkg dependency errors
        if "makepkg: cannot find the" in error_output:
            lines = error_output.split('\n')
            for line in lines:
                if "makepkg: cannot find the" in line:
                    # Extract package name from line like "makepkg: cannot find the 'gcc14' package"
                    dep_match = re.search(r"cannot find the '([^']+)'", line)
                    if dep_match:
                        missing_deps.append(dep_match.group(1))
        
        # Remove duplicates
        missing_deps = list(set(missing_deps))
        return missing_deps
    
    def get_hokibot_data(self):
        """Get collected hokibot data"""
        return self.hokibot_data
--- FILE: .github/scripts/modules/repo/__init__.py ---
"""
Repository management modules
"""

# Placeholder for repository modules - will be implemented in Phase 3
--- FILE: .github/scripts/modules/repo/version_tracker.py ---
"""
Version tracking for Zero-Residue cleanup policy - SERVER-FIRST ARCHITECTURE
Enhanced with VCS version resolution and protected artifacts
"""

import os
import json
import re
import subprocess
import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set, Any
from datetime import datetime

from modules.vps.ssh_client import SSHClient


class VersionTracker:
    """
    Tracks package versions with VCS resolution and protected artifacts
    """
    
    def __init__(self, repo_root: Path, ssh_client: SSHClient, logger: Optional[logging.Logger] = None):
        """
        Initialize VersionTracker
        
        Args:
            repo_root: Repository root directory
            ssh_client: SSHClient instance for remote operations
            logger: Optional logger instance
        """
        self.repo_root = repo_root
        self.ssh_client = ssh_client
        self.logger = logger or logging.getLogger(__name__)
        
        # VCS cache for git package versions
        self._vcs_version_cache: Dict[str, str] = {}
        
        # Protected artifacts registry (multi-package PKGBUILD outputs)
        self._protected_files: Set[str] = set()
        
        # Pending deletions queue
        self._pending_deletions: List[str] = []
        
        # Target versions: {pkg_name: target_version} - versions we want to keep
        self._target_versions: Dict[str, str] = {}
        
        # Skipped packages: {pkg_name: remote_version} - packages skipped as up-to-date
        self._skipped_packages: Dict[str, str] = {}
        
        # Built packages: {pkg_name: built_version} - packages we just built
        self._built_packages: Dict[str, str] = {}
        
        # JSON state file
        self.state_file = self._get_state_path()
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        self.state: Dict[str, Any] = self._load_state()
        
        # Load remote inventory at initialization
        self._load_remote_inventory()
    
    def _get_state_path(self) -> Path:
        """
        Get platform-appropriate state file path
        
        Returns:
            Path to state file
        """
        # Priority 1: GitHub Actions workspace
        github_workspace = os.getenv('GITHUB_WORKSPACE')
        if github_workspace:
            workspace_path = Path(github_workspace)
            if workspace_path.exists():
                state_path = workspace_path / ".build_tracking" / "vps_state.json"
                self.logger.info(f"Using GitHub workspace state path: {state_path}")
                return state_path
        
        # Priority 2: Repository root
        repo_state_path = self.repo_root / ".build_tracking" / "vps_state.json"
        if self.repo_root.exists():
            self.logger.info(f"Using repository state path: {repo_state_path}")
            return repo_state_path
        
        # Priority 3: User home directory
        home_dir = Path.home()
        home_state_path = home_dir / ".build_tracking" / "vps_state.json"
        home_state_path.parent.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"Using home directory state path: {home_state_path}")
        return home_state_path
    
    def _load_state(self) -> Dict[str, Any]:
        """Load state from JSON file"""
        try:
            if self.state_file.exists():
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                self.logger.info(f"Loaded state from {self.state_file}")
                return state
            else:
                self.logger.info(f"State file {self.state_file} does not exist, creating new")
                return {"packages": {}, "metadata": {"created": datetime.now().isoformat()}}
        except Exception as e:
            self.logger.error(f"Failed to load state file: {e}")
            return {"packages": {}, "metadata": {"created": datetime.now().isoformat()}}
    
    def _load_remote_inventory(self):
        """Load remote inventory at initialization"""
        self.logger.info("ðŸ” Loading remote inventory...")
        remote_files = self.ssh_client.get_cached_inventory(force_refresh=True)
        
        self.logger.info(f"ðŸ“‹ Remote inventory loaded: {len(remote_files)} files")
    
    def resolve_vcs_version(self, pkg_name: str, pkg_dir: Path, force_refresh: bool = False) -> Optional[Tuple[str, str, str]]:
        """
        Resolve VCS package version by running makepkg --printsrcinfo
        
        Args:
            pkg_name: Package name
            pkg_dir: Directory containing PKGBUILD
            force_refresh: If True, ignore cache and refresh
        
        Returns:
            Tuple of (pkgver, pkgrel, epoch) or None if failed
        """
        # Check cache first
        cache_key = f"{pkg_name}_{pkg_dir}"
        if not force_refresh and cache_key in self._vcs_version_cache:
            version_str = self._vcs_version_cache[cache_key]
            return self._parse_version_string(version_str)
        
        # Determine if this is a VCS package
        is_vcs_package = self._is_vcs_package(pkg_name, pkg_dir)
        
        try:
            if is_vcs_package:
                self.logger.info(f"ðŸ” Resolving VCS version for {pkg_name}...")
                
                # For VCS packages, we need to run makepkg --printsrcinfo
                # This will resolve git commit hashes to actual versions
                result = subprocess.run(
                    ['makepkg', '--printsrcinfo'],
                    cwd=pkg_dir,
                    capture_output=True,
                    text=True,
                    check=False,
                    timeout=300
                )
                
                if result.returncode == 0 and result.stdout:
                    version_info = self._extract_version_from_srcinfo(result.stdout)
                    if version_info:
                        pkgver, pkgrel, epoch = version_info
                        version_str = self._format_version_string(pkgver, pkgrel, epoch)
                        self._vcs_version_cache[cache_key] = version_str
                        self.logger.info(f"âœ… VCS version resolved: {version_str}")
                        return version_info
                    else:
                        self.logger.warning(f"Could not parse version from SRCINFO for {pkg_name}")
                else:
                    self.logger.warning(f"makepkg --printsrcinfo failed for {pkg_name}: {result.stderr}")
            
            # Fall back to standard .SRCINFO parsing
            return self._extract_version_from_srcinfo_file(pkg_dir)
            
        except subprocess.TimeoutExpired:
            self.logger.error(f"Timeout resolving VCS version for {pkg_name}")
            return None
        except Exception as e:
            self.logger.error(f"Error resolving VCS version for {pkg_name}: {e}")
            return None
    
    def _is_vcs_package(self, pkg_name: str, pkg_dir: Path) -> bool:
        """
        Check if package is a VCS package
        
        Args:
            pkg_name: Package name
            pkg_dir: Package directory
        
        Returns:
            True if VCS package
        """
        # Check package name patterns
        vcs_patterns = ['-git', '_git', '-svn', '_svn', '-hg', '_hg', '-bzr', '_bzr']
        if any(pattern in pkg_name.lower() for pattern in vcs_patterns):
            return True
        
        # Check PKGBUILD content
        pkgbuild_path = pkg_dir / "PKGBUILD"
        if pkgbuild_path.exists():
            try:
                with open(pkgbuild_path, 'r') as f:
                    content = f.read()
                
                # Look for VCS URLs
                vcs_url_patterns = [
                    r'git\+https?://',
                    r'https?://.*\.git',
                    r'svn\+https?://',
                    r'hg\+https?://',
                    r'bzr\+https?://'
                ]
                
                for pattern in vcs_url_patterns:
                    if re.search(pattern, content):
                        return True
            except Exception:
                pass
        
        return False
    
    def _extract_version_from_srcinfo(self, srcinfo_content: str) -> Optional[Tuple[str, str, Optional[str]]]:
        """Parse SRCINFO content to extract version information"""
        pkgver = None
        pkgrel = None
        epoch = None
        
        lines = srcinfo_content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Handle key-value pairs
            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip()
                
                if key == 'pkgver':
                    pkgver = value
                elif key == 'pkgrel':
                    pkgrel = value
                elif key == 'epoch':
                    epoch = value
        
        if not pkgver or not pkgrel:
            return None
        
        return pkgver, pkgrel, epoch
    
    def _extract_version_from_srcinfo_file(self, pkg_dir: Path) -> Optional[Tuple[str, str, Optional[str]]]:
        """Extract version from .SRCINFO file"""
        srcinfo_path = pkg_dir / ".SRCINFO"
        
        if srcinfo_path.exists():
            try:
                with open(srcinfo_path, 'r') as f:
                    content = f.read()
                return self._extract_version_from_srcinfo(content)
            except Exception as e:
                self.logger.warning(f"Failed to read .SRCINFO: {e}")
        
        return None
    
    def _parse_version_string(self, version_str: str) -> Optional[Tuple[str, str, Optional[str]]]:
        """Parse version string into components"""
        if ':' in version_str:
            # Has epoch
            epoch_part, rest = version_str.split(':', 1)
            epoch = epoch_part
            if '-' in rest:
                pkgver, pkgrel = rest.split('-', 1)
            else:
                pkgver = rest
                pkgrel = "1"
        else:
            # No epoch
            epoch = None
            if '-' in version_str:
                pkgver, pkgrel = version_str.split('-', 1)
            else:
                pkgver = version_str
                pkgrel = "1"
        
        return pkgver, pkgrel, epoch
    
    def _format_version_string(self, pkgver: str, pkgrel: str, epoch: Optional[str]) -> str:
        """Format version components into string"""
        if epoch and epoch != '0':
            return f"{epoch}:{pkgver}-{pkgrel}"
        return f"{pkgver}-{pkgrel}"
    
    def register_protected_files(self, pkg_name: str, files: List[str]):
        """
        Register protected files that should not be deleted
        
        Args:
            pkg_name: Package name
            files: List of filenames to protect
        """
        protected_count = 0
        for filename in files:
            if filename not in self._protected_files:
                self._protected_files.add(filename)
                protected_count += 1
                self.logger.debug(f"ðŸ”’ Protected file: {filename}")
        
        if protected_count > 0:
            self.logger.info(f"ðŸ”’ Registered {protected_count} protected files for {pkg_name}")
    
    def is_protected(self, filename: str) -> bool:
        """
        Check if a file is protected from deletion
        
        Args:
            filename: Filename to check
        
        Returns:
            True if file is protected
        """
        return filename in self._protected_files
    
    def get_protected_files(self) -> Set[str]:
        """Get all protected files"""
        return self._protected_files.copy()
    
    def clear_protected_files(self):
        """Clear protected files registry"""
        self._protected_files.clear()
        self.logger.debug("Cleared protected files registry")
    
    def queue_deletion(self, remote_path: str):
        """
        Queue a file for batch deletion
        
        Args:
            remote_path: Full remote path to delete
        """
        filename = Path(remote_path).name
        
        # Don't queue protected files
        if self.is_protected(filename):
            self.logger.debug(f"Skipping protected file: {filename}")
            return
        
        self._pending_deletions.append(remote_path)
        self.logger.debug(f"Queued for deletion: {filename}")
    
    def commit_queued_deletions(self) -> bool:
        """
        Execute all queued deletions via SSH client
        
        Returns:
            True if successful
        """
        if not self._pending_deletions:
            return True
        
        self.logger.info(f"ðŸ”§ Committing {len(self._pending_deletions)} queued deletions...")
        success = self.ssh_client.commit_queued_deletions()
        
        if success:
            self._pending_deletions.clear()
        
        return success
    
    def get_pending_deletions(self) -> List[str]:
        """Get list of pending deletions"""
        return self._pending_deletions.copy()
    
    def clear_pending_deletions(self):
        """Clear pending deletions queue"""
        self._pending_deletions.clear()
        self.logger.debug("Cleared pending deletions queue")
    
    def save_state(self) -> bool:
        """Save state to JSON file"""
        try:
            self.state["metadata"]["last_updated"] = datetime.now().isoformat()
            self.state["metadata"]["protected_files"] = list(self._protected_files)
            
            with open(self.state_file, 'w') as f:
                json.dump(self.state, f, indent=2)
            
            self.logger.debug(f"Saved state to {self.state_file}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to save state: {e}")
            return False
    
    def is_package_on_remote(self, pkg_name: str, version: str) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        SERVER-FIRST: Check if package with specific version exists on remote server
        
        Args:
            pkg_name: Package name (e.g., 'libinput-gestures')
            version: Package version (e.g., '2.81-1')
        
        Returns:
            Tuple of (found, remote_version, remote_hash) or (False, None, None)
        """
        self.logger.debug(f"Checking if {pkg_name} version {version} exists on remote...")
        
        # Get cached inventory
        remote_inventory = self.ssh_client.get_cached_inventory()
        
        # Normalize package name for matching (case-insensitive)
        pkg_name_lower = pkg_name.lower()
        
        for filename, file_path in remote_inventory.items():
            # Parse filename to extract name, version, and architecture
            parsed = self._parse_package_filename_with_arch(filename)
            if not parsed:
                continue
            
            remote_pkg_name, remote_version, architecture = parsed
            remote_pkg_name_lower = remote_pkg_name.lower()
            
            # Check if package name matches (case-insensitive)
            if remote_pkg_name_lower == pkg_name_lower:
                # Check if version matches (including epoch handling)
                if self._versions_match(remote_version, version):
                    self.logger.info(f"âœ… Found matching package on remote: {pkg_name} {version}")
                    
                    # Get hash from remote file
                    remote_hash = self.ssh_client.get_remote_hash(file_path)
                    
                    return True, remote_version, remote_hash
        
        self.logger.debug(f"Package {pkg_name} version {version} not found in remote files")
        return False, None, None
    
    def _versions_match(self, version1: str, version2: str) -> bool:
        """Check if two version strings match (handles epoch and architecture)"""
        # Normalize versions by removing epoch if it's 0
        def normalize_version(v: str) -> str:
            if ':' in v:
                epoch, rest = v.split(':', 1)
                if epoch == '0':
                    return rest
            return v
        
        v1_norm = normalize_version(version1)
        v2_norm = normalize_version(version2)
        
        return v1_norm == v2_norm
    
    def discover_and_adopt_remote_packages(self, pkg_name: str) -> Optional[Tuple[str, Optional[str]]]:
        """
        Enhanced adoption logic: Check remote server for package and adopt if found
        
        Args:
            pkg_name: Package name to search for
        
        Returns:
            Tuple of (version, hash) or None if not found
        """
        self.logger.info(f"ðŸ” Searching for {pkg_name} on remote server...")
        
        # Get cached inventory
        remote_inventory = self.ssh_client.get_cached_inventory()
        
        # Case-insensitive matching with architecture suffix handling
        pkg_name_lower = pkg_name.lower()
        
        for filename, file_path in remote_inventory.items():
            self.logger.debug(f"Checking file: {filename}")
            
            # Parse package name and version from filename
            parsed = self._parse_package_filename_with_arch(filename)
            if not parsed:
                continue
            
            remote_pkg_name, version, architecture = parsed
            remote_pkg_name_lower = remote_pkg_name.lower()
            
            # Case-insensitive comparison with architecture suffix handling
            if remote_pkg_name_lower == pkg_name_lower:
                self.logger.info(f"âœ… Found {pkg_name} on remote server: {filename}")
                
                # Get hash from remote file
                remote_hash = self.ssh_client.get_remote_hash(file_path)
                
                # Update state
                self.state["packages"][pkg_name] = {
                    "version": version,
                    "hash": remote_hash,
                    "last_updated": datetime.now().isoformat(),
                    "source": "adopted",
                    "filename": filename,
                    "architecture": architecture
                }
                
                # Save state immediately
                self.save_state()
                
                # Update target versions
                self._target_versions[pkg_name] = version
                self._skipped_packages[pkg_name] = version
                
                self.logger.info(f"ðŸ“¥ Adopted {pkg_name} version {version} from remote server")
                return version, remote_hash
        
        self.logger.debug(f"Package {pkg_name} not found in remote files")
        return None
    
    def _parse_package_filename_with_arch(self, filename: str) -> Optional[Tuple[str, str, str]]:
        """
        Parse package filename to extract name, version, and architecture
        
        Args:
            filename: Package filename (e.g., 'qownnotes-26.1.9-1-x86_64.pkg.tar.zst')
        
        Returns:
            Tuple of (package_name, version_string, architecture) or None
        """
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            if len(parts) < 4:
                return None
            
            # Try to find where package name ends and architecture begins
            # Architecture is usually the last part (x86_64, any, etc.)
            # Version is usually the 2 or 3 parts before architecture
            
            # Start from the end and work backwards
            for i in range(len(parts) - 2, 0, -1):
                # Check if the remaining parts look like version-release-architecture
                remaining = parts[i:]
                
                if len(remaining) >= 3:
                    # Check for architecture suffix (x86_64, any, etc.)
                    arch = remaining[-1]
                    
                    # Check for epoch format (e.g., "2-26.1.9-1-x86_64")
                    if remaining[0].isdigit() and len(remaining) >= 4:
                        epoch = remaining[0]
                        version_part = remaining[1]
                        release_part = remaining[2]
                        version_str = f"{epoch}:{version_part}-{release_part}"
                        package_name = '-'.join(parts[:i])
                        return package_name, version_str, arch
                    # Standard format (e.g., "26.1.9-1-x86_64")
                    elif any(c.isdigit() for c in remaining[0]) and remaining[1].isdigit():
                        version_part = remaining[0]
                        release_part = remaining[1]
                        version_str = f"{version_part}-{release_part}"
                        package_name = '-'.join(parts[:i])
                        return package_name, version_str, arch
        
        except Exception as e:
            self.logger.debug(f"Could not parse filename {filename}: {e}")
        
        return None
    
    def register_built_package(self, pkg_name: str, version: str, hash_value: Optional[str] = None) -> None:
        """
        Register a package that was just built or adopted from VPS
        
        Args:
            pkg_name: Package name
            version: Package version
            hash_value: Optional hash value for verification
        """
        # Update built packages registry
        self._built_packages[pkg_name] = version
        self._target_versions[pkg_name] = version
        
        # Update JSON state
        if "packages" not in self.state:
            self.state["packages"] = {}
        
        source = "adopted" if hash_value is not None else "built"
        
        self.state["packages"][pkg_name] = {
            "version": version,
            "hash": hash_value,
            "last_updated": datetime.now().isoformat(),
            "source": source
        }
        
        # Save state
        self.save_state()
        
        self.logger.info(f"ðŸ“ Registered {source} package: {pkg_name} ({version})")
    
    def register_target_version(self, pkg_name: str, target_version: str) -> None:
        """
        Register the target version for a package
        
        Args:
            pkg_name: Package name
            target_version: The version we want to keep (either built or latest from server)
        """
        self._target_versions[pkg_name] = target_version
        self.logger.info(f"ðŸ“ Registered target version for {pkg_name}: {target_version}")
    
    def register_skipped_package(self, pkg_name: str, remote_version: str) -> None:
        """
        Register a package that was skipped because it's up-to-date
        
        Args:
            pkg_name: Package name
            remote_version: The remote version that should be kept (not deleted)
        """
        # Store in skipped registry
        self._skipped_packages[pkg_name] = remote_version
        
        # ðŸš¨ CRITICAL: Explicitly set target version to remote version
        self._target_versions[pkg_name] = remote_version
        
        # Update JSON state
        if "packages" not in self.state:
            self.state["packages"] = {}
        
        self.state["packages"][pkg_name] = {
            "version": remote_version,
            "hash": None,
            "last_updated": datetime.now().isoformat(),
            "source": "skipped"
        }
        
        # Save state
        self.save_state()
        
        self.logger.info(f"ðŸ“ Registered SKIPPED package: {pkg_name} (remote: {remote_version}, target: {remote_version})")
    
    def get_target_version(self, pkg_name: str) -> Optional[str]:
        """
        Get target version for a package
        
        Args:
            pkg_name: Package name
        
        Returns:
            Target version or None if not registered
        """
        return self._target_versions.get(pkg_name)
    
    def has_target_version(self, pkg_name: str) -> bool:
        """
        Check if a package has a registered target version
        
        Args:
            pkg_name: Package name
        
        Returns:
            True if target version exists
        """
        return pkg_name in self._target_versions
    
    def is_skipped(self, pkg_name: str) -> bool:
        """
        Check if a package was skipped
        
        Args:
            pkg_name: Package name
        
        Returns:
            True if package was skipped
        """
        return pkg_name in self._skipped_packages
    
    def is_built(self, pkg_name: str) -> bool:
        """
        Check if a package was built in this run
        
        Args:
            pkg_name: Package name
        
        Returns:
            True if package was built
        """
        return pkg_name in self._built_packages
    
    def set_remote_inventory(self, remote_files: Dict[str, str]) -> None:
        """
        Set remote inventory from VPS
        
        Args:
            remote_files: Dictionary of {filename: full_path} from VPS
        """
        self.logger.info(f"ðŸ“‹ Remote inventory updated: {len(remote_files)} files")
    
    def get_remote_inventory(self) -> Dict[str, str]:
        """
        Get current remote inventory from cache
        
        Returns:
            Dictionary of {filename: full_path}
        """
        return self.ssh_client.get_cached_inventory()
    
    def get_files_to_keep(self) -> Set[str]:
        """
        Determine which files should be kept based on target versions
        
        Returns:
            Set of filenames that match target versions
        """
        files_to_keep = set()
        remote_inventory = self.get_remote_inventory()
        
        for filename in remote_inventory:
            # Parse filename to extract package name and version
            parsed = self._parse_package_filename(filename)
            if not parsed:
                # Can't parse, keep it to be safe
                files_to_keep.add(filename)
                continue
            
            pkg_name, version_str = parsed
            
            # Check if this package has a target version
            if pkg_name in self._target_versions:
                target_version = self._target_versions[pkg_name]
                if version_str == target_version:
                    # This is the version we want to keep
                    files_to_keep.add(filename)
                    self.logger.debug(f"âœ… Keeping {filename} (matches target version {target_version})")
                else:
                    self.logger.debug(f"ðŸ—‘ï¸ Marking for deletion: {filename} (target is {target_version})")
            else:
                # No target version registered - keep to be safe
                files_to_keep.add(filename)
                self.logger.debug(f"âš ï¸ Keeping unknown package: {filename} (not in target versions)")
        
        return files_to_keep
    
    def get_files_to_delete(self) -> List[str]:
        """
        Determine which files should be deleted based on target versions
        
        Returns:
            List of full paths to delete
        """
        files_to_delete = []
        remote_inventory = self.get_remote_inventory()
        files_to_keep = self.get_files_to_keep()
        
        for filename, full_path in remote_inventory.items():
            if filename not in files_to_keep and not self.is_protected(filename):
                files_to_delete.append(full_path)
        
        return files_to_delete
    
    def _parse_package_filename(self, filename: str) -> Optional[Tuple[str, str]]:
        """
        Parse package filename to extract name and version (without architecture)
        
        Args:
            filename: Package filename (e.g., 'qownnotes-26.1.9-1-x86_64.pkg.tar.zst')
        
        Returns:
            Tuple of (package_name, version_string) or None
        """
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            if len(parts) < 4:
                return None
            
            # Try to find where package name ends
            for i in range(len(parts) - 3, 0, -1):
                potential_name = '-'.join(parts[:i])
                remaining = parts[i:]
                
                if len(remaining) >= 3:
                    # Check for epoch format (e.g., "2-26.1.9-1")
                    if remaining[0].isdigit() and '-' in '-'.join(remaining[1:]):
                        epoch = remaining[0]
                        version_part = remaining[1]
                        release_part = remaining[2]
                        version_str = f"{epoch}:{version_part}-{release_part}"
                        return potential_name, version_str
                    # Standard format (e.g., "26.1.9-1")
                    elif any(c.isdigit() for c in remaining[0]) and remaining[1].isdigit():
                        version_part = remaining[0]
                        release_part = remaining[1]
                        version_str = f"{version_part}-{release_part}"
                        return potential_name, version_str
        
        except Exception as e:
            self.logger.debug(f"Could not parse filename {filename}: {e}")
        
        return None
    
    def clear_remote_inventory(self) -> None:
        """Clear remote inventory cache"""
        self.ssh_client.clear_cache()
        self.logger.debug("Cleared remote inventory cache")
    
    def get_target_packages(self) -> Dict[str, str]:
        """Get all target packages"""
        return self._target_versions.copy()
    
    def get_skipped_packages_dict(self) -> Dict[str, str]:
        """Get all skipped packages"""
        return self._skipped_packages.copy()
    
    def get_built_packages_dict(self) -> Dict[str, str]:
        """Get all built packages"""
        return self._built_packages.copy()
    
    def has_packages(self) -> bool:
        """Check if any packages are registered"""
        return bool(self._target_versions)
    
    def get_state_summary(self) -> Dict[str, Any]:
        """Get state summary for logging"""
        return {
            "total_packages": len(self.state.get("packages", {})),
            "last_updated": self.state.get("metadata", {}).get("last_updated"),
            "protected_files": len(self._protected_files),
            "pending_deletions": len(self._pending_deletions),
        }
--- FILE: .github/scripts/modules/repo/database_manager.py ---
"""
Repository database management with additive updates and GPG signing
Implements robust bidirectional sync with local staging
"""

import os
import sys
import shutil
import subprocess
import logging
import tempfile
import glob
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

from modules.vps.ssh_client import SSHClient
from modules.vps.rsync_client import RsyncClient


class DatabaseManager:
    """Manages repository database operations with additive updates and GPG signing"""
    
    def __init__(self, config: Dict[str, Any], ssh_client: SSHClient,
                 rsync_client: RsyncClient, logger: Optional[logging.Logger] = None):
        """
        Initialize DatabaseManager
        
        Args:
            config: Configuration dictionary
            ssh_client: SSHClient instance
            rsync_client: RsyncClient instance
            logger: Optional logger instance
        """
        self.config = config
        self.ssh_client = ssh_client
        self.rsync_client = rsync_client
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract configuration
        self.repo_name = config.get('repo_name', '')
        self.output_dir = Path(config.get('output_dir', 'built_packages'))
        self.remote_dir = config.get('remote_dir', '')
        
        # GPG configuration
        self.gpg_enabled = bool(config.get('gpg_key_id') and config.get('gpg_private_key'))
        self.gpg_key_id = config.get('gpg_key_id', '')
        
        # Staging directory for safe database operations
        self._staging_dir: Optional[Path] = None
    
    def create_staging_dir(self) -> Path:
        """
        Create temporary staging directory for database operations
        
        Returns:
            Path to staging directory
        """
        if self._staging_dir and self._staging_dir.exists():
            shutil.rmtree(self._staging_dir, ignore_errors=True)
        
        self._staging_dir = Path(tempfile.mkdtemp(prefix="repo_staging_"))
        self.logger.info(f"ðŸ“ Created staging directory: {self._staging_dir}")
        return self._staging_dir
    
    def cleanup_staging_dir(self):
        """Clean up staging directory"""
        if self._staging_dir and self._staging_dir.exists():
            try:
                shutil.rmtree(self._staging_dir, ignore_errors=True)
                self.logger.debug(f"ðŸ§¹ Cleaned up staging directory: {self._staging_dir}")
                self._staging_dir = None
            except Exception as e:
                self.logger.warning(f"Could not clean staging directory: {e}")
    
    def download_existing_database(self) -> bool:
        """
        Download existing database files from VPS to staging directory
        
        Returns:
            True if successful or no database exists (first run)
        """
        self.logger.info("ðŸ“¥ Downloading existing database files from VPS...")
        
        patterns = [
            f"{self.repo_name}.db.tar.gz*",
            f"{self.repo_name}.files.tar.gz*"
        ]
        
        for pattern in patterns:
            self.logger.debug(f"  Downloading pattern: {pattern}")
            
            success = self.rsync_client.mirror_remote(
                remote_pattern=pattern,
                local_dir=self._staging_dir,
                temp_dir=None  # Use staging_dir directly
            )
            
            if not success:
                self.logger.debug(f"âš ï¸ Failed to download {pattern} (may not exist yet)")
        
        # Check what was downloaded
        db_files = list(self._staging_dir.glob(f"{self.repo_name}.db.tar.gz*"))
        files_files = list(self._staging_dir.glob(f"{self.repo_name}.files.tar.gz*"))
        
        total_files = len(db_files) + len(files_files)
        
        if total_files > 0:
            self.logger.info(f"âœ… Downloaded {total_files} database files from VPS")
            for f in db_files + files_files:
                size_mb = f.stat().st_size / (1024 * 1024)
                self.logger.debug(f"  - {f.name} ({size_mb:.2f} MB)")
            return True
        else:
            self.logger.info("â„¹ï¸ No existing database files found (first run or clean state)")
            return True  # Not an error - first run is OK
    
    def copy_new_packages_to_staging(self) -> List[Path]:
        """
        Copy newly built packages to staging directory
        
        Returns:
            List of paths to new packages in staging
        """
        new_packages = list(self.output_dir.glob("*.pkg.tar.zst"))
        if not new_packages:
            self.logger.info("â„¹ï¸ No new packages to add to database")
            return []
        
        self.logger.info(f"ðŸ“¦ Moving {len(new_packages)} new packages to staging...")
        
        moved_packages = []
        
        for new_pkg in new_packages:
            try:
                dest = self._staging_dir / new_pkg.name
                if dest.exists():
                    dest.unlink()
                shutil.move(str(new_pkg), str(dest))
                moved_packages.append(dest)
                
                # Move signature if exists
                sig_file = new_pkg.with_suffix(new_pkg.suffix + '.sig')
                if sig_file.exists():
                    sig_dest = dest.with_suffix(dest.suffix + '.sig')
                    if sig_dest.exists():
                        sig_dest.unlink()
                    shutil.move(str(sig_file), str(sig_dest))
                
                self.logger.debug(f"  Moved: {new_pkg.name}")
            except Exception as e:
                self.logger.error(f"Failed to move {new_pkg.name}: {e}")
        
        self.logger.info(f"âœ… Moved {len(moved_packages)} new packages to staging")
        return moved_packages
    
    def update_database_additive(self, force_repair: bool = False) -> bool:
        """
        Update repository database additively with GPG signing
        
        Args:
            force_repair: If True, force database re-signing even if no new packages
        
        Returns:
            True if successful
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ADDITIVE DATABASE UPDATE WITH GPG SIGNING")
        self.logger.info("=" * 60)
        
        try:
            # Step 1: Create staging directory
            self.create_staging_dir()
            
            # Step 2: Download existing database files
            if not self.download_existing_database():
                self.logger.error("âŒ Failed to download existing database")
                return False
            
            # Step 3: Copy new packages to staging
            new_packages = self.copy_new_packages_to_staging()
            
            # Step 4: Check if we have anything to update
            existing_db = list(self._staging_dir.glob(f"{self.repo_name}.db.tar.gz"))
            has_existing_db = len(existing_db) > 0
            
            if not new_packages and not (force_repair and has_existing_db):
                self.logger.info("â„¹ï¸ No new packages and not forcing repair - skipping database update")
                return True
            
            # Step 5: Update database locally with GPG signing
            old_cwd = os.getcwd()
            os.chdir(self._staging_dir)
            
            try:
                db_file = f"{self.repo_name}.db.tar.gz"
                
                # Clean any partial database files
                for f in [f"{self.repo_name}.db", f"{self.repo_name}.files"]:
                    if os.path.exists(f):
                        os.remove(f)
                
                # Build repo-add command with appropriate flags
                if self.gpg_enabled:
                    self.logger.info("ðŸ” Running repo-add with GPG signing...")
                    
                    # Set environment for non-interactive GPG signing
                    env = os.environ.copy()
                    env['GNUPGHOME'] = '/etc/pacman.d/gnupg'
                    
                    cmd = f"repo-add --sign --key {self.gpg_key_id} --remove {db_file} *.pkg.tar.zst"
                    
                    result = subprocess.run(
                        cmd,
                        shell=True,
                        capture_output=True,
                        text=True,
                        env=env,
                        check=False
                    )
                else:
                    self.logger.info("ðŸ”§ Running repo-add without signing...")
                    cmd = f"repo-add --remove {db_file} *.pkg.tar.zst"
                    
                    result = subprocess.run(
                        cmd,
                        shell=True,
                        capture_output=True,
                        text=True,
                        check=False
                    )
                
                if result.returncode == 0:
                    self.logger.info("âœ… Database updated successfully")
                    
                    # Verify the database was created
                    if not os.path.exists(db_file):
                        self.logger.error("âŒ Database file not created")
                        return False
                    
                    # Verify database entries
                    self._verify_database_entries(db_file)
                    
                    # Generate .files database if needed
                    if not os.path.exists(f"{self.repo_name}.files.tar.gz"):
                        self.logger.info("Generating .files database...")
                        files_cmd = f"repo-add --files {db_file}"
                        subprocess.run(files_cmd, shell=True, check=False)
                    
                    return True
                else:
                    self.logger.error(f"âŒ repo-add failed with exit code {result.returncode}:")
                    if result.stdout:
                        self.logger.error(f"STDOUT: {result.stdout[:500]}")
                    if result.stderr:
                        self.logger.error(f"STDERR: {result.stderr[:500]}")
                    return False
                    
            except Exception as e:
                self.logger.error(f"âŒ Database update error: {e}")
                import traceback
                traceback.print_exc()
                return False
            finally:
                os.chdir(old_cwd)
                
        except Exception as e:
            self.logger.error(f"âŒ Additive update failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _verify_database_entries(self, db_file: str) -> None:
        """Verify database entries after update"""
        try:
            list_cmd = ["tar", "-tzf", db_file]
            result = subprocess.run(list_cmd, capture_output=True, text=True, check=False)
            if result.returncode == 0:
                db_entries = [line for line in result.stdout.split('\n') if line.endswith('/desc')]
                self.logger.info(f"âœ… Database contains {len(db_entries)} package entries")
                if len(db_entries) == 0:
                    self.logger.warning("âš ï¸ Database is empty (no packages)")
                else:
                    self.logger.debug(f"Sample entries: {db_entries[:3]}")
            else:
                self.logger.warning(f"Could not list database contents: {result.stderr}")
        except Exception as e:
            self.logger.warning(f"Could not verify database: {e}")
    
    def upload_updated_files(self) -> bool:
        """
        Upload updated database and new packages to VPS
        
        Returns:
            True if successful
        """
        if not self._staging_dir or not self._staging_dir.exists():
            self.logger.error("âŒ Staging directory not found")
            return False
        
        self.logger.info("ðŸ“¤ Uploading updated files to VPS...")
        
        # Collect all files to upload
        files_to_upload = []
        
        # 1. Database files and signatures
        repo_patterns = [
            f"{self.repo_name}.db*",
            f"{self.repo_name}.files*",
        ]
        
        for pattern in repo_patterns:
            for file_path in self._staging_dir.glob(pattern):
                if file_path.stat().st_size > 0:  # Skip empty files
                    files_to_upload.append(file_path)
        
        # 2. New package files and their signatures
        for pkg_file in self._staging_dir.glob("*.pkg.tar.zst"):
            if pkg_file.stat().st_size > 0:
                files_to_upload.append(pkg_file)
                
                # Include signature if it exists
                sig_file = pkg_file.with_suffix(pkg_file.suffix + '.sig')
                if sig_file.exists() and sig_file.stat().st_size > 0:
                    files_to_upload.append(sig_file)
        
        if not files_to_upload:
            self.logger.warning("âš ï¸ No files to upload")
            return True
        
        self.logger.info(f"ðŸ“¦ Total files to upload: {len(files_to_upload)}")
        
        # Log file details
        for f in files_to_upload:
            size_mb = f.stat().st_size / (1024 * 1024)
            file_type = "PACKAGE" if ".pkg.tar.zst" in f.name else "DATABASE"
            if f.name.endswith('.sig'):
                file_type = "SIGNATURE"
            self.logger.debug(f"  - {f.name} ({size_mb:.1f}MB) [{file_type}]")
        
        # Upload using rsync
        files_list = [str(f) for f in files_to_upload]
        upload_success = self.rsync_client.upload(files_list, self._staging_dir)
        
        if upload_success:
            self.logger.info("âœ… All files uploaded successfully")
            
            # Verify upload by checking file counts
            self._verify_remote_upload(files_to_upload)
            return True
        else:
            self.logger.error("âŒ File upload failed")
            return False
    
    def _verify_remote_upload(self, expected_files: List[Path]):
        """Verify that files were uploaded successfully"""
        self.logger.info("ðŸ” Verifying remote upload...")
        
        for local_file in expected_files:
            filename = local_file.name
            remote_path = f"{self.remote_dir}/{filename}"
            
            if self.ssh_client.file_exists(remote_path):
                self.logger.debug(f"âœ… Verified: {filename}")
            else:
                self.logger.warning(f"âš ï¸ File not found on remote: {filename}")
    
    def generate_database(self) -> bool:
        """
        Legacy method for backward compatibility
        Uses the new additive update approach
        
        Returns:
            True if successful
        """
        return self.update_database_additive()
    
    def check_database_files(self) -> Tuple[List[str], List[str]]:
        """
        Check if repository database files exist on server
        
        Returns:
            Tuple of (existing_files, missing_files)
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("Checking existing database files on server")
        self.logger.info("=" * 60)
        
        db_files = [
            f"{self.repo_name}.db",
            f"{self.repo_name}.db.tar.gz",
            f"{self.repo_name}.files",
            f"{self.repo_name}.files.tar.gz"
        ]
        
        existing_files = []
        missing_files = []
        
        for db_file in db_files:
            if self.ssh_client.file_exists(f"{self.remote_dir}/{db_file}"):
                existing_files.append(db_file)
                self.logger.info(f"âœ… Database file exists: {db_file}")
            else:
                missing_files.append(db_file)
                self.logger.info(f"â„¹ï¸ Database file missing: {db_file}")
        
        if existing_files:
            self.logger.info(f"Found {len(existing_files)} database files on server")
        else:
            self.logger.info("No database files found on server")
        
        return existing_files, missing_files
    
    def get_database_files(self) -> List[Path]:
        """Get list of generated database files"""
        if not self._staging_dir:
            return []
        
        db_files = []
        patterns = [
            f"{self.repo_name}.db",
            f"{self.repo_name}.db.tar.gz",
            f"{self.repo_name}.files",
            f"{self.repo_name}.files.tar.gz",
            f"{self.repo_name}.db.sig",
            f"{self.repo_name}.db.tar.gz.sig",
            f"{self.repo_name}.files.sig",
            f"{self.repo_name}.files.tar.gz.sig",
        ]
        
        for pattern in patterns:
            for file_path in self._staging_dir.glob(pattern):
                if file_path.exists():
                    db_files.append(file_path)
        
        return db_files
    
    def cleanup_old_databases(self) -> None:
        """Clean up old database files from output directory"""
        patterns = [
            f"{self.repo_name}.db",
            f"{self.repo_name}.db.tar.gz",
            f"{self.repo_name}.files",
            f"{self.repo_name}.files.tar.gz",
        ]
        
        for pattern in patterns:
            for file_path in self.output_dir.glob(pattern):
                try:
                    if file_path.exists():
                        file_path.unlink()
                        self.logger.debug(f"Removed old database file: {file_path.name}")
                except Exception as e:
                    self.logger.warning(f"Could not remove {file_path}: {e}")
--- FILE: .github/scripts/modules/repo/cleanup_manager.py ---
"""
Cleanup manager for Zero-Residue policy
Handles surgical removal of old package versions from local and remote systems
Extracted from RepoManager with enhanced precision
"""

import os
import shutil
import subprocess
import re
import logging
from pathlib import Path
from typing import Dict, Any, List, Set, Optional, Tuple

from modules.repo.version_tracker import VersionTracker
from modules.vps.ssh_client import SSHClient
from modules.vps.rsync_client import RsyncClient


class CleanupManager:
    """Manages Zero-Residue cleanup operations for local and remote systems"""
    
    def __init__(self, config: Dict[str, Any], version_tracker: VersionTracker,
                 ssh_client: SSHClient, rsync_client: RsyncClient,
                 logger: Optional[logging.Logger] = None):
        """
        Initialize CleanupManager
        
        Args:
            config: Configuration dictionary
            version_tracker: VersionTracker instance
            ssh_client: SSHClient instance
            rsync_client: RsyncClient instance
            logger: Optional logger instance
        """
        self.config = config
        self.version_tracker = version_tracker
        self.ssh_client = ssh_client
        self.rsync_client = rsync_client
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract configuration
        self.repo_name = config.get('repo_name', '')
        self.output_dir = Path(config.get('output_dir', 'built_packages'))
        self.remote_dir = config.get('remote_dir', '')
        
        # Upload success flag for safety valve
        self._upload_successful = False
    
    def set_upload_successful(self, successful: bool):
        """Set the upload success flag for safety valve"""
        self._upload_successful = successful
    
    def purge_old_local(self, pkg_name: str, old_version: str, target_version: Optional[str] = None):
        """
        ðŸš¨ ZERO-RESIDUE POLICY: Surgical old version removal BEFORE building
        
        Removes old versions from local output directory before new build.
        
        Args:
            pkg_name: Package name
            old_version: Version to potentially delete
            target_version: Version we want to keep (None if building new)
        """
        # If we have a registered target version, use it
        if target_version is None:
            target_version = self.version_tracker.get_target_version(pkg_name)
        
        if target_version and old_version == target_version:
            # This is the version we want to keep
            self.logger.info(f"âœ… No pre-build purge needed: {pkg_name} version {old_version} is target version")
            return
        
        # Delete old version from output directory
        self._delete_specific_version_local(pkg_name, old_version)
    
    def _delete_specific_version_local(self, pkg_name: str, version_to_delete: str):
        """Delete a specific version of a package from local output_dir"""
        patterns = self._version_to_patterns(pkg_name, version_to_delete)
        deleted_count = 0
        
        for pattern in patterns:
            for old_file in self.output_dir.glob(pattern):
                try:
                    # Verify this is actually the version we want to delete
                    extracted_version = self._extract_version_from_filename(old_file.name, pkg_name)
                    if extracted_version == version_to_delete:
                        old_file.unlink()
                        self.logger.info(f"ðŸ—‘ï¸ Surgically removed local {old_file.name}")
                        deleted_count += 1
                        
                        # Also remove signature
                        sig_file = old_file.with_suffix(old_file.suffix + '.sig')
                        if sig_file.exists():
                            sig_file.unlink()
                            self.logger.info(f"ðŸ—‘ï¸ Removed local signature {sig_file.name}")
                except Exception as e:
                    self.logger.warning(f"Could not delete local {old_file}: {e}")
        
        if deleted_count > 0:
            self.logger.info(f"âœ… Removed {deleted_count} local files for {pkg_name} version {version_to_delete}")
    
    def validate_output_dir(self):
        """
        ðŸ”¥ ZOMBIE PROTECTION: Final validation before database generation
        
        Enhanced to recognize skipped packages as legitimate (not zombies)
        
        Scans output_dir and ensures:
        1. Only one version per package exists
        2. If multiple versions exist, keep only the target version
        3. Delete any "zombie" files (old versions that shouldn't be there)
        
        This is the LAST CHANCE to clean up before repo-add runs.
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ðŸš¨ FINAL VALIDATION: Removing zombie packages from output_dir")
        self.logger.info("=" * 60)
        
        # Get all package files in output_dir
        package_files = list(self.output_dir.glob("*.pkg.tar.*"))
        
        if not package_files:
            self.logger.info("â„¹ï¸ No package files in output_dir to validate")
            return
        
        self.logger.info(f"ðŸ” Validating {len(package_files)} package files in output_dir...")
        
        # Group files by package name
        packages_dict: Dict[str, List[Tuple[str, Path]]] = {}
        
        for pkg_file in package_files:
            # Extract package name and version from filename
            extracted = self._parse_package_filename(pkg_file.name)
            if extracted:
                pkg_name, version_str = extracted
                if pkg_name not in packages_dict:
                    packages_dict[pkg_name] = []
                packages_dict[pkg_name].append((version_str, pkg_file))
        
        # Process each package
        total_deleted = 0
        
        for pkg_name, files in packages_dict.items():
            if len(files) > 1:
                self.logger.warning(f"âš ï¸ Multiple versions found for {pkg_name}: {[v[0] for v in files]}")
                
                # Check if we have a registered target version
                target_version = self.version_tracker.get_target_version(pkg_name)
                
                if target_version:
                    # Keep only the target version
                    kept = False
                    for version_str, file_path in files:
                        if version_str == target_version:
                            self.logger.info(f"âœ… Keeping target version: {file_path.name} ({version_str})")
                            kept = True
                        else:
                            try:
                                file_path.unlink()
                                self.logger.info(f"ðŸ—‘ï¸ Removing non-target version: {file_path.name}")
                                total_deleted += 1
                            except Exception as e:
                                self.logger.warning(f"Could not delete {file_path}: {e}")
                    
                    if not kept:
                        self.logger.error(f"âŒ Target version {target_version} for {pkg_name} not found in output_dir!")
                else:
                    # No target version registered, keep the latest
                    self.logger.warning(f"âš ï¸ No target version registered for {pkg_name}, using version comparison")
                    latest_version = self._find_latest_version([v[0] for v in files])
                    for version_str, file_path in files:
                        if version_str == latest_version:
                            self.logger.info(f"âœ… Keeping latest version: {file_path.name} ({version_str})")
                        else:
                            try:
                                file_path.unlink()
                                self.logger.info(f"ðŸ—‘ï¸ Removing older version: {file_path.name}")
                                total_deleted += 1
                            except Exception as e:
                                self.logger.warning(f"Could not delete {file_path}: {e}")
        
        if total_deleted > 0:
            self.logger.info(f"ðŸŽ¯ Final validation: Removed {total_deleted} zombie package files")
        else:
            self.logger.info("âœ… Output_dir validation passed - no zombie packages found")
    
    def cleanup_server(self):
        """
        ðŸš¨ ZERO-RESIDUE SERVER CLEANUP: Remove zombie packages from VPS 
        using TARGET VERSIONS as SOURCE OF TRUTH.
        
        Only keeps packages that match registered target versions.
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ðŸ”’ ZERO-RESIDUE SERVER CLEANUP: Target Versions are Source of Truth")
        self.logger.info("=" * 60)
        
        # VALVE: Check if we have any target versions registered
        if not self.version_tracker.has_packages():
            self.logger.warning("âš ï¸ No target versions registered - skipping server cleanup")
            return
        
        self.logger.info(f"ðŸ”„ Zero-Residue cleanup initiated with {len(self.version_tracker.get_target_packages())} target versions")
        
        # STEP 1: Get ALL files from VPS
        vps_files = self.ssh_client.get_file_inventory()
        if not vps_files:
            self.logger.info("â„¹ï¸ No files found on VPS - nothing to clean up")
            return
        
        # Update version tracker with remote inventory
        self.version_tracker.set_remote_inventory(vps_files)
        
        # STEP 2: Determine files to delete based on target versions
        files_to_delete = self.version_tracker.get_files_to_delete()
        
        if not files_to_delete:
            self.logger.info("âœ… No zombie packages found on VPS")
            return
        
        self.logger.warning(f"ðŸš¨ Identified {len(files_to_delete)} zombie packages for deletion")
        
        # STEP 3: Execute deletion
        deleted_count = 0
        batch_size = 50
        
        for i in range(0, len(files_to_delete), batch_size):
            batch = files_to_delete[i:i + batch_size]
            if self.ssh_client.delete_remote_files(batch):
                deleted_count += len(batch)
        
        self.logger.info(f"ðŸ“Š Server cleanup complete: Deleted {deleted_count} zombie packages")
    
    def _parse_package_filename(self, filename: str) -> Optional[Tuple[str, str]]:
        """Parse package filename to extract name and version"""
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            # The package name is everything before the last 3 parts (version-release-arch)
            # or last 4 parts (epoch-version-release-arch)
            if len(parts) >= 4:
                # Try to find where package name ends
                for i in range(len(parts) - 3, 0, -1):
                    potential_name = '-'.join(parts[:i])
                    
                    # Check if remaining parts look like version-release-arch
                    remaining = parts[i:]
                    if len(remaining) >= 3:
                        # Check for epoch format (e.g., "2-26.1.9-1-x86_64")
                        if remaining[0].isdigit() and '-' in '-'.join(remaining[1:]):
                            epoch = remaining[0]
                            version_part = remaining[1]
                            release_part = remaining[2]
                            version_str = f"{epoch}:{version_part}-{release_part}"
                            return potential_name, version_str
                        # Standard format (e.g., "26.1.9-1-x86_64")
                        elif any(c.isdigit() for c in remaining[0]) and remaining[1].isdigit():
                            version_part = remaining[0]
                            release_part = remaining[1]
                            version_str = f"{version_part}-{release_part}"
                            return potential_name, version_str
        except Exception as e:
            self.logger.debug(f"Could not parse filename {filename}: {e}")
        
        return None
    
    def _version_to_patterns(self, pkg_name: str, version: str) -> List[str]:
        """Convert version string to filename patterns"""
        patterns = []
        
        if ':' in version:
            # Version with epoch: "2:26.1.9-1" -> "2-26.1.9-1-*.pkg.tar.*"
            epoch, rest = version.split(':', 1)
            patterns.append(f"{pkg_name}-{epoch}-{rest}-*.pkg.tar.*")
        else:
            # Standard version: "26.1.9-1" -> "*26.1.9-1-*.pkg.tar.*"
            patterns.append(f"{pkg_name}-{version}-*.pkg.tar.*")
        
        return patterns
    
    def _extract_version_from_filename(self, filename: str, pkg_name: str) -> Optional[str]:
        """
        Extract version from package filename
        
        Args:
            filename: Package filename (e.g., 'qownnotes-26.1.9-1-x86_64.pkg.tar.zst')
            pkg_name: Package name (e.g., 'qownnotes')
        
        Returns:
            Version string (e.g., '26.1.9-1') or None if cannot parse
        """
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            # Find where package name ends
            for i in range(len(parts) - 2, 0, -1):
                possible_name = '-'.join(parts[:i])
                if possible_name == pkg_name or possible_name.startswith(pkg_name + '-'):
                    # Remaining parts: version-release-architecture
                    if len(parts) >= i + 3:
                        version_part = parts[i]
                        release_part = parts[i+1]
                        
                        # Check for epoch (e.g., "2-26.1.9-1" -> "2:26.1.9-1")
                        if i + 2 < len(parts) and parts[i].isdigit():
                            epoch_part = parts[i]
                            version_part = parts[i+1]
                            release_part = parts[i+2]
                            return f"{epoch_part}:{version_part}-{release_part}"
                        else:
                            return f"{version_part}-{release_part}"
        except Exception as e:
            self.logger.debug(f"Could not extract version from {filename}: {e}")
        
        return None
    
    def _find_latest_version(self, versions: List[str]) -> str:
        """
        Find the latest version from a list using vercmp
        
        Args:
            versions: List of version strings
        
        Returns:
            The latest version string
        """
        if not versions:
            return ""
        
        if len(versions) == 1:
            return versions[0]
        
        # Try to use vercmp for accurate comparison
        try:
            latest = versions[0]
            for i in range(1, len(versions)):
                result = subprocess.run(
                    ['vercmp', versions[i], latest],
                    capture_output=True,
                    text=True,
                    check=False
                )
                if result.returncode == 0:
                    cmp_result = int(result.stdout.strip())
                    if cmp_result > 0:
                        latest = versions[i]
            
            return latest
        except Exception as e:
            # Fallback: use string comparison (less accurate but works for simple cases)
            self.logger.warning(f"vercmp failed, using fallback version comparison: {e}")
            return max(versions)
    
    def cleanup_temp_directories(self):
        """Clean up temporary directories"""
        temp_dirs = [
            Path(self.config.get('mirror_temp_dir', '/tmp/repo_mirror')),
            Path(self.config.get('sync_clone_dir', '/tmp/manjaro-awesome-gitclone')),
        ]
        
        for temp_dir in temp_dirs:
            if temp_dir.exists():
                try:
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    self.logger.debug(f"Cleaned up temporary directory: {temp_dir}")
                except Exception as e:
                    self.logger.warning(f"Could not clean up {temp_dir}: {e}")
--- FILE: .github/scripts/modules/vps/__init__.py ---
"""
VPS and remote operations modules
"""

# Placeholder for VPS modules - will be implemented in Phase 4
--- FILE: .github/scripts/modules/vps/ssh_client.py ---
"""
SSH client for remote VPS operations
Handles SSH connections, file operations, and remote command execution
"""

import os
import shutil
import time
import logging
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any

from modules.common.shell_executor import ShellExecutor


class SSHClient:
    """Handles SSH connections and remote operations on VPS with caching"""
    
    def __init__(self, config: Dict[str, Any], shell_executor: ShellExecutor,
                 logger: Optional[logging.Logger] = None):
        """
        Initialize SSHClient
        
        Args:
            config: Configuration dictionary with VPS settings
            shell_executor: ShellExecutor instance for command execution
            logger: Optional logger instance
        """
        self.config = config
        self.shell_executor = shell_executor
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract VPS configuration
        self.vps_user = config.get('vps_user', '')
        self.vps_host = config.get('vps_host', '')
        self.remote_dir = config.get('remote_dir', '')
        self.ssh_options = config.get('ssh_options', [])
        self.repo_name = config.get('repo_name', '')
        
        # SSH key path
        self.ssh_key_path = Path("/home/builder/.ssh/id_ed25519")
        
        # Add quiet flag to SSH options
        self.ssh_options_with_quiet = self.ssh_options + ["-q"]
        
        # Cache for remote operations
        self._remote_inventory_cache: Optional[Dict[str, str]] = None
        self._cache_timestamp: float = 0
        self._cache_ttl = 300  # 5 minutes
        
        # Pending operations
        self._pending_deletions: List[str] = []
    
    def get_cached_inventory(self, force_refresh: bool = False) -> Dict[str, str]:
        """
        Get cached remote inventory with TTL
        
        Args:
            force_refresh: If True, ignore cache and refresh
            
        Returns:
            Dictionary of {filename: full_path}
        """
        current_time = time.time()
        
        if (force_refresh or 
            not self._remote_inventory_cache or 
            current_time - self._cache_timestamp > self._cache_ttl):
            
            self.logger.info("ðŸ” Refreshing remote inventory cache...")
            self._remote_inventory_cache = self._get_remote_file_list_optimized()
            self._cache_timestamp = current_time
            self.logger.info(f"ðŸ“‹ Cache updated: {len(self._remote_inventory_cache)} files")
        else:
            cache_age = int(current_time - self._cache_timestamp)
            self.logger.debug(f"ðŸ“‹ Using cached inventory ({cache_age}s old)")
        
        return self._remote_inventory_cache.copy()
    
    def _get_remote_file_list_optimized(self) -> Dict[str, str]:
        """
        Get optimized list of package files from remote server
        
        Returns:
            Dictionary of {filename: full_path}
        """
        remote_cmd = f"cd {self.remote_dir} && ls -1 *.pkg.tar.* 2>/dev/null || echo ''"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                files = {}
                for line in result.stdout.strip().splitlines():
                    line = line.strip()
                    if line and not line.startswith("Welcome") and not line.startswith("Last login"):
                        full_path = f"{self.remote_dir}/{line}"
                        files[line] = full_path
                
                return files
            else:
                self.logger.warning(f"âš ï¸ Failed to list remote files: {result.stderr[:200]}")
                return {}
                
        except Exception as e:
            self.logger.error(f"âŒ Error listing remote files: {e}")
            return {}
    
    def batch_delete(self, file_paths: List[str], batch_size: int = 100) -> bool:
        """
        Delete multiple files in batches via single SSH session
        
        Args:
            file_paths: List of full remote paths to delete
            batch_size: Maximum files per batch to avoid command line limits
            
        Returns:
            True if all deletions successful, False otherwise
        """
        if not file_paths:
            self.logger.debug("No files to delete")
            return True
        
        self.logger.info(f"ðŸ—‘ï¸ Batch deleting {len(file_paths)} remote file(s)...")
        
        # Extract just filenames from full paths
        all_filenames = []
        for file_path in file_paths:
            filename = Path(file_path).name
            all_filenames.append(filename)
        
        # Process in batches
        success = True
        total_batches = (len(all_filenames) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            batch_start = batch_num * batch_size
            batch_end = batch_start + batch_size
            batch_filenames = all_filenames[batch_start:batch_end]
            
            batch_num_display = batch_num + 1
            
            # Build delete command with proper escaping
            filenames_str = ' '.join([f"'{f}'" for f in batch_filenames])
            remote_cmd = f"cd {self.remote_dir} && rm -f {filenames_str} && echo 'BATCH_DELETE_SUCCESS_{batch_num_display}'"
            
            # Use string command with shell=True
            ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
            
            try:
                result = self.shell_executor.run(
                    ssh_cmd,
                    capture=True,
                    check=False,
                    shell=True,
                    log_cmd=False
                )
                
                if result.returncode == 0 and f"BATCH_DELETE_SUCCESS_{batch_num_display}" in result.stdout:
                    self.logger.info(f"âœ… Batch {batch_num_display}/{total_batches}: Deleted {len(batch_filenames)} files")
                    
                    # Update cache by removing deleted files
                    if self._remote_inventory_cache:
                        for filename in batch_filenames:
                            self._remote_inventory_cache.pop(filename, None)
                else:
                    self.logger.warning(f"âš ï¸ Batch {batch_num_display} failed: {result.stderr[:200]}")
                    success = False
                    
            except Exception as e:
                self.logger.error(f"âŒ Error in batch delete {batch_num_display}: {e}")
                success = False
        
        if success:
            self.logger.info(f"âœ… All {len(file_paths)} files deleted successfully")
        else:
            self.logger.warning(f"âš ï¸ Some batch deletions failed")
        
        return success
    
    def atomic_command_sequence(self, commands: List[str], timeout: int = 60) -> Tuple[bool, str]:
        """
        Execute multiple remote commands in a single SSH session using heredoc
        
        Args:
            commands: List of shell commands to execute
            timeout: Total timeout in seconds
            
        Returns:
            Tuple of (success, combined_output)
        """
        if not commands:
            return True, "No commands to execute"
        
        self.logger.info(f"ðŸ”§ Executing {len(commands)} commands atomically...")
        
        # Build heredoc script
        heredoc_script = "set -e\n"  # Exit on error
        heredoc_script += f"cd {self.remote_dir}\n"
        
        for i, cmd in enumerate(commands):
            heredoc_script += f"echo '>>> COMMAND {i+1}: {cmd}'\n"
            heredoc_script += f"{cmd}\n"
            heredoc_script += "echo '>>> SUCCESS'\n"
        
        heredoc_script += "echo 'ATOMIC_SEQUENCE_COMPLETE'\n"
        
        # Escape for SSH
        escaped_script = heredoc_script.replace('"', '\\"').replace('$', '\\$')
        
        # Use heredoc via bash -s
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"bash -s\" << 'EOF'\n{heredoc_script}\nEOF"
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                timeout=timeout,
                shell=True,
                log_cmd=False
            )
            
            combined_output = ""
            if result.stdout:
                combined_output += result.stdout
            
            if result.returncode == 0 and "ATOMIC_SEQUENCE_COMPLETE" in result.stdout:
                self.logger.info("âœ… Atomic command sequence completed successfully")
                return True, combined_output
            else:
                error_msg = result.stderr[:500] if result.stderr else "No error output"
                self.logger.error(f"âŒ Atomic command sequence failed: {error_msg}")
                return False, combined_output + f"\nERROR: {error_msg}"
                
        except Exception as e:
            self.logger.error(f"âŒ Atomic command sequence exception: {e}")
            return False, str(e)
    
    def queue_deletion(self, remote_path: str):
        """
        Queue a file for batch deletion
        
        Args:
            remote_path: Full remote path to delete
        """
        self._pending_deletions.append(remote_path)
    
    def commit_queued_deletions(self) -> bool:
        """
        Execute all queued deletions
        
        Returns:
            True if successful
        """
        if not self._pending_deletions:
            return True
        
        self.logger.info(f"ðŸ”§ Committing {len(self._pending_deletions)} queued deletions...")
        success = self.batch_delete(self._pending_deletions)
        
        if success:
            self._pending_deletions.clear()
        
        return success
    
    def clear_cache(self):
        """Clear the remote inventory cache"""
        self._remote_inventory_cache = None
        self._cache_timestamp = 0
        self.logger.debug("Cleared remote inventory cache")
    
    def invalidate_cache_for_file(self, filename: str):
        """Remove a specific file from cache"""
        if self._remote_inventory_cache and filename in self._remote_inventory_cache:
            del self._remote_inventory_cache[filename]
            self.logger.debug(f"Invalidated cache entry for: {filename}")
    
    # Existing methods remain unchanged but use cache where appropriate
    
    def delete_remote_files(self, file_list: List[str]) -> bool:
        """Legacy method - delegates to batch_delete"""
        return self.batch_delete(file_list)
    
    def setup_ssh_config(self, ssh_key: Optional[str] = None) -> bool:
        """
        Setup SSH config file for builder user
        
        Args:
            ssh_key: Optional SSH private key content
        
        Returns:
            True if setup successful
        """
        try:
            ssh_dir = Path("/home/builder/.ssh")
            ssh_dir.mkdir(exist_ok=True, mode=0o700)
            
            # Write SSH config file
            config_content = f"""Host {self.vps_host}
  HostName {self.vps_host}
  User {self.vps_user}
  IdentityFile ~/.ssh/id_ed25519
  StrictHostKeyChecking no
  ConnectTimeout 30
  ServerAliveInterval 15
  ServerAliveCountMax 3
"""
            
            config_file = ssh_dir / "config"
            with open(config_file, "w") as f:
                f.write(config_content)
            
            config_file.chmod(0o600)
            
            # Ensure SSH key exists and has correct permissions
            if not self.ssh_key_path.exists() and ssh_key:
                with open(self.ssh_key_path, "w") as f:
                    f.write(ssh_key)
                self.ssh_key_path.chmod(0o600)
            
            # Set ownership to builder
            try:
                shutil.chown(ssh_dir, "builder", "builder")
                for item in ssh_dir.iterdir():
                    shutil.chown(item, "builder", "builder")
            except Exception as e:
                self.logger.warning(f"Could not change SSH dir ownership: {e}")
            
            self.logger.info("âœ… SSH configuration setup complete")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ SSH configuration failed: {e}")
            return False
    
    def test_connection(self) -> bool:
        """
        Test SSH connection to VPS
        
        Returns:
            True if connection successful
        """
        self.logger.info("ðŸ” Testing SSH connection to VPS...")
        
        ssh_test_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"cd {self.remote_dir} && echo SSH_TEST_SUCCESS\""
        
        try:
            result = self.shell_executor.run(
                ssh_test_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result and result.returncode == 0 and "SSH_TEST_SUCCESS" in result.stdout:
                self.logger.info("âœ… SSH connection successful")
                return True
            else:
                error_msg = result.stderr[:100] if result and result.stderr else 'No output'
                self.logger.warning(f"âš ï¸ SSH connection failed: {error_msg}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ SSH test exception: {e}")
            return False
    
    def get_remote_file_list(self) -> List[str]:
        """
        Get explicit list of package files from remote server (legacy)
        
        Returns:
            List of package filenames
        """
        inventory = self.get_cached_inventory()
        return list(inventory.values())
    
    def file_exists(self, remote_path: str) -> bool:
        """
        Check if a file exists on remote server
        
        Args:
            remote_path: Full remote path to check
        
        Returns:
            True if file exists
        """
        # Extract filename from path
        filename = Path(remote_path).name
        
        # Use cache first
        inventory = self.get_cached_inventory()
        if filename in inventory:
            return True
        
        # Fallback to direct check
        remote_cmd = f"cd {self.remote_dir} && test -f \"{filename}\" && echo EXISTS || echo NOT_EXISTS"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0 and "EXISTS" in result.stdout:
                # Update cache
                if self._remote_inventory_cache is not None:
                    self._remote_inventory_cache[filename] = remote_path
                return True
            return False
        except Exception as e:
            self.logger.warning(f"Could not check file existence {remote_path}: {e}")
            return False
    
    def get_remote_hash(self, remote_path: str) -> Optional[str]:
        """
        Get SHA256 hash of remote file
        
        Args:
            remote_path: Full remote path to file
        
        Returns:
            SHA256 hash string or None if failed
        """
        # Extract filename from path
        filename = Path(remote_path).name
        
        remote_cmd = f"cd {self.remote_dir} && sha256sum \"{filename}\" 2>/dev/null | cut -d' ' -f1"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0 and result.stdout.strip():
                hash_value = result.stdout.strip()
                if len(hash_value) == 64:  # SHA256 hash length
                    return hash_value
                else:
                    self.logger.warning(f"Invalid hash format for {remote_path}")
            return None
        except Exception as e:
            self.logger.warning(f"Could not get hash for {remote_path}: {e}")
            return None
    
    def ensure_directory(self) -> bool:
        """
        Ensure remote directory exists and has correct permissions
        
        Returns:
            True if directory exists or was created successfully
        """
        self.logger.info("ðŸ”§ Ensuring remote directory exists...")
        
        remote_cmd = f"""
        # Check if directory exists
        if [ ! -d "{self.remote_dir}" ]; then
            echo "Creating directory {self.remote_dir}"
            sudo mkdir -p "{self.remote_dir}"
            sudo chown -R {self.vps_user}:www-data "{self.remote_dir}"
            sudo chmod -R 755 "{self.remote_dir}"
            echo "âœ… Directory created and permissions set"
        else
            echo "âœ… Directory exists"
            # Ensure correct permissions
            sudo chown -R {self.vps_user}:www-data "{self.remote_dir}"
            sudo chmod -R 755 "{self.remote_dir}"
            echo "âœ… Permissions verified"
        fi
        """
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                self.logger.info("âœ… Remote directory verified")
                for line in result.stdout.splitlines():
                    if line.strip():
                        self.logger.debug(f"REMOTE DIR: {line}")
                return True
            else:
                self.logger.warning(f"âš ï¸ Could not ensure remote directory: {result.stderr[:200]}")
                return False
                
        except Exception as e:
            self.logger.warning(f"Could not ensure remote directory: {e}")
            return False
    
    def check_repository_exists(self) -> Tuple[bool, bool]:
        """
        Check if repository exists on VPS via SSH
        
        Returns:
            Tuple of (exists, has_packages)
        """
        self.logger.info("ðŸ” Checking if repository exists on VPS...")
        
        remote_cmd = f"""
        # Check for package files
        if cd "{self.remote_dir}" && find . -maxdepth 1 -name "*.pkg.tar.*" -type f 2>/dev/null | head -1 >/dev/null; then
            echo "REPO_EXISTS_WITH_PACKAGES"
        # Check for database files
        elif cd "{self.remote_dir}" && ([ -f "{self.repo_name}.db.tar.gz" ] || [ -f "{self.repo_name}.db" ]); then
            echo "REPO_EXISTS_WITH_DB"
        else
            echo "REPO_NOT_FOUND"
        fi
        """
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                timeout=30,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                output = result.stdout.strip()
                if "REPO_EXISTS_WITH_PACKAGES" in output:
                    self.logger.info("âœ… Repository exists on VPS (has package files)")
                    return True, True
                elif "REPO_EXISTS_WITH_DB" in output:
                    self.logger.info("âœ… Repository exists on VPS (has database)")
                    return True, False
                else:
                    self.logger.info("â„¹ï¸ Repository does not exist on VPS (first run)")
                    return False, False
            else:
                self.logger.warning(f"âš ï¸ Could not check repository existence: {result.stderr[:200]}")
                return False, False
                
        except Exception as e:
            self.logger.error(f"âŒ Error checking repository: {e}")
            return False, False
    
    def list_remote_files(self, pattern: str = "*.pkg.tar.*") -> List[str]:
        """
        List files on remote server matching pattern
        
        Args:
            pattern: File pattern to match
        
        Returns:
            List of remote file paths
        """
        if pattern == "*.pkg.tar.*":
            # Use cached inventory for common pattern
            inventory = self.get_cached_inventory()
            return list(inventory.values())
        
        # For other patterns, fall back to direct query
        remote_cmd = f"cd {self.remote_dir} && find . -maxdepth 1 -type f -name '{pattern}' 2>/dev/null | sed 's|^\./||'"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        self.logger.info(f"Listing remote files with pattern: {pattern}")
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                files = []
                for line in result.stdout.strip().splitlines():
                    line = line.strip()
                    if line and not line.startswith("Welcome") and not line.startswith("Last login"):
                        full_path = f"{self.remote_dir}/{line}"
                        files.append(full_path)
                
                self.logger.info(f"âœ… Found {len(files)} remote files")
                return files
            else:
                self.logger.warning(f"âš ï¸ Failed to list remote files: {result.stderr[:200]}")
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ Error listing remote files: {e}")
            return []
    
    def execute_remote_command(self, command: str, timeout: int = 30) -> Tuple[bool, str]:
        """
        Execute a command on remote server
        
        Args:
            command: Command to execute
            timeout: Command timeout in seconds
        
        Returns:
            Tuple of (success, output)
        """
        # Prepend cd to remote_dir
        remote_cmd = f"cd {self.remote_dir} && {command}"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                timeout=timeout,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                return True, result.stdout.strip()
            else:
                return False, result.stderr.strip()
                
        except Exception as e:
            self.logger.error(f"âŒ Remote command execution failed: {e}")
            return False, str(e)
    
    def debug_remote_directory(self) -> bool:
        """
        Debug: List remote directory contents with full details
        
        Returns:
            True if command executed successfully
        """
        self.logger.info("ðŸ” DEBUG: Listing remote directory contents...")
        
        remote_cmd = f"cd {self.remote_dir} && pwd && echo '=== DIRECTORY CONTENTS ===' && ls -la && echo '=== PACKAGE FILES ===' && ls -la *.pkg.tar.* 2>/dev/null || echo 'No package files found'"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        self.logger.debug(f"DEBUG COMMAND: {ssh_cmd}")
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            self.logger.info("[DEBUG] REMOTE DIR CONTENT:")
            self.logger.info("=" * 60)
            if result.stdout:
                for line in result.stdout.strip().splitlines():
                    self.logger.info(f"[DEBUG] {line}")
            self.logger.info("=" * 60)
            
            if result.stderr:
                self.logger.warning(f"[DEBUG] STDERR: {result.stderr[:200]}")
            
            return result.returncode == 0
        except Exception as e:
            self.logger.error(f"[DEBUG] Error listing remote directory: {e}")
            return False
--- FILE: .github/scripts/modules/vps/rsync_client.py ---
"""
Rsync client for efficient file transfer between local and remote systems
Handles uploads, downloads, and mirroring operations
"""

import os
import shutil
import time
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any

from modules.common.shell_executor import ShellExecutor


class RsyncClient:
    """Handles rsync operations for file transfer between local and remote"""
    
    def __init__(self, config: Dict[str, Any], shell_executor: ShellExecutor,
                 logger: Optional[logging.Logger] = None):
        """
        Initialize RsyncClient
        
        Args:
            config: Configuration dictionary with VPS settings
            shell_executor: ShellExecutor instance for command execution
            logger: Optional logger instance
        """
        self.config = config
        self.shell_executor = shell_executor
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract VPS configuration
        self.vps_user = config.get('vps_user', '')
        self.vps_host = config.get('vps_host', '')
        self.remote_dir = config.get('remote_dir', '')
        self.ssh_options = config.get('ssh_options', [])
        
        # Add quiet flag to SSH options
        self.ssh_options_with_quiet = self.ssh_options + ["-q"]
        
        # Build SSH options string for rsync
        self.ssh_opts_str = ' '.join(self.ssh_options_with_quiet)
    
    def upload(self, local_files: List[str], local_base_dir: Optional[Path] = None) -> bool:
        """
        Upload files to server using RSYNC WITHOUT --delete flag
        
        Args:
            local_files: List of local file paths to upload
            local_base_dir: Base directory for relative paths
        
        Returns:
            True if upload successful
        """
        if not local_files:
            self.logger.warning("No files to upload")
            return False
        
        # Log files to upload (safe - only filenames, not paths)
        self.logger.info(f"Files to upload ({len(local_files)}):")
        for file_path in local_files:
            try:
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                filename = os.path.basename(file_path)
                file_type = "PACKAGE"
                if self.config.get('repo_name', '') in filename:
                    file_type = "DATABASE" if not file_path.endswith('.sig') else "SIGNATURE"
                self.logger.info(f"  - {filename} ({size_mb:.1f}MB) [{file_type}]")
            except Exception:
                self.logger.info(f"  - {os.path.basename(file_path)} [UNKNOWN SIZE]")
        
        # Build RSYNC command WITHOUT --delete
        rsync_cmd = self._build_rsync_command(local_files, local_base_dir, delete=False)
        
        self.logger.info(f"RUNNING RSYNC COMMAND WITHOUT --delete:")
        self.logger.info(rsync_cmd.strip())
        
        # FIRST ATTEMPT
        start_time = time.time()
        
        try:
            result = self.shell_executor.run(
                rsync_cmd,
                shell=True,
                capture=True,
                check=False,
                log_cmd=True
            )
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            self.logger.info(f"EXIT CODE (attempt 1): {result.returncode}")
            
            if result.returncode == 0:
                self._log_rsync_output(result, "RSYNC")
                self.logger.info(f"âœ… RSYNC upload successful! ({duration} seconds)")
                return True
            else:
                self.logger.warning(f"âš ï¸ First RSYNC attempt failed (code: {result.returncode})")
                
        except Exception as e:
            self.logger.error(f"RSYNC execution error: {e}")
        
        # SECOND ATTEMPT (with different SSH options)
        self.logger.info("âš ï¸ Retrying with different SSH options...")
        time.sleep(5)
        
        rsync_cmd_retry = self._build_rsync_command(
            local_files, 
            local_base_dir, 
            delete=False,
            enhanced_ssh=True
        )
        
        self.logger.info(f"RUNNING RSYNC RETRY COMMAND WITHOUT --delete:")
        self.logger.info(rsync_cmd_retry.strip())
        
        start_time = time.time()
        
        try:
            result = self.shell_executor.run(
                rsync_cmd_retry,
                shell=True,
                capture=True,
                check=False,
                log_cmd=True
            )
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            self.logger.info(f"EXIT CODE (attempt 2): {result.returncode}")
            
            if result.returncode == 0:
                self._log_rsync_output(result, "RSYNC RETRY")
                self.logger.info(f"âœ… RSYNC upload successful on retry! ({duration} seconds)")
                return True
            else:
                self.logger.error(f"âŒ RSYNC upload failed on both attempts!")
                return False
                
        except Exception as e:
            self.logger.error(f"RSYNC retry execution error: {e}")
            return False
    
    def _build_rsync_command(self, local_files: List[str], local_base_dir: Optional[Path],
                            delete: bool = False, enhanced_ssh: bool = False) -> str:
        """
        Build rsync command with syntax safety (ALWAYS prefixing with ./)
        
        Args:
            local_files: List of local file paths
            local_base_dir: Base directory for relative paths
            delete: Whether to add --delete flag
            enhanced_ssh: Whether to use enhanced SSH options
        
        Returns:
            Rsync command string
        """
        # Build file list with safety prefix - ALWAYS use ./ prefix
        if local_base_dir:
            # Use relative paths from base directory with ./ prefix
            file_args = []
            for file_path in local_files:
                rel_path = os.path.relpath(file_path, local_base_dir)
                # ALWAYS prefix with ./ to ensure rsync treats it as local file
                safe_path = f"./{rel_path}"
                file_args.append(f"'{safe_path}'")
        else:
            # Use absolute paths but extract filename and prefix with ./
            file_args = []
            for file_path in local_files:
                # Extract just the filename and ALWAYS prefix with ./
                filename = os.path.basename(file_path)
                safe_path = f"./{filename}"
                file_args.append(f"'{safe_path}'")
        
        file_args_str = ' '.join(file_args)
        
        # Build SSH options
        if enhanced_ssh:
            ssh_opts = '-e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=60 -o ServerAliveInterval=30 -o ServerAliveCountMax=3 -q"'
        else:
            ssh_opts = f'-e "ssh {self.ssh_opts_str}"' if self.ssh_opts_str else '-q'
        
        # Build delete flag
        delete_flag = '--delete' if delete else ''
        
        # Build command
        if local_base_dir:
            # Change to base directory and use relative paths with ./ prefix
            cmd = f"cd '{local_base_dir}' && rsync -avzq --progress --stats {delete_flag} {ssh_opts} {file_args_str} '{self.vps_user}@{self.vps_host}:{self.remote_dir}/'"
        else:
            # Use current directory and ./ prefix
            cmd = f"rsync -avzq --progress --stats {delete_flag} {ssh_opts} {file_args_str} '{self.vps_user}@{self.vps_host}:{self.remote_dir}/'"
        
        return cmd.strip()
    
    def mirror_remote(self, remote_pattern: str, local_dir: Path, 
                      temp_dir: Optional[Path] = None) -> bool:
        """
        Download remote files to local directory (mirror)
        
        Args:
            remote_pattern: Remote file pattern to download
            local_dir: Local directory to save files
            temp_dir: Temporary directory for download (optional)
        
        Returns:
            True if mirror successful
        """
        if temp_dir is None:
            temp_dir = Path("/tmp/repo_mirror")
        
        # Create temporary local repository directory
        if temp_dir.exists():
            shutil.rmtree(temp_dir, ignore_errors=True)
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"Created local mirror directory: {temp_dir}")
        
        # Use rsync to download files from server
        self.logger.info(f"ðŸ“¥ Downloading remote files to local mirror...")
        
        # Build rsync command for mirroring with safety prefix
        rsync_cmd = f"rsync -avzq --progress --stats -e \"ssh -o StrictHostKeyChecking=no -o ConnectTimeout=60 -q\" '{self.vps_user}@{self.vps_host}:{self.remote_dir}/{remote_pattern}' './' 2>/dev/null || true"
        
        self.logger.info(f"RUNNING RSYNC MIRROR COMMAND:")
        self.logger.info(rsync_cmd.strip())
        
        start_time = time.time()
        
        # Change to temp directory for safe rsync
        old_cwd = os.getcwd()
        os.chdir(temp_dir)
        
        try:
            result = self.shell_executor.run(
                rsync_cmd,
                shell=True,
                capture=True,
                text=True,
                check=False,
                log_cmd=True
            )
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            self.logger.info(f"EXIT CODE: {result.returncode}")
            self._log_rsync_output(result, "RSYNC MIRROR")
            
            # List downloaded files
            downloaded_files = list(temp_dir.glob("*"))
            file_count = len(downloaded_files)
            
            if file_count > 0:
                self.logger.info(f"âœ… Successfully mirrored {file_count} files ({duration} seconds)")
                self.logger.info(f"Sample mirrored files: {[f.name for f in downloaded_files[:5]]}")
                
                # Verify file integrity and copy to target directory
                valid_files = []
                for file_path in downloaded_files:
                    if file_path.stat().st_size > 0:
                        valid_files.append(file_path)
                    else:
                        self.logger.warning(f"âš ï¸ Empty file: {file_path.name}")
                
                self.logger.info(f"Valid mirrored files: {len(valid_files)}/{file_count}")
                
                # Copy mirrored files to target directory
                self.logger.info(f"ðŸ“‹ Copying {len(valid_files)} mirrored files to target directory...")
                copied_count = 0
                for file_path in valid_files:
                    dest = local_dir / file_path.name
                    if not dest.exists():  # Don't overwrite existing files
                        shutil.copy2(file_path, dest)
                        copied_count += 1
                
                self.logger.info(f"Copied {copied_count} mirrored files to target directory")
                
                # Clean up temporary directory
                shutil.rmtree(temp_dir, ignore_errors=True)
                
                return True
            else:
                self.logger.info("â„¹ï¸ No files were mirrored")
                shutil.rmtree(temp_dir, ignore_errors=True)
                return True
                
        except Exception as e:
            self.logger.error(f"RSYNC mirror execution error: {e}")
            if temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)
            return False
        finally:
            os.chdir(old_cwd)
    
    def _log_rsync_output(self, result: Any, prefix: str = "RSYNC") -> None:
        """Log rsync output"""
        if result.stdout:
            for line in result.stdout.splitlines():
                if line.strip():
                    self.logger.debug(f"{prefix}: {line}")
        if result.stderr:
            for line in result.stderr.splitlines():
                if line.strip() and "No such file or directory" not in line:
                    self.logger.error(f"{prefix} ERR: {line}")
    
    def sync_directories(self, local_dir: Path, remote_subdir: str = "", 
                        delete: bool = False) -> bool:
        """
        Sync entire directories with safety prefix
        
        Args:
            local_dir: Local directory to sync
            remote_subdir: Remote subdirectory (optional)
            delete: Whether to delete extra files on remote
        
        Returns:
            True if sync successful
        """
        remote_target = f"{self.remote_dir}/{remote_subdir}" if remote_subdir else self.remote_dir
        
        # Build rsync command for directory sync with safety prefix
        delete_flag = '--delete' if delete else ''
        
        # Use ./ prefix for directory sync
        rsync_cmd = f"rsync -avzq --progress --stats {delete_flag} -e \"ssh {self.ssh_opts_str}\" './' '{self.vps_user}@{self.vps_host}:{remote_target}/'"
        
        self.logger.info(f"Syncing directory {local_dir} to {remote_target}")
        self.logger.debug(f"RSYNC command: {rsync_cmd.strip()}")
        
        # Change to local directory before running rsync
        old_cwd = os.getcwd()
        os.chdir(local_dir)
        
        try:
            result = self.shell_executor.run(
                rsync_cmd,
                shell=True,
                capture=True,
                check=False,
                log_cmd=True
            )
            
            if result.returncode == 0:
                self.logger.info("âœ… Directory sync successful")
                return True
            else:
                self.logger.error(f"âŒ Directory sync failed: {result.stderr[:500]}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Directory sync error: {e}")
            return False
        finally:
            os.chdir(old_cwd)
--- FILE: .github/scripts/modules/orchestrator/__init__.py ---
"""
Orchestration and workflow management
"""

# Placeholder for orchestrator modules - will be implemented in Phase 5
--- FILE: .github/scripts/modules/orchestrator/state.py ---
"""
Build state tracking for package builder system
Tracks built, skipped, and failed packages with comprehensive statistics
"""

import time
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime
import logging


@dataclass
class PackageInfo:
    """Information about a specific package"""
    name: str
    version: str
    is_aur: bool
    timestamp: float = field(default_factory=time.time)
    build_duration: Optional[float] = None
    success: bool = True
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        return {
            'name': self.name,
            'version': self.version,
            'is_aur': self.is_aur,
            'timestamp': self.timestamp,
            'build_duration': self.build_duration,
            'success': self.success,
            'error_message': self.error_message
        }


class BuildState:
    """Tracks build state, statistics, and package outcomes"""
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        Initialize BuildState
        
        Args:
            logger: Optional logger instance
        """
        self.logger = logger or logging.getLogger(__name__)
        
        # Package tracking
        self._built_packages: List[PackageInfo] = []
        self._skipped_packages: List[PackageInfo] = []
        self._failed_packages: List[PackageInfo] = []
        
        # Statistics
        self.start_time = time.time()
        self.end_time: Optional[float] = None
        self.total_packages = 0
        
        # Detailed statistics
        self.stats = {
            'aur_success': 0,
            'local_success': 0,
            'aur_failed': 0,
            'local_failed': 0,
            'aur_skipped': 0,
            'local_skipped': 0,
        }
    
    def add_built(self, pkg_name: str, version: str, is_aur: bool = False, 
                  build_duration: Optional[float] = None) -> None:
        """
        Add a successfully built package
        
        Args:
            pkg_name: Package name
            version: Package version
            is_aur: Whether it's an AUR package
            build_duration: Build duration in seconds
        """
        pkg_info = PackageInfo(
            name=pkg_name,
            version=version,
            is_aur=is_aur,
            build_duration=build_duration,
            success=True
        )
        
        self._built_packages.append(pkg_info)
        
        if is_aur:
            self.stats['aur_success'] += 1
        else:
            self.stats['local_success'] += 1
        
        self.total_packages += 1
        self.logger.info(f"âœ… Built package registered: {pkg_name} ({version})")
    
    def add_skipped(self, pkg_name: str, version: str, is_aur: bool = False, 
                    reason: str = "up-to-date") -> None:
        """
        Add a skipped package
        
        Args:
            pkg_name: Package name
            version: Package version
            is_aur: Whether it's an AUR package
            reason: Reason for skipping
        """
        pkg_info = PackageInfo(
            name=pkg_name,
            version=version,
            is_aur=is_aur,
            success=True,
            error_message=f"Skipped: {reason}"
        )
        
        self._skipped_packages.append(pkg_info)
        
        if is_aur:
            self.stats['aur_skipped'] += 1
        else:
            self.stats['local_skipped'] += 1
        
        self.total_packages += 1
        self.logger.info(f"â­ï¸ Skipped package registered: {pkg_name} ({version}) - {reason}")
    
    def add_failed(self, pkg_name: str, version: str, is_aur: bool = False, 
                   error_message: str = "Build failed") -> None:
        """
        Add a failed package
        
        Args:
            pkg_name: Package name
            version: Package version
            is_aur: Whether it's an AUR package
            error_message: Error description
        """
        pkg_info = PackageInfo(
            name=pkg_name,
            version=version,
            is_aur=is_aur,
            success=False,
            error_message=error_message
        )
        
        self._failed_packages.append(pkg_info)
        
        if is_aur:
            self.stats['aur_failed'] += 1
        else:
            self.stats['local_failed'] += 1
        
        self.total_packages += 1
        self.logger.error(f"âŒ Failed package registered: {pkg_name} - {error_message}")
    
    def mark_complete(self) -> None:
        """Mark build as complete and calculate final statistics"""
        self.end_time = time.time()
    
    def get_duration(self) -> float:
        """Get total build duration in seconds"""
        end = self.end_time or time.time()
        return end - self.start_time
    
    def get_summary(self) -> Dict[str, Any]:
        """Get comprehensive build summary"""
        return {
            'total_packages': self.total_packages,
            'built': len(self._built_packages),
            'skipped': len(self._skipped_packages),
            'failed': len(self._failed_packages),
            'aur_success': self.stats['aur_success'],
            'local_success': self.stats['local_success'],
            'aur_failed': self.stats['aur_failed'],
            'local_failed': self.stats['local_failed'],
            'aur_skipped': self.stats['aur_skipped'],
            'local_skipped': self.stats['local_skipped'],
            'duration_seconds': self.get_duration(),
            'start_time': datetime.fromtimestamp(self.start_time).isoformat(),
            'end_time': datetime.fromtimestamp(self.end_time).isoformat() if self.end_time else None,
            'success_rate': (len(self._built_packages) / self.total_packages * 100) if self.total_packages > 0 else 0,
        }
    
    def get_built_packages(self) -> List[Dict[str, Any]]:
        """Get list of built packages as dictionaries"""
        return [pkg.to_dict() for pkg in self._built_packages]
    
    def get_skipped_packages(self) -> List[Dict[str, Any]]:
        """Get list of skipped packages as dictionaries"""
        return [pkg.to_dict() for pkg in self._skipped_packages]
    
    def get_failed_packages(self) -> List[Dict[str, Any]]:
        """Get list of failed packages as dictionaries"""
        return [pkg.to_dict() for pkg in self._failed_packages]
    
    def get_all_packages(self) -> List[Dict[str, Any]]:
        """Get all packages combined"""
        return (
            self.get_built_packages() +
            self.get_skipped_packages() +
            self.get_failed_packages()
        )
    
    def reset(self) -> None:
        """Reset build state for a new build"""
        self._built_packages.clear()
        self._skipped_packages.clear()
        self._failed_packages.clear()
        
        self.start_time = time.time()
        self.end_time = None
        self.total_packages = 0
        
        for key in self.stats:
            self.stats[key] = 0
        
        self.logger.info("Build state reset for new build")
--- FILE: .github/scripts/modules/orchestrator/package_builder.py ---
"""
Manjaro Package Builder - Complete Build System with PKGBUILD Synchronization
Implements temporary clone method with tracking and self-healing database
"""

import os
import sys
import time
import re
import logging
import shutil
import subprocess
import tempfile
import glob
import json
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Set
from datetime import datetime

# Common utilities
from modules.common.logging_utils import setup_logging, get_logger
from modules.common.config_loader import ConfigLoader
from modules.common.environment import EnvironmentValidator
from modules.common.shell_executor import ShellExecutor

# State management
from modules.orchestrator.state import BuildState
from modules.repo.version_tracker import VersionTracker

# VPS communication
from modules.vps.ssh_client import SSHClient
from modules.vps.rsync_client import RsyncClient

# Build logic
from modules.build.version_manager import VersionManager
from modules.build.aur_builder import AURBuilder
from modules.build.local_builder import LocalBuilder

# Repository management
from modules.repo.database_manager import DatabaseManager
from modules.repo.cleanup_manager import CleanupManager

# GPG handling
from modules.gpg.gpg_handler import GPGHandler


class PackageBuilder:
    """Main orchestrator - Complete Build System with PKGBUILD Sync"""
    
    def __init__(self):
        """Initialize PackageBuilder with complete build system"""
        self.logger = get_logger(__name__)
        
        # Phase 1: Common utilities
        self.env_validator = EnvironmentValidator(self.logger)
        self.env_validator.validate()
        
        self.repo_root = self.env_validator.get_repo_root()
        self.config_loader = ConfigLoader(self.repo_root, self.logger)
        self.config = self.config_loader.load_config()
        
        # Setup logging with debug mode from config
        debug_mode = self.config.get('debug_mode', False)
        setup_logging(debug_mode=debug_mode)
        
        # Shell executor with debug mode
        self.shell_executor = ShellExecutor(
            debug_mode=debug_mode,
            default_timeout=1800
        )
        
        # CRITICAL FIX: Run pacman -Sy BEFORE any operations
        self._sync_pacman_databases_initial()
        
        # Phase 2: State management and VPS communication
        self.build_state = BuildState(self.logger)
        
        # VPS clients
        self.ssh_client = SSHClient(self.config, self.shell_executor, self.logger)
        self.rsync_client = RsyncClient(self.config, self.shell_executor, self.logger)
        
        # Setup SSH configuration for VPS
        vps_ssh_key = os.getenv('VPS_SSH_KEY', '')
        self.ssh_client.setup_ssh_config(vps_ssh_key)
        
        # Git SSH setup for CI_PUSH_SSH_KEY
        self._git_ssh_temp_key: Optional[Path] = None
        self._git_ssh_env: Optional[Dict[str, str]] = None
        
        # Version tracker with JSON state
        self.version_tracker = VersionTracker(
            repo_root=self.repo_root,
            ssh_client=self.ssh_client,
            logger=self.logger
        )
        
        # Phase 3: Build and repository logic
        self.version_manager = VersionManager(self.shell_executor, self.logger)
        
        # Builders (will be used only when server says we need to build)
        self.aur_builder = AURBuilder(
            config=self.config,
            shell_executor=self.shell_executor,
            version_manager=self.version_manager,
            version_tracker=self.version_tracker,
            build_state=self.build_state,
            logger=self.logger
        )
        
        # Repository managers
        self.database_manager = DatabaseManager(
            config=self.config,
            ssh_client=self.ssh_client,
            rsync_client=self.rsync_client,
            logger=self.logger
        )
        
        self.cleanup_manager = CleanupManager(
            config=self.config,
            version_tracker=self.version_tracker,
            ssh_client=self.ssh_client,
            rsync_client=self.rsync_client,
            logger=self.logger
        )
        
        # GPG handler
        self.gpg_handler = GPGHandler()
        
        # Package lists
        self.local_packages: List[str] = []
        self.aur_packages: List[str] = []
        
        # Track sanitized artifacts
        self._sanitized_files: Dict[str, str] = {}
        
        # Local staging directory for database operations
        self._staging_dir: Optional[Path] = None
        
        # Temporary clone directory
        self._temp_clone_dir: Optional[Path] = None
        
        # Build tracking directory in temp clone
        self._build_tracking_dir: Optional[Path] = None
        
        # Auto-recovery state
        self._recovered_packages: List[str] = []
        self._missing_from_db: List[str] = []
        
        self.logger.info("âœ… PackageBuilder initialized with COMPLETE BUILD SYSTEM")
    
    def _setup_git_ssh(self) -> bool:
        """
        Setup Git SSH authentication using CI_PUSH_SSH_KEY
        
        Returns:
            True if successful
        """
        ci_push_ssh_key = os.getenv('CI_PUSH_SSH_KEY')
        if not ci_push_ssh_key:
            self.logger.error("âŒ CI_PUSH_SSH_KEY environment variable not set")
            return False
        
        try:
            # Create temporary directory for SSH key
            temp_ssh_dir = Path(tempfile.mkdtemp(prefix="git_ssh_"))
            self._git_ssh_temp_key = temp_ssh_dir / "id_ed25519"
            
            # Write SSH key to file
            with open(self._git_ssh_temp_key, 'w') as f:
                f.write(ci_push_ssh_key)
            
            # Set proper permissions
            self._git_ssh_temp_key.chmod(0o600)
            
            # Create known_hosts with github.com key
            known_hosts = temp_ssh_dir / "known_hosts"
            ssh_keyscan_cmd = [
                "ssh-keyscan",
                "-t", "ed25519",
                "github.com"
            ]
            
            result = subprocess.run(
                ssh_keyscan_cmd,
                capture_output=True,
                text=True,
                check=True
            )
            
            if result.returncode == 0 and result.stdout:
                with open(known_hosts, 'w') as f:
                    f.write(result.stdout)
                known_hosts.chmod(0o644)
            else:
                # Fallback: accept host key without verification
                with open(known_hosts, 'w') as f:
                    f.write("github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl")
            
            # Create Git SSH command
            git_ssh_cmd = (
                f"ssh -o IdentitiesOnly=yes "
                f"-o IdentityFile={self._git_ssh_temp_key} "
                f"-o UserKnownHostsFile={known_hosts} "
                f"-o StrictHostKeyChecking=yes "
                f"-o ConnectTimeout=30"
            )
            
            self._git_ssh_env = os.environ.copy()
            self._git_ssh_env['GIT_SSH_COMMAND'] = git_ssh_cmd
            
            self.logger.info("âœ… Git SSH authentication configured")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to setup Git SSH: {e}")
            return False
    
    def _cleanup_git_ssh(self):
        """Clean up temporary Git SSH key"""
        if self._git_ssh_temp_key and self._git_ssh_temp_key.exists():
            try:
                temp_dir = self._git_ssh_temp_key.parent
                shutil.rmtree(temp_dir, ignore_errors=True)
                self._git_ssh_temp_key = None
                self._git_ssh_env = None
                self.logger.debug("ðŸ§¹ Cleaned up Git SSH temporary files")
            except Exception as e:
                self.logger.warning(f"Failed to cleanup Git SSH: {e}")
    
    def _setup_temp_repo(self) -> bool:
        """
        GIT SYNC: Robust repository clone with proper SSH authentication
        
        Returns:
            True if successful
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("GIT SYNC: Setting up temporary repository clone")
        self.logger.info("=" * 60)
        
        # Setup Git SSH authentication
        if not self._setup_git_ssh():
            self.logger.error("âŒ Failed to setup Git SSH authentication")
            return False
        
        # Get clone directory from config
        clone_dir_config = self.config.get('sync_clone_dir', '/tmp/manjaro-awesome-gitclone')
        self._temp_clone_dir = Path(clone_dir_config)
        
        # Clean up existing temp clone if exists
        if self._temp_clone_dir.exists():
            self.logger.info("ðŸ§¹ Cleaning existing temporary clone...")
            try:
                shutil.rmtree(self._temp_clone_dir, ignore_errors=True)
                self.logger.info("âœ… Existing clone removed")
            except Exception as e:
                self.logger.error(f"Failed to remove existing clone: {e}")
                return False
        
        # Get SSH repo URL from config
        ssh_repo_url = self.config.get('ssh_repo_url', 'git@github.com:megvadulthangya/manjaro-awesome.git')
        
        self.logger.info(f"ðŸ“¥ Cloning repository from {ssh_repo_url}...")
        self.logger.info(f"Clone directory: {self._temp_clone_dir}")
        
        try:
            # Use git command directly with SSH environment
            clone_cmd = [
                'git', 'clone',
                '--depth', '1',
                ssh_repo_url,
                str(self._temp_clone_dir)
            ]
            
            # Run git clone with SSH environment
            result = subprocess.run(
                clone_cmd,
                env=self._git_ssh_env,
                capture_output=True,
                text=True,
                timeout=300,
                check=False
            )
            
            if result.returncode == 0:
                self.logger.info(f"âœ… Repository cloned to {self._temp_clone_dir}")
                
                # Verify clone integrity
                git_dir = self._temp_clone_dir / ".git"
                if not git_dir.exists():
                    self.logger.error("âŒ Git directory not found after clone")
                    return False
                
                # Setup build tracking directory in temp clone
                self._build_tracking_dir = self._temp_clone_dir / ".build_tracking"
                self._build_tracking_dir.mkdir(exist_ok=True)
                
                self.logger.info(f"ðŸ“ Build tracking directory: {self._build_tracking_dir}")
                
                # Configure git user for commits
                self._configure_git_identity()
                
                return True
            else:
                self.logger.error(f"âŒ Failed to clone repository (exit code: {result.returncode})")
                if result.stderr:
                    self.logger.error(f"Error: {result.stderr[:500]}")
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ Git clone timed out after 5 minutes")
            return False
        except Exception as e:
            self.logger.error(f"âŒ Clone operation failed: {e}")
            return False
    
    def _configure_git_identity(self):
        """Configure git user identity for commits"""
        if not self._temp_clone_dir or not self._temp_clone_dir.exists():
            return
        
        # Get packager identity from config or environment
        packager_env = self.config.get('packager_env', 'Maintainer <no-reply@gshoots.hu>')
        
        try:
            # Configure git user
            subprocess.run(
                ['git', 'config', 'user.email', 'no-reply@gshoots.hu'],
                cwd=self._temp_clone_dir,
                capture_output=True,
                check=False
            )
            
            subprocess.run(
                ['git', 'config', 'user.name', 'Manjaro Awesome Builder'],
                cwd=self._temp_clone_dir,
                capture_output=True,
                check=False
            )
            
            self.logger.info("âœ… Git identity configured")
            
        except Exception as e:
            self.logger.warning(f"Could not configure git identity: {e}")
    
    def _sync_pacman_databases_initial(self) -> bool:
        """
        CRITICAL FIX: Sync pacman databases BEFORE any operations
        
        Returns:
            True if sync successful
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("CRITICAL: Syncing pacman databases BEFORE operations")
        self.logger.info("=" * 60)
        
        cmd = "sudo LC_ALL=C pacman -Sy --noconfirm"
        result = self.shell_executor.run(
            cmd,
            log_cmd=True,
            timeout=300,
            check=False,
            shell=True
        )
        
        if result.returncode == 0:
            self.logger.info("âœ… Pacman databases synced successfully")
            return True
        else:
            self.logger.error("âŒ Initial pacman sync failed")
            if result.stderr:
                self.logger.error(f"Error: {result.stderr[:500]}")
            return False
    
    def _get_package_lists(self) -> Tuple[List[str], List[str]]:
        """Get package lists from configuration"""
        if not self.local_packages or not self.aur_packages:
            self.local_packages, self.aur_packages = self.config_loader.get_package_lists()
        return self.local_packages, self.aur_packages
    
    def _get_pkgbuild_hash(self, pkgbuild_path: Path) -> str:
        """
        Calculate SHA256 hash of PKGBUILD file
        
        Args:
            pkgbuild_path: Path to PKGBUILD file
        
        Returns:
            SHA256 hash as hex string
        """
        if not pkgbuild_path.exists():
            return ""
        
        try:
            with open(pkgbuild_path, 'rb') as f:
                file_hash = hashlib.sha256()
                chunk = f.read(8192)
                while chunk:
                    file_hash.update(chunk)
                    chunk = f.read(8192)
                return file_hash.hexdigest()
        except Exception as e:
            self.logger.error(f"Failed to calculate hash for {pkgbuild_path}: {e}")
            return ""
    
    def _load_tracking_json(self, pkg_name: str) -> Dict[str, Any]:
        """
        Load tracking JSON for a package
        
        Args:
            pkg_name: Package name
        
        Returns:
            Dictionary with tracking data or empty dict if not found
        """
        if not self._build_tracking_dir:
            return {}
        
        tracking_file = self._build_tracking_dir / f"{pkg_name}.json"
        
        if not tracking_file.exists():
            return {}
        
        try:
            with open(tracking_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"Failed to load tracking JSON for {pkg_name}: {e}")
            return {}
    
    def _save_tracking_json(self, pkg_name: str, data: Dict[str, Any]) -> bool:
        """
        Save tracking JSON for a package
        
        Args:
            pkg_name: Package name
            data: Tracking data to save
        
        Returns:
            True if successful
        """
        if not self._build_tracking_dir:
            return False
        
        tracking_file = self._build_tracking_dir / f"{pkg_name}.json"
        
        try:
            with open(tracking_file, 'w') as f:
                json.dump(data, f, indent=2)
            self.logger.debug(f"Saved tracking JSON for {pkg_name}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to save tracking JSON for {pkg_name}: {e}")
            return False
    
    def _extract_version_from_pkgbuild(self, pkg_dir: Path) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """
        Extract version information from PKGBUILD using makepkg --printsrcinfo
        
        Args:
            pkg_dir: Package directory containing PKGBUILD
        
        Returns:
            Tuple of (pkgver, pkgrel, epoch) or (None, None, None) if failed
        """
        try:
            # Generate .SRCINFO using makepkg
            result = subprocess.run(
                ['makepkg', '--printsrcinfo'],
                cwd=pkg_dir,
                capture_output=True,
                text=True,
                check=False,
                timeout=300
            )
            
            if result.returncode == 0 and result.stdout:
                # Parse SRCINFO content
                pkgver = None
                pkgrel = None
                epoch = None
                
                lines = result.stdout.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    if '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip()
                        
                        if key == 'pkgver':
                            pkgver = value
                        elif key == 'pkgrel':
                            pkgrel = value
                        elif key == 'epoch':
                            epoch = value
                
                if pkgver and pkgrel:
                    return pkgver, pkgrel, epoch
                else:
                    self.logger.warning(f"Could not extract version from PKGBUILD in {pkg_dir}")
                    return None, None, None
            else:
                self.logger.warning(f"makepkg --printsrcinfo failed for {pkg_dir}: {result.stderr}")
                return None, None, None
                
        except Exception as e:
            self.logger.error(f"Error extracting version from PKGBUILD: {e}")
            return None, None, None
    
    def _update_pkgbuild_version(self, pkg_dir: Path, new_pkgver: str, new_pkgrel: str) -> bool:
        """
        Update pkgver and pkgrel in PKGBUILD file using regex
        
        Args:
            pkg_dir: Package directory
            new_pkgver: New pkgver value
            new_pkgrel: New pkgrel value
        
        Returns:
            True if successful
        """
        pkgbuild_path = pkg_dir / "PKGBUILD"
        
        if not pkgbuild_path.exists():
            self.logger.error(f"PKGBUILD not found: {pkgbuild_path}")
            return False
        
        try:
            with open(pkgbuild_path, 'r') as f:
                content = f.read()
            
            # Update pkgver
            pkgver_pattern = r'(pkgver\s*=\s*)[^\s#\n]+'
            content = re.sub(pkgver_pattern, f'\\g<1>{new_pkgver}', content)
            
            # Update pkgrel
            pkgrel_pattern = r'(pkgrel\s*=\s*)[^\s#\n]+'
            content = re.sub(pkgrel_pattern, f'\\g<1>{new_pkgrel}', content)
            
            # Write back to file
            with open(pkgbuild_path, 'w') as f:
                f.write(content)
            
            self.logger.info(f"âœ… Updated PKGBUILD: pkgver={new_pkgver}, pkgrel={new_pkgrel}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to update PKGBUILD: {e}")
            return False
    
    def _should_build_local_package(self, pkg_name: str, temp_pkg_dir: Path) -> Tuple[bool, Optional[Dict[str, Any]]]:
        """
        Determine if local package needs to be built using tracking system
        
        Args:
            pkg_name: Package name
            temp_pkg_dir: Package directory in temp clone
        
        Returns:
            Tuple of (should_build, tracking_data)
        """
        # Load tracking data
        tracking_data = self._load_tracking_json(pkg_name)
        
        # Check if package directory exists in temp clone
        if not temp_pkg_dir.exists():
            self.logger.error(f"Package directory not found in temp clone: {pkg_name}")
            return False, tracking_data
        
        pkgbuild_path = temp_pkg_dir / "PKGBUILD"
        if not pkgbuild_path.exists():
            self.logger.error(f"PKGBUILD not found for {pkg_name}")
            return False, tracking_data
        
        # Calculate current PKGBUILD hash
        current_hash = self._get_pkgbuild_hash(pkgbuild_path)
        if not current_hash:
            self.logger.error(f"Failed to calculate PKGBUILD hash for {pkg_name}")
            return False, tracking_data
        
        # Extract current version from PKGBUILD
        current_pkgver, current_pkgrel, current_epoch = self._extract_version_from_pkgbuild(temp_pkg_dir)
        if not current_pkgver or not current_pkgrel:
            self.logger.error(f"Failed to extract version from PKGBUILD for {pkg_name}")
            return False, tracking_data
        
        current_version = f"{current_pkgver}-{current_pkgrel}"
        if current_epoch and current_epoch != '0':
            current_version = f"{current_epoch}:{current_version}"
        
        # Check tracking data
        if not tracking_data:
            # No tracking data - first build
            self.logger.info(f"ðŸ†• First build detected for {pkg_name}")
            return True, {
                'last_hash': current_hash,
                'last_version': current_version,
                'last_built': datetime.now().isoformat(),
                'pkgver': current_pkgver,
                'pkgrel': current_pkgrel,
                'epoch': current_epoch
            }
        
        # Check if PKGBUILD has changed
        last_hash = tracking_data.get('last_hash', '')
        last_version = tracking_data.get('last_version', '')
        
        if current_hash != last_hash:
            self.logger.info(f"ðŸ”€ PKGBUILD changed for {pkg_name} (hash mismatch)")
            return True, {
                'last_hash': current_hash,
                'last_version': current_version,
                'last_built': datetime.now().isoformat(),
                'pkgver': current_pkgver,
                'pkgrel': current_pkgrel,
                'epoch': current_epoch
            }
        
        # Check if version has changed (in case hash is same but version updated elsewhere)
        if current_version != last_version:
            self.logger.info(f"ðŸ”€ Version changed for {pkg_name}: {last_version} -> {current_version}")
            return True, {
                'last_hash': current_hash,
                'last_version': current_version,
                'last_built': datetime.now().isoformat(),
                'pkgver': current_pkgver,
                'pkgrel': current_pkgrel,
                'epoch': current_epoch
            }
        
        # Also check if package exists on server with same version
        found, remote_version, remote_hash = self.version_tracker.is_package_on_remote(pkg_name, current_version)
        
        if found:
            self.logger.info(f"âœ… {pkg_name} already on server with same version ({current_version})")
            return False, tracking_data
        else:
            self.logger.info(f"ðŸ”„ {pkg_name} not on server or different version, needs build")
            return True, {
                'last_hash': current_hash,
                'last_version': current_version,
                'last_built': datetime.now().isoformat(),
                'pkgver': current_pkgver,
                'pkgrel': current_pkgrel,
                'epoch': current_epoch
            }
    
    def _build_local_package_temp_clone(self, pkg_name: str) -> bool:
        """
        Build local package using temporary clone method with tracking
        
        Args:
            pkg_name: Package name
        
        Returns:
            True if successful
        """
        if not self._temp_clone_dir:
            self.logger.error("Temporary clone not set up")
            return False
        
        temp_pkg_dir = self._temp_clone_dir / pkg_name
        
        self.logger.info(f"\n--- Building Local Package: {pkg_name} ---")
        self.logger.info(f"Package directory: {temp_pkg_dir}")
        
        # Check if we should build
        should_build, tracking_data = self._should_build_local_package(pkg_name, temp_pkg_dir)
        
        if not should_build:
            self.build_state.add_skipped(
                pkg_name,
                tracking_data.get('last_version', 'unknown'),
                is_aur=False,
                reason="up-to-date"
            )
            return True
        
        # Build the package
        try:
            self.logger.info(f"ðŸš€ Building {pkg_name}...")
            
            # Extract version info from tracking data
            pkgver = tracking_data.get('pkgver', '')
            pkgrel = tracking_data.get('pkgrel', '')
            epoch = tracking_data.get('epoch')
            
            version_str = f"{pkgver}-{pkgrel}"
            if epoch and epoch != '0':
                version_str = f"{epoch}:{version_str}"
            
            # Build using makepkg
            build_result = subprocess.run(
                ['makepkg', '-si', '--noconfirm', '--clean'],
                cwd=temp_pkg_dir,
                capture_output=True,
                text=True,
                check=False,
                timeout=3600
            )
            
            if build_result.returncode == 0:
                # Find built package files
                built_packages = list(temp_pkg_dir.glob("*.pkg.tar.*"))
                
                if built_packages:
                    # Move built packages to output directory
                    output_dir = Path(self.config.get('output_dir', 'built_packages'))
                    output_dir.mkdir(exist_ok=True)
                    
                    moved_count = 0
                    for pkg_file in built_packages:
                        dest = output_dir / pkg_file.name
                        shutil.move(str(pkg_file), str(dest))
                        moved_count += 1
                        self.logger.info(f"âœ… Built: {pkg_file.name}")
                    
                    if moved_count > 0:
                        # Update tracking data
                        tracking_data['last_built'] = datetime.now().isoformat()
                        self._save_tracking_json(pkg_name, tracking_data)
                        
                        # Register built package
                        self.version_tracker.register_built_package(pkg_name, version_str)
                        self.build_state.add_built(pkg_name, version_str, is_aur=False)
                        
                        # Queue old version cleanup
                        self._queue_old_version_cleanup(pkg_name, version_str)
                        
                        # Sanitize artifacts
                        self._sanitize_artifacts(pkg_name)
                        
                        return True
                    else:
                        self.logger.error(f"No package files moved for {pkg_name}")
                        return False
                else:
                    self.logger.error(f"No package files created for {pkg_name}")
                    return False
            else:
                self.logger.error(f"Build failed for {pkg_name}: {build_result.stderr[:500]}")
                
                # Try dependency fallback
                self.logger.info("ðŸ”„ Trying dependency fallback...")
                return self._build_with_dependency_fallback(pkg_name, temp_pkg_dir, version_str)
                
        except Exception as e:
            self.logger.error(f"Error building {pkg_name}: {e}")
            self.build_state.add_failed(
                pkg_name,
                version_str if 'version_str' in locals() else "unknown",
                is_aur=False,
                error_message=str(e)
            )
            return False
    
    def _build_with_dependency_fallback(self, pkg_name: str, pkg_dir: Path, version_str: str) -> bool:
        """
        Build with dependency fallback using yay
        
        Args:
            pkg_name: Package name
            pkg_dir: Package directory
            version_str: Version string
        
        Returns:
            True if successful
        """
        try:
            # Extract missing dependencies from error output
            self.logger.info("ðŸ” Checking for missing dependencies...")
            
            # First attempt to install dependencies with yay
            yay_cmd = "LC_ALL=C yay -S --needed --noconfirm $(makepkg --printsrcinfo | grep 'depends =' | cut -d'=' -f2 | tr '\n' ' ')"
            yay_result = subprocess.run(
                yay_cmd,
                shell=True,
                capture_output=True,
                text=True,
                check=False,
                timeout=1800
            )
            
            if yay_result.returncode == 0:
                self.logger.info("âœ… Missing dependencies installed, retrying build...")
                
                # Retry the build
                build_result = subprocess.run(
                    ['makepkg', '-si', '--noconfirm', '--clean'],
                    cwd=pkg_dir,
                    capture_output=True,
                    text=True,
                    check=False,
                    timeout=3600
                )
                
                if build_result.returncode == 0:
                    # Find built package files
                    built_packages = list(pkg_dir.glob("*.pkg.tar.*"))
                    
                    if built_packages:
                        # Move built packages to output directory
                        output_dir = Path(self.config.get('output_dir', 'built_packages'))
                        output_dir.mkdir(exist_ok=True)
                        
                        moved_count = 0
                        for pkg_file in built_packages:
                            dest = output_dir / pkg_file.name
                            shutil.move(str(pkg_file), str(dest))
                            moved_count += 1
                            self.logger.info(f"âœ… Built with fallback: {pkg_file.name}")
                        
                        if moved_count > 0:
                            # Update tracking data
                            tracking_data = self._load_tracking_json(pkg_name)
                            tracking_data['last_built'] = datetime.now().isoformat()
                            self._save_tracking_json(pkg_name, tracking_data)
                            
                            # Register built package
                            self.version_tracker.register_built_package(pkg_name, version_str)
                            self.build_state.add_built(pkg_name, version_str, is_aur=False)
                            
                            # Queue old version cleanup
                            self._queue_old_version_cleanup(pkg_name, version_str)
                            
                            # Sanitize artifacts
                            self._sanitize_artifacts(pkg_name)
                            
                            return True
                
                self.logger.error(f"Retry build failed: {build_result.stderr[:500]}")
                return False
            else:
                self.logger.error(f"Dependency installation failed: {yay_result.stderr[:500]}")
                return False
                
        except Exception as e:
            self.logger.error(f"Dependency fallback failed: {e}")
            self.build_state.add_failed(
                pkg_name,
                version_str,
                is_aur=False,
                error_message=f"Dependency fallback failed: {e}"
            )
            return False
    
    def _commit_and_push_changes(self) -> bool:
        """
        Commit and push changes from temporary clone using Git SSH authentication
        
        Returns:
            True if successful
        """
        if not self._temp_clone_dir:
            self.logger.error("Temporary clone not set up")
            return False
        
        if not self._git_ssh_env:
            self.logger.error("Git SSH environment not configured")
            return False
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("GIT: Committing and pushing changes")
        self.logger.info("=" * 60)
        
        old_cwd = os.getcwd()
        os.chdir(self._temp_clone_dir)
        
        try:
            # Check if there are any changes
            status_result = subprocess.run(
                ['git', 'status', '--porcelain'],
                env=self._git_ssh_env,
                capture_output=True,
                text=True,
                check=False
            )
            
            if status_result.returncode != 0:
                self.logger.error(f"Git status failed: {status_result.stderr}")
                return False
            
            if not status_result.stdout.strip():
                self.logger.info("â„¹ï¸ No changes to commit")
                return True
            
            self.logger.info("ðŸ“‹ Changes detected:")
            for line in status_result.stdout.strip().splitlines():
                self.logger.info(f"  {line}")
            
            # Add all changes including build tracking
            self.logger.info("âž• Adding changes...")
            add_result = subprocess.run(
                ['git', 'add', '.'],
                env=self._git_ssh_env,
                capture_output=True,
                text=True,
                check=False
            )
            
            if add_result.returncode != 0:
                self.logger.error(f"Git add failed: {add_result.stderr}")
                return False
            
            # Commit changes
            commit_message = f"update: Packages built {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            self.logger.info(f"ðŸ’¾ Committing: {commit_message}")
            
            commit_result = subprocess.run(
                ['git', 'commit', '-m', commit_message],
                env=self._git_ssh_env,
                capture_output=True,
                text=True,
                check=False
            )
            
            if commit_result.returncode != 0:
                self.logger.error(f"Git commit failed: {commit_result.stderr}")
                # Check if it's just an empty commit
                if "nothing to commit" not in commit_result.stderr.lower():
                    return False
                self.logger.info("â„¹ï¸ No changes to commit")
                return True
            
            # Push changes to main branch
            self.logger.info("ðŸ“¤ Pushing to remote...")
            push_result = subprocess.run(
                ['git', 'push', 'origin', 'main'],
                env=self._git_ssh_env,
                capture_output=True,
                text=True,
                check=False,
                timeout=300
            )
            
            if push_result.returncode == 0:
                self.logger.info("âœ… Changes pushed successfully")
                return True
            else:
                self.logger.error(f"Git push failed: {push_result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ Git push timed out after 5 minutes")
            return False
        except Exception as e:
            self.logger.error(f"Git operation failed: {e}")
            return False
        finally:
            os.chdir(old_cwd)
    
    def _sanitize_artifacts(self, pkg_name: str) -> List[Path]:
        """
        INTERNAL SANITIZATION: Replace ':' with '_' in filenames BEFORE rsync
        
        Args:
            pkg_name: Package name
        
        Returns:
            List of sanitized file paths
        """
        self.logger.info(f"ðŸ”§ Sanitizing artifacts for {pkg_name}...")
        
        output_dir = Path(self.config.get('output_dir', 'built_packages'))
        sanitized_files = []
        
        patterns = [f"*{pkg_name}*.pkg.tar.*", f"{pkg_name}*.pkg.tar.*"]
        
        for pattern in patterns:
            for pkg_file in output_dir.glob(pattern):
                original_name = pkg_file.name
                
                if ':' in original_name:
                    sanitized_name = original_name.replace(':', '_')
                    sanitized_path = pkg_file.with_name(sanitized_name)
                    
                    try:
                        pkg_file.rename(sanitized_path)
                        self.logger.info(f"  ðŸ”„ Renamed: {original_name} -> {sanitized_name}")
                        
                        self._sanitized_files[str(pkg_file)] = str(sanitized_path)
                        
                        sig_file = pkg_file.with_suffix(pkg_file.suffix + '.sig')
                        if sig_file.exists():
                            sanitized_sig = sanitized_path.with_suffix(sanitized_path.suffix + '.sig')
                            sig_file.rename(sanitized_sig)
                            self.logger.info(f"  ðŸ”„ Renamed signature: {sig_file.name} -> {sanitized_sig.name}")
                        
                        sanitized_files.append(sanitized_path)
                    except Exception as e:
                        self.logger.error(f"Failed to rename {original_name}: {e}")
                        sanitized_files.append(pkg_file)
                else:
                    sanitized_files.append(pkg_file)
        
        self.logger.info(f"âœ… Sanitized {len(sanitized_files)} files for {pkg_name}")
        return sanitized_files
    
    def _check_server_for_package(self, pkg_name: str, version_str: str, is_aur: bool) -> str:
        """
        SERVER-FIRST LOGIC: Check if package exists on server
        
        Args:
            pkg_name: Package name
            version_str: Full version string
            is_aur: Whether it's an AUR package
        
        Returns:
            "ADOPT" if package exists on server, "BUILD" if not
        """
        found, remote_version, remote_hash = self.version_tracker.is_package_on_remote(pkg_name, version_str)
        
        if found:
            self.logger.info(f"âœ… [ADOPT] {pkg_name} {version_str} found on server. Skipping build.")
            
            self.version_tracker.register_built_package(pkg_name, version_str, remote_hash)
            self.build_state.add_skipped(pkg_name, version_str, is_aur=is_aur, reason="already-on-server")
            
            return "ADOPT"
        else:
            self.logger.info(f"ðŸ”„ [BUILD] {pkg_name} {version_str} not on server or outdated. Starting build.")
            return "BUILD"
    
    def _build_aur_packages_server_first(self) -> None:
        """
        Build AUR packages using SERVER-FIRST logic
        """
        if not self.aur_packages:
            self.logger.info("No AUR packages to build")
            return
        
        self.logger.info(f"\nðŸ”¨ Processing {len(self.aur_packages)} AUR packages (SERVER-FIRST)")
        
        for pkg_name in self.aur_packages:
            self.logger.info(f"\n--- Processing AUR: {pkg_name} ---")
            
            # For AUR packages, we use the existing builder
            # Note: AUR builder doesn't use temp clone method
            
            # Check server first
            # We need to get version info differently for AUR packages
            # This is a simplified approach - in reality we'd need to clone the AUR
            # and check version like we do for local packages
            
            # For now, we'll build all AUR packages
            # In a production system, you'd want to implement proper version checking
            
            self.logger.info(f"ðŸš€ Building AUR package: {pkg_name}...")
            
            success = self.aur_builder.build(pkg_name, None)
            
            if success:
                version_str = "unknown"  # Should get actual version from builder
                self.version_tracker.register_built_package(pkg_name, version_str)
                self.build_state.add_built(pkg_name, version_str, is_aur=True)
                
                self._queue_old_version_cleanup(pkg_name, version_str)
            else:
                self.build_state.add_failed(
                    pkg_name,
                    "unknown",
                    is_aur=True,
                    error_message="AUR build failed"
                )
    
    def _queue_old_version_cleanup(self, pkg_name: str, keep_version: str):
        """
        Queue old versions for cleanup (but don't execute yet)
        
        Args:
            pkg_name: Package name
            keep_version: Version to keep (newly built/adopted)
        """
        self.logger.info(f"ðŸ§¹ Queuing cleanup of old versions for {pkg_name}...")
        
        remote_files = self.ssh_client.get_cached_inventory()
        if not remote_files:
            return
        
        for remote_path in remote_files.values():
            filename = Path(remote_path).name
            
            parsed = self.version_tracker._parse_package_filename_with_arch(filename)
            if not parsed:
                continue
            
            remote_pkg_name, remote_version, architecture = parsed
            
            if remote_pkg_name.lower() == pkg_name.lower():
                if not self.version_tracker._versions_match(remote_version, keep_version):
                    self.version_tracker.queue_deletion(remote_path)
                    self.logger.debug(f"ðŸ—‘ï¸ Queued for deletion: {filename} (old version: {remote_version})")
                else:
                    self.logger.debug(f"âœ… Keeping: {filename} (current version: {remote_version})")
    
    def _create_local_staging(self) -> Path:
        """
        Create local staging directory for database operations
        
        Returns:
            Path to staging directory
        """
        if self._staging_dir and self._staging_dir.exists():
            shutil.rmtree(self._staging_dir, ignore_errors=True)
        
        self._staging_dir = Path(tempfile.mkdtemp(prefix="repo_staging_"))
        self.logger.info(f"ðŸ“ Created staging directory: {self._staging_dir}")
        return self._staging_dir
    
    def _download_existing_database_only(self) -> bool:
        """
        Download ONLY database files from VPS to staging
        
        Returns:
            True if successful or no database exists (first run)
        """
        repo_name = self.config.get('repo_name', '')
        
        self.logger.info("ðŸ“¥ Downloading existing database files from VPS...")
        
        patterns = [
            f"{repo_name}.db.tar.gz*",
            f"{repo_name}.files.tar.gz*"
        ]
        
        success_count = 0
        
        for pattern in patterns:
            self.logger.info(f"  Downloading pattern: {pattern}")
            
            success = self.rsync_client.mirror_remote(
                remote_pattern=pattern,
                local_dir=self._staging_dir,
                temp_dir=None
            )
            
            if success:
                success_count += 1
            else:
                self.logger.warning(f"âš ï¸ Failed to download {pattern}")
        
        db_files = list(self._staging_dir.glob(f"{repo_name}.db.tar.gz*"))
        files_files = list(self._staging_dir.glob(f"{repo_name}.files.tar.gz*"))
        
        total_files = len(db_files) + len(files_files)
        
        if total_files > 0:
            self.logger.info(f"âœ… Downloaded {total_files} database files")
        else:
            self.logger.info("â„¹ï¸ No existing database files found (first run or clean state)")
        
        return True
    
    def _get_db_package_list(self) -> Set[str]:
        """
        Extract package list from existing database file
        
        Returns:
            Set of package filenames (without path) in the database
        """
        repo_name = self.config.get('repo_name', '')
        db_file = self._staging_dir / f"{repo_name}.db.tar.gz"
        
        if not db_file.exists():
            self.logger.info("â„¹ï¸ No database file found in staging")
            return set()
        
        self.logger.info("ðŸ“‹ Extracting package list from existing database...")
        
        try:
            # Use tar to list contents and find package entries
            cmd = ["tar", "-tzf", str(db_file)]
            result = subprocess.run(cmd, capture_output=True, text=True, check=False)
            
            if result.returncode != 0:
                self.logger.warning(f"Failed to list database contents: {result.stderr}")
                return set()
            
            # Parse tar output to find package entries
            package_files = set()
            for line in result.stdout.splitlines():
                if line.strip() and '/' in line:
                    # Extract filename from path like: awesome-git-4.0.r123.gabc123def-1-x86_64/pkgname/desc
                    parts = line.split('/')
                    if len(parts) >= 2 and parts[1].endswith('/desc'):
                        # The directory name is the package filename
                        package_files.add(parts[0])
            
            self.logger.info(f"ðŸ“Š Database contains {len(package_files)} package entries")
            if package_files:
                self.logger.debug(f"Sample DB entries: {list(package_files)[:5]}")
            
            return package_files
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to extract package list from DB: {e}")
            return set()
    
    def _discover_missing_packages(self) -> List[str]:
        """
        AUTO-RECOVERY: Compare remote inventory with database entries
        
        Returns:
            List of package filenames missing from database
        """
        self.logger.info("ðŸ” Discovering packages missing from database...")
        
        # Get packages in database
        db_packages = self._get_db_package_list()
        
        # Get remote inventory (physical files on VPS)
        remote_inventory = self.ssh_client.get_cached_inventory(force_refresh=True)
        
        if not remote_inventory:
            self.logger.info("â„¹ï¸ No remote packages found")
            return []
        
        # Filter for package files only
        remote_packages = set()
        for filename in remote_inventory.keys():
            if filename.endswith('.pkg.tar.zst'):
                remote_packages.add(filename)
        
        self.logger.info(f"ðŸ“Š Remote inventory: {len(remote_packages)} package files")
        
        # Find packages on VPS that are NOT in database
        missing_packages = []
        for pkg_file in remote_packages:
            if pkg_file not in db_packages:
                missing_packages.append(pkg_file)
                self.logger.info(f"âš ï¸ Missing from DB: {pkg_file}")
        
        self.logger.info(f"ðŸ“Š Found {len(missing_packages)} packages missing from database")
        
        # Store for reporting
        self._missing_from_db = missing_packages
        
        return missing_packages
    
    def _download_missing_packages(self, missing_packages: List[str]) -> int:
        """
        Download missing packages from VPS to staging
        
        Args:
            missing_packages: List of package filenames to download
        
        Returns:
            Number of successfully downloaded packages
        """
        if not missing_packages:
            return 0
        
        self.logger.info(f"ðŸ“¥ Downloading {len(missing_packages)} missing packages from VPS...")
        
        downloaded_count = 0
        
        for pkg_filename in missing_packages:
            try:
                # Find full remote path
                remote_inventory = self.ssh_client.get_cached_inventory()
                remote_path = remote_inventory.get(pkg_filename)
                
                if not remote_path:
                    self.logger.warning(f"Could not find remote path for {pkg_filename}")
                    continue
                
                # Download using scp
                scp_cmd = [
                    "scp",
                    "-o", "StrictHostKeyChecking=no",
                    "-o", "ConnectTimeout=30",
                    f"{self.config.get('vps_user')}@{self.config.get('vps_host')}:{remote_path}",
                    str(self._staging_dir / pkg_filename)
                ]
                
                result = subprocess.run(
                    scp_cmd,
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                if result.returncode == 0:
                    # Check if file was downloaded
                    local_path = self._staging_dir / pkg_filename
                    if local_path.exists() and local_path.stat().st_size > 0:
                        self.logger.info(f"âœ… Downloaded: {pkg_filename}")
                        downloaded_count += 1
                        
                        # Add to recovered packages list
                        self._recovered_packages.append(pkg_filename)
                    else:
                        self.logger.warning(f"Downloaded file is empty: {pkg_filename}")
                else:
                    self.logger.warning(f"Failed to download {pkg_filename}: {result.stderr}")
                    
            except Exception as e:
                self.logger.error(f"Error downloading {pkg_filename}: {e}")
        
        self.logger.info(f"âœ… Downloaded {downloaded_count} missing packages")
        return downloaded_count
    
    def _move_new_packages_to_staging(self) -> List[Path]:
        """
        Move newly built packages to staging directory
        
        Returns:
            List of paths to new packages moved to staging
        """
        output_dir = Path(self.config.get('output_dir', 'built_packages'))
        
        new_packages = list(output_dir.glob("*.pkg.tar.zst"))
        if not new_packages:
            self.logger.info("â„¹ï¸ No new packages to move to staging")
            return []
        
        self.logger.info(f"ðŸ“¦ Moving {len(new_packages)} new packages to staging...")
        
        moved_packages = []
        
        for new_pkg in new_packages:
            try:
                dest = self._staging_dir / new_pkg.name
                if dest.exists():
                    dest.unlink()
                shutil.move(str(new_pkg), str(dest))
                moved_packages.append(dest)
                
                # Move signature if exists
                sig_file = new_pkg.with_suffix(new_pkg.suffix + '.sig')
                if sig_file.exists():
                    sig_dest = dest.with_suffix(dest.suffix + '.sig')
                    if sig_dest.exists():
                        sig_dest.unlink()
                    shutil.move(str(sig_file), str(sig_dest))
                
                self.logger.debug(f"  Moved: {new_pkg.name}")
            except Exception as e:
                self.logger.error(f"Failed to move {new_pkg.name}: {e}")
        
        self.logger.info(f"âœ… Moved {len(moved_packages)} new packages to staging")
        return moved_packages
    
    def _update_database_additive(self) -> bool:
        """
        Additive database update: Add ALL packages in staging to database
        
        Returns:
            True if successful
        """
        repo_name = self.config.get('repo_name', '')
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ADDITIVE DATABASE UPDATE (SELF-HEALING)")
        self.logger.info("=" * 60)
        
        old_cwd = os.getcwd()
        os.chdir(self._staging_dir)
        
        try:
            db_file = f"{repo_name}.db.tar.gz"
            
            # Get all package files in staging
            all_packages = list(glob.glob("*.pkg.tar.zst"))
            
            if not all_packages:
                self.logger.info("â„¹ï¸ No packages to add to database")
                
                # Check if we have an existing database (force repair mode)
                existing_db_files = list(glob.glob(f"{repo_name}.db.tar.gz*"))
                if existing_db_files:
                    self.logger.info("ðŸ”§ Force repairing existing database...")
                    return self._force_repair_database(db_file)
                else:
                    self.logger.info("â„¹ï¸ No existing database to repair")
                    return True
            
            self.logger.info(f"ðŸ“Š Total packages to process: {len(all_packages)}")
            self.logger.info(f"  - Newly built: {len([p for p in all_packages if p not in self._recovered_packages])}")
            self.logger.info(f"  - Recovered: {len([p for p in all_packages if p in self._recovered_packages])}")
            
            # Run repo-add with ALL packages (additive update)
            if self.gpg_handler.gpg_enabled:
                self.logger.info("ðŸ” Running repo-add with GPG signing (additive)...")
                
                # Set GNUPGHOME environment variable for non-interactive signing
                env = os.environ.copy()
                if hasattr(self.gpg_handler, 'gpg_home') and self.gpg_handler.gpg_home:
                    env['GNUPGHOME'] = self.gpg_handler.gpg_home
                
                # Build command
                if self.gpg_handler.gpg_key_id:
                    cmd = f"repo-add --sign --key {self.gpg_handler.gpg_key_id} --remove {db_file} *.pkg.tar.zst"
                else:
                    cmd = f"repo-add --sign --remove {db_file} *.pkg.tar.zst"
                
                result = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    env=env,
                    check=False
                )
            else:
                self.logger.info("ðŸ”§ Running repo-add without signing (additive)...")
                cmd = f"repo-add --remove {db_file} *.pkg.tar.zst"
                
                result = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    check=False
                )
            
            if result.returncode == 0:
                self.logger.info("âœ… Database updated successfully (additive)")
                
                if not os.path.exists(db_file):
                    self.logger.error("âŒ Database file not created")
                    return False
                
                # Verify the database was created with signatures if GPG enabled
                if self.gpg_handler.gpg_enabled:
                    sig_file = f"{db_file}.sig"
                    if os.path.exists(sig_file):
                        sig_size = os.path.getsize(sig_file)
                        if sig_size > 0:
                            self.logger.info(f"âœ… Database signed successfully ({sig_size} bytes)")
                        else:
                            self.logger.warning("âš ï¸ Database signature file is empty")
                    else:
                        self.logger.warning("âš ï¸ Database signature file not found")
                
                # Verify database entries
                self._verify_database_entries(db_file)
                return True
            else:
                self.logger.error(f"âŒ repo-add failed with exit code {result.returncode}:")
                if result.stdout:
                    self.logger.error(f"STDOUT: {result.stdout[:500]}")
                if result.stderr:
                    self.logger.error(f"STDERR: {result.stderr[:500]}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Database update error: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            os.chdir(old_cwd)
    
    def _force_repair_database(self, db_file: str) -> bool:
        """
        Force repair existing database (re-signing)
        
        Args:
            db_file: Database file name
        
        Returns:
            True if successful
        """
        self.logger.info("ðŸ”§ Force repairing existing database...")
        
        try:
            # Recreate database with proper signing
            if self.gpg_handler.gpg_enabled:
                self.logger.info("ðŸ” Re-signing existing database...")
                
                env = os.environ.copy()
                if hasattr(self.gpg_handler, 'gpg_home') and self.gpg_handler.gpg_home:
                    env['GNUPGHOME'] = self.gpg_handler.gpg_home
                
                if self.gpg_handler.gpg_key_id:
                    cmd = f"repo-add --sign --key {self.gpg_handler.gpg_key_id} --remove {db_file} *.pkg.tar.zst"
                else:
                    cmd = f"repo-add --sign --remove {db_file} *.pkg.tar.zst"
                
                result = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    env=env,
                    check=False
                )
            else:
                self.logger.info("ðŸ”§ Recreating database without signing...")
                cmd = f"repo-add --remove {db_file} *.pkg.tar.zst"
                
                result = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    check=False
                )
            
            if result.returncode == 0:
                self.logger.info("âœ… Database repaired successfully")
                return True
            else:
                self.logger.error(f"Database repair failed: {result.stderr[:500]}")
                return False
                
        except Exception as e:
            self.logger.error(f"Database repair error: {e}")
            return False
    
    def _verify_database_entries(self, db_file: str) -> None:
        """Verify database entries after update"""
        try:
            list_cmd = ["tar", "-tzf", db_file]
            result = subprocess.run(list_cmd, capture_output=True, text=True, check=False)
            if result.returncode == 0:
                db_entries = [line for line in result.stdout.split('\n') if line.endswith('/desc')]
                self.logger.info(f"âœ… Database contains {len(db_entries)} package entries")
                if len(db_entries) == 0:
                    self.logger.error("âŒâŒâŒ DATABASE IS EMPTY!")
                else:
                    self.logger.info(f"Sample entries: {db_entries[:3]}")
            else:
                self.logger.warning(f"Could not list database contents: {result.stderr}")
        except Exception as e:
            self.logger.warning(f"Could not verify database: {e}")
    
    def _upload_updated_files(self) -> bool:
        """
        Upload updated database and new packages to VPS
        
        Returns:
            True if successful
        """
        if not self._staging_dir or not self._staging_dir.exists():
            self.logger.error("âŒ Staging directory not found")
            return False
        
        self.logger.info("\nðŸ“¤ Uploading updated files to VPS...")
        
        files_to_upload = []
        
        # 1. Database files and signatures (ALWAYS upload these)
        repo_patterns = [
            f"{self.config.get('repo_name', '')}.db*",
            f"{self.config.get('repo_name', '')}.files*",
        ]
        
        for pattern in repo_patterns:
            for file_path in self._staging_dir.glob(pattern):
                if file_path.stat().st_size > 0:  # Skip empty files
                    files_to_upload.append(file_path)
        
        # 2. NEW packages (not recovered) and their signatures
        for pkg_file in self._staging_dir.glob("*.pkg.tar.zst"):
            if pkg_file.stat().st_size > 0:
                # Check if this is a recovered package (already on VPS)
                if pkg_file.name not in self._recovered_packages:
                    files_to_upload.append(pkg_file)
                    
                    # Include signature if it exists
                    sig_file = pkg_file.with_suffix(pkg_file.suffix + '.sig')
                    if sig_file.exists() and sig_file.stat().st_size > 0:
                        files_to_upload.append(sig_file)
                else:
                    self.logger.debug(f"Skipping recovered package: {pkg_file.name}")
        
        if not files_to_upload:
            self.logger.warning("âš ï¸ No files to upload")
            return True
        
        self.logger.info(f"ðŸ“¦ Total files to upload: {len(files_to_upload)}")
        
        # Log file details
        for f in files_to_upload:
            size_mb = f.stat().st_size / (1024 * 1024)
            file_type = "PACKAGE" if ".pkg.tar.zst" in f.name else "DATABASE"
            if f.name.endswith('.sig'):
                file_type = "SIGNATURE"
            self.logger.debug(f"  - {f.name} ({size_mb:.1f}MB) [{file_type}]")
        
        # Upload using rsync
        files_list = [str(f) for f in files_to_upload]
        upload_success = self.rsync_client.upload(files_list, self._staging_dir)
        
        if upload_success:
            self.logger.info("âœ… All files uploaded successfully")
            return True
        else:
            self.logger.error("âŒ File upload failed")
            return False
    
    def _cleanup_staging(self) -> None:
        """Clean up staging directory"""
        if self._staging_dir and os.path.exists(self._staging_dir):
            try:
                shutil.rmtree(self._staging_dir, ignore_errors=True)
                self.logger.debug(f"ðŸ§¹ Cleaned up staging directory: {self._staging_dir}")
                self._staging_dir = None
            except Exception as e:
                self.logger.warning(f"Could not clean staging directory: {e}")
    
    def _cleanup_temp_clone(self) -> None:
        """Clean up temporary clone directory"""
        if self._temp_clone_dir and os.path.exists(self._temp_clone_dir):
            try:
                shutil.rmtree(self._temp_clone_dir, ignore_errors=True)
                self.logger.debug(f"ðŸ§¹ Cleaned up temporary clone: {self._temp_clone_dir}")
                self._temp_clone_dir = None
                self._build_tracking_dir = None
            except Exception as e:
                self.logger.warning(f"Could not clean temporary clone: {e}")
    
    def _build_local_packages_with_tracking(self) -> None:
        """
        Build local packages using temporary clone with tracking system
        """
        if not self.local_packages:
            self.logger.info("No local packages to build")
            return
        
        self.logger.info(f"\nðŸ”¨ Processing {len(self.local_packages)} local packages (TRACKING SYSTEM)")
        
        built_count = 0
        skipped_count = 0
        failed_count = 0
        
        for pkg_name in self.local_packages:
            self.logger.info(f"\n--- Processing Local: {pkg_name} ---")
            
            try:
                success = self._build_local_package_temp_clone(pkg_name)
                
                if success:
                    built_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                self.logger.error(f"Unexpected error building {pkg_name}: {e}")
                failed_count += 1
        
        self.logger.info(f"\nðŸ“Š Local packages summary:")
        self.logger.info(f"  Built: {built_count}")
        self.logger.info(f"  Failed: {failed_count}")
    
    def _force_database_update(self) -> bool:
        """
        FORCE database update with self-healing additive strategy
        
        Returns:
            True if successful
        """
        self.logger.info("\nðŸ”§ FORCING DATABASE UPDATE (SELF-HEALING)")
        self.logger.info("=" * 60)
        
        try:
            # Reset recovery state
            self._recovered_packages = []
            self._missing_from_db = []
            
            # Step 1: Create staging directory
            self.logger.info("\n[1/8] Creating local staging directory")
            self.logger.info("-" * 40)
            staging_dir = self._create_local_staging()
            
            # Step 2: Download existing database files
            self.logger.info("\n[2/8] Downloading existing database files from VPS")
            self.logger.info("-" * 40)
            self._download_existing_database_only()
            
            # Step 3: AUTO-RECOVERY: Discover packages missing from database
            self.logger.info("\n[3/8] AUTO-RECOVERY: Discovering missing packages")
            self.logger.info("-" * 40)
            missing_packages = self._discover_missing_packages()
            
            # Step 4: Download missing packages from VPS
            if missing_packages:
                self.logger.info("\n[4/8] Downloading missing packages from VPS")
                self.logger.info("-" * 40)
                downloaded = self._download_missing_packages(missing_packages)
                self.logger.info(f"âœ… Downloaded {downloaded} missing packages")
            
            # Step 5: Move newly built packages to staging
            self.logger.info("\n[5/8] Moving newly built packages to staging")
            self.logger.info("-" * 40)
            self._move_new_packages_to_staging()
            
            # Step 6: Update database additively with ALL packages
            self.logger.info("\n[6/8] Updating database additively (self-healing)")
            self.logger.info("-" * 40)
            if not self._update_database_additive():
                self.logger.error("âŒ Additive database update failed")
                return False
            
            # Step 7: Upload updated files to VPS
            self.logger.info("\n[7/8] Uploading updated files to VPS")
            self.logger.info("-" * 40)
            if not self._upload_updated_files():
                self.logger.error("âŒ File upload failed")
                return False
            
            # Step 8: Execute queued cleanup
            self.logger.info("\n[8/8] Executing queued cleanup operations")
            self.logger.info("-" * 40)
            cleanup_success = self.version_tracker.commit_queued_deletions()
            
            if cleanup_success:
                self.logger.info("âœ… Cleanup operations completed")
            else:
                self.logger.warning("âš ï¸ Some cleanup operations failed")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Self-healing database update failed: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            self._cleanup_staging()
    
    def run(self) -> int:
        """
        Main execution workflow - COMPLETE BUILD SYSTEM
        
        Returns:
            Exit code (0 for success, 1 for failure)
        """
        try:
            self.logger.info("\n" + "=" * 60)
            self.logger.info("ðŸš€ MANJARO PACKAGE BUILDER - COMPLETE BUILD SYSTEM")
            self.logger.info("=" * 60)
            
            self.logger.info("\nðŸ”§ Initial setup...")
            self.logger.info(f"Repository root: {self.repo_root}")
            self.logger.info(f"Repository name: {self.config.get('repo_name')}")
            self.logger.info(f"Output directory: {self.config.get('output_dir')}")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 0: GPG INITIALIZATION")
            self.logger.info("=" * 60)
            
            if self.gpg_handler.gpg_enabled:
                if not self.gpg_handler.import_gpg_key():
                    self.logger.error("âŒ Failed to import GPG key, disabling signing")
                    self.gpg_handler.gpg_enabled = False
                else:
                    self.logger.info("âœ… GPG initialized successfully")
            else:
                self.logger.info("â„¹ï¸ GPG signing disabled (no key provided)")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 1: SSH CONNECTION TEST")
            self.logger.info("=" * 60)
            
            if not self.ssh_client.test_connection():
                self.logger.error("âŒ SSH connection failed")
                return 1
            
            self.ssh_client.debug_remote_directory()
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 2: REMOTE DIRECTORY SETUP")
            self.logger.info("=" * 60)
            
            if not self.ssh_client.ensure_directory():
                self.logger.warning("âš ï¸ Could not ensure remote directory exists")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 3: TEMPORARY REPOSITORY CLONE")
            self.logger.info("=" * 60)
            
            if not self._setup_temp_repo():
                self.logger.error("âŒ Failed to setup temporary repository clone")
                return 1
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 4: PACKAGE DISCOVERY")
            self.logger.info("=" * 60)
            
            self.local_packages, self.aur_packages = self._get_package_lists()
            
            self.logger.info(f"ðŸ“¦ Package statistics:")
            self.logger.info(f"   Local packages: {len(self.local_packages)}")
            self.logger.info(f"   AUR packages: {len(self.aur_packages)}")
            self.logger.info(f"   Total packages: {len(self.local_packages) + len(self.aur_packages)}")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 5: LOCAL PACKAGE BUILDING (TRACKING SYSTEM)")
            self.logger.info("=" * 60)
            
            self._build_local_packages_with_tracking()
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 6: AUR PACKAGE BUILDING")
            self.logger.info("=" * 60)
            
            self._build_aur_packages_server_first()
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 7: COMMIT AND PUSH CHANGES")
            self.logger.info("=" * 60)
            
            if not self._commit_and_push_changes():
                self.logger.error("âŒ Failed to commit and push changes")
                # Don't fail the build - continue with database update
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 8: SELF-HEALING DATABASE UPDATE")
            self.logger.info("=" * 60)
            
            # ALWAYS run self-healing database update
            db_update_success = self._force_database_update()
            
            if not db_update_success:
                self.logger.error("\nâŒ Database update failed!")
                return 1
            
            self.gpg_handler.cleanup()
            self.logger.info("\nâœ… Repository maintenance completed successfully!")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 9: FINAL STATISTICS")
            self.logger.info("=" * 60)
            
            self.build_state.mark_complete()
            summary = self.build_state.get_summary()
            
            self.logger.info(f"Duration: {summary['duration_seconds']:.1f}s")
            self.logger.info(f"AUR packages:    {summary['aur_success']} built, {summary['aur_skipped']} adopted, {summary['aur_failed']} failed")
            self.logger.info(f"Local packages:  {summary['local_success']} built, {summary['local_skipped']} adopted, {summary['local_failed']} failed")
            self.logger.info(f"Total built:     {summary['built']}")
            self.logger.info(f"Total adopted:   {summary['skipped']}")
            self.logger.info(f"GPG signing:     {'Enabled' if self.gpg_handler.gpg_enabled else 'Disabled'}")
            self.logger.info(f"Temporary clone: âœ… Implemented")
            self.logger.info(f"PKGBUILD tracking: âœ… Implemented")
            self.logger.info(f"Self-healing DB update: âœ… Implemented")
            self.logger.info(f"Packages recovered from VPS: {len(self._recovered_packages)}")
            self.logger.info(f"Packages missing from DB: {len(self._missing_from_db)}")
            self.logger.info(f"Git commit & push: âœ… Completed")
            
            state_summary = self.version_tracker.get_state_summary()
            self.logger.info(f"Packages tracked: {state_summary['total_packages']}")
            self.logger.info("=" * 60)
            
            return 0
            
        except Exception as e:
            self.logger.error(f"\nâŒ Build failed: {e}")
            import traceback
            traceback.print_exc()
            
            if hasattr(self, 'gpg_handler'):
                self.gpg_handler.cleanup()
            
            if hasattr(self, 'version_tracker'):
                self.version_tracker.save_state()
            
            if hasattr(self, '_staging_dir') and self._staging_dir and os.path.exists(self._staging_dir):
                shutil.rmtree(self._staging_dir, ignore_errors=True)
            
            if hasattr(self, '_temp_clone_dir') and self._temp_clone_dir and os.path.exists(self._temp_clone_dir):
                shutil.rmtree(self._temp_clone_dir, ignore_errors=True)
            
            self._cleanup_git_ssh()
            
            return 1
        finally:
            self._cleanup_git_ssh()
--- FILE: .github/scripts/modules/gpg/__init__.py ---
"""
GPG and signing operations
"""

# GPG handler already exists, will be moved in a later phase
--- FILE: .github/scripts/modules/gpg/gpg_handler.py ---
"""
GPG Handler Module - Handles GPG key import, signing, and pacman-key operations
"""

import os
import subprocess
import shutil
import tempfile
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


class GPGHandler:
    """Handles GPG key import, repository signing, and pacman-key operations"""
    
    def __init__(self):
        self.gpg_private_key = os.getenv('GPG_PRIVATE_KEY')
        self.gpg_key_id = os.getenv('GPG_KEY_ID')
        self.gpg_enabled = bool(self.gpg_private_key and self.gpg_key_id)
        self.gpg_home = None
        self.gpg_env = None
        
        # Safe logging - no sensitive information
        if self.gpg_key_id:
            logger.info(f"GPG Environment Check: Key ID found: YES, Key data found: {'YES' if self.gpg_private_key else 'NO'}")
        else:
            logger.info("GPG Environment Check: No GPG key ID configured")
    
    def import_gpg_key(self) -> bool:
        """Import GPG private key and set trust level WITHOUT interactive terminal (container-safe)"""
        if not self.gpg_enabled:
            logger.info("GPG Key not detected. Skipping repository signing.")
            return False
        
        logger.info("GPG Key detected. Importing private key...")
        
        # Handle both string and bytes for the private key
        key_data = self.gpg_private_key
        if isinstance(key_data, bytes):
            key_data_str = key_data.decode('utf-8')
        else:
            key_data_str = str(key_data)
        
        # Validate private key format before attempting import
        if not key_data_str or '-----BEGIN PGP PRIVATE KEY BLOCK-----' not in key_data_str:
            logger.error("âŒ CRITICAL: Invalid GPG private key format.")
            logger.error("Disabling GPG signing for this build.")
            self.gpg_enabled = False
            return False
        
        try:
            # Create a temporary GPG home directory
            temp_gpg_home = tempfile.mkdtemp(prefix="gpg_home_")
            
            # Set environment for GPG
            env = os.environ.copy()
            env['GNUPGHOME'] = temp_gpg_home
            
            # Import the private key
            if isinstance(self.gpg_private_key, bytes):
                key_input = self.gpg_private_key
            else:
                key_input = self.gpg_private_key.encode('utf-8')
            
            import_process = subprocess.run(
                ['gpg', '--batch', '--import'],
                input=key_input,
                capture_output=True,
                text=False,
                env=env,
                check=False
            )
            
            if import_process.returncode != 0:
                stderr = import_process.stderr.decode('utf-8') if isinstance(import_process.stderr, bytes) else import_process.stderr
                logger.error(f"Failed to import GPG key: {stderr}")
                shutil.rmtree(temp_gpg_home, ignore_errors=True)
                return False
            
            logger.info("âœ… GPG key imported successfully")
            
            # Get fingerprint and set ultimate trust
            list_process = subprocess.run(
                ['gpg', '--list-keys', '--with-colons', self.gpg_key_id],
                capture_output=True,
                text=True,
                env=env,
                check=False
            )
            
            fingerprint = None
            if list_process.returncode == 0:
                for line in list_process.stdout.split('\n'):
                    if line.startswith('fpr:'):
                        parts = line.split(':')
                        if len(parts) > 9:
                            fingerprint = parts[9]
                            # Set ultimate trust (6 = ultimate)
                            trust_process = subprocess.run(
                                ['gpg', '--import-ownertrust'],
                                input=f"{fingerprint}:6:\n".encode('utf-8'),
                                capture_output=True,
                                text=False,
                                env=env,
                                check=False
                            )
                            if trust_process.returncode == 0:
                                logger.info("âœ… Set ultimate trust for GPG key")
                            break
            
            # Export public key and add to pacman-key WITHOUT interactive terminal
            if fingerprint:
                try:
                    # Export public key to a temporary file
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.asc', delete=False) as pub_key_file:
                        export_process = subprocess.run(
                            ['gpg', '--armor', '--export', fingerprint],
                            capture_output=True,
                            text=True,
                            env=env,
                            check=True
                        )
                        pub_key_file.write(export_process.stdout)
                        pub_key_path = pub_key_file.name
                    
                    # Add to pacman-key WITH SUDO
                    logger.info("Adding GPG key to pacman-key...")
                    add_process = subprocess.run(
                        ['sudo', 'pacman-key', '--add', pub_key_path],
                        capture_output=True,
                        text=True,
                        check=False
                    )
                    
                    if add_process.returncode != 0:
                        logger.error(f"Failed to add key to pacman-key: {add_process.stderr}")
                    else:
                        logger.info("âœ… Key added to pacman-key")
                    
                    # Import ownertrust into pacman keyring
                    logger.info("Setting ultimate trust in pacman keyring...")
                    ownertrust_content = f"{fingerprint}:6:\n"
                    
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.trust', delete=False) as trust_file:
                        trust_file.write(ownertrust_content)
                        trust_file_path = trust_file.name
                    
                    trust_cmd = [
                        'sudo', 'gpg',
                        '--homedir', '/etc/pacman.d/gnupg',
                        '--batch',
                        '--import-ownertrust',
                        trust_file_path
                    ]
                    
                    try:
                        trust_process = subprocess.run(
                            trust_cmd,
                            capture_output=True,
                            text=True,
                            check=False
                        )
                        
                        if trust_process.returncode == 0:
                            logger.info("âœ… Set ultimate trust for key in pacman keyring")
                        else:
                            logger.warning(f"âš ï¸ Failed to set trust with gpg: {trust_process.stderr[:200]}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Error setting trust with gpg: {e}")
                    finally:
                        os.unlink(trust_file_path)
                        os.unlink(pub_key_path)
                    
                except Exception as e:
                    logger.error(f"Error during pacman-key setup: {e}")
            
            # Store the GPG home directory for later use
            self.gpg_home = temp_gpg_home
            self.gpg_env = env
            
            return True
            
        except Exception as e:
            logger.error(f"Error importing GPG key: {e}")
            if 'temp_gpg_home' in locals():
                shutil.rmtree(temp_gpg_home, ignore_errors=True)
            return False
    
    def sign_repository_files(self, repo_name: str, output_dir: str) -> bool:
        """Sign repository database files with GPG"""
        if not self.gpg_enabled:
            logger.info("GPG signing disabled - skipping repository signing")
            return False
        
        if not hasattr(self, 'gpg_home') or not hasattr(self, 'gpg_env'):
            logger.error("GPG key not imported. Cannot sign repository files.")
            return False
        
        try:
            output_path = Path(output_dir)
            files_to_sign = [
                output_path / f"{repo_name}.db",
                output_path / f"{repo_name}.files"
            ]
            
            signed_count = 0
            failed_count = 0
            
            for file_to_sign in files_to_sign:
                if not file_to_sign.exists():
                    logger.warning(f"Repository file not found for signing: {file_to_sign.name}")
                    continue
                
                logger.info(f"Signing repository database: {file_to_sign.name}")
                
                # Create detached signature
                sig_file = file_to_sign.with_suffix(file_to_sign.suffix + '.sig')
                
                sign_process = subprocess.run(
                    [
                        'gpg', '--detach-sign',
                        '--default-key', self.gpg_key_id,
                        '--output', str(sig_file),
                        str(file_to_sign)
                    ],
                    capture_output=True,
                    text=True,
                    env=self.gpg_env,
                    check=False
                )
                
                if sign_process.returncode == 0:
                    logger.info(f"âœ… Created signature: {sig_file.name}")
                    signed_count += 1
                else:
                    logger.warning(f"âš ï¸ Failed to sign {file_to_sign.name}: {sign_process.stderr[:200]}")
                    failed_count += 1
            
            if signed_count > 0:
                logger.info(f"âœ… Successfully signed {signed_count} repository file(s)")
                # CRITICAL FIX: Minor warnings should not block the build
                if failed_count > 0:
                    logger.warning(f"âš ï¸ {failed_count} file(s) failed to sign, but continuing anyway")
                return True
            else:
                logger.error("Failed to sign any repository files")
                # CRITICAL FIX: Don't fail the build if GPG signing has issues
                logger.warning("âš ï¸ Continuing build without GPG signatures")
                return False
                
        except Exception as e:
            logger.error(f"Error signing repository files: {e}")
            # CRITICAL FIX: Don't fail the build if GPG signing has issues
            logger.warning("âš ï¸ Continuing build without GPG signatures due to error")
            return False
    
    def cleanup(self):
        """Clean up temporary GPG home directory"""
        if hasattr(self, 'gpg_home'):
            try:
                shutil.rmtree(self.gpg_home, ignore_errors=True)
                logger.debug("Cleaned up temporary GPG home directory")
            except Exception as e:
                logger.warning(f"Could not clean up GPG directory: {e}")
--- FILE: .github/scripts/config.py ---
"""
Configuration file for Manjaro Package Builder
"""

import os

# Repository configuration
REPO_DB_NAME = "manjaro-awesome"  # Default repository name
OUTPUT_DIR = "built_packages"     # Local output directory
BUILD_TRACKING_DIR = ".buildtracking"  # Build tracking directory

# PACKAGER identity from environment variable (secure via GitHub Secrets)
PACKAGER_ID = os.getenv("PACKAGER_ENV", "Maintainer <no-reply@gshoots.hu>")

# SSH and Git configuration
SSH_REPO_URL = "git@github.com:megvadulthangya/manjaro-awesome.git"
SSH_OPTIONS = [
    "-o", "StrictHostKeyChecking=no",
    "-o", "ConnectTimeout=30",
    "-o", "BatchMode=yes"
]

# Build timeouts (seconds)
MAKEPKG_TIMEOUT = {
    "default": 3600,        # 1 hour for normal packages
    "large_packages": 7200, # 2 hours for large packages (gtk, qt, chromium)
    "simplescreenrecorder": 5400,  # 1.5 hours
}

# Special dependency mappings
SPECIAL_DEPENDENCIES = {
    "gtk2": ["gtk-doc", "docbook-xsl", "libxslt", "gobject-introspection"],
    "awesome-git": ["lua", "lgi", "imagemagick", "asciidoc"],
    "awesome-freedesktop-git": ["lua", "lgi", "imagemagick", "asciidoc"],
    "lain-git": ["lua", "lgi", "imagemagick", "asciidoc"],
    "simplescreenrecorder": ["jack2"],  # Convert jack to jack2
}

# Build tool checks (will be installed if missing)
REQUIRED_BUILD_TOOLS = [
    "make", "gcc", "pkg-config", "autoconf", "automake", 
    "libtool", "cmake", "meson", "ninja", "patch"
]

# Temporary directories (runtime-required, /tmp is POSIX invariant)
MIRROR_TEMP_DIR = "/tmp/repo_mirror"
SYNC_CLONE_DIR = "/tmp/manjaro-awesome-gitclone"

# AUR configuration
AUR_URLS = [
    "https://aur.archlinux.org/{pkg_name}.git",
    "git://aur.archlinux.org/{pkg_name}.git"
]

# Build directory names
AUR_BUILD_DIR = "build_aur"

# GitHub repository for synchronization
GITHUB_REPO = "megvadulthangya/manjaro-awesome.git"

# Debug mode configuration - when True, bypass logger for critical build output
DEBUG_MODE = True
--- FILE: .github/scripts/packages.py ---
"""
Package definitions for Manjaro Package Builder
"""

# LOCAL packages (from our repository)
LOCAL_PACKAGES = [
    "gghelper",
#    "gtk2",
#    "awesome-freedesktop-git",
#    "lain-git",
    "awesome-rofi",
#    "awesome-git",
    "awesome-welcome",
    "tilix-git",
    "nordic-backgrounds",
    "awesome-copycats-manjaro",
    "i3lock-fancy-git",
    "ttf-font-awesome-5",
    "nvidia-driver-assistant"
#    "grayjay-bin"
]

# AUR packages (from Arch User Repository)
AUR_PACKAGES = [
    "awesome-git",
    "awesome-freedesktop-git",
    "lain-git",
    "grayjay-bin",
    "libinput-gestures",
    "gtk2",
    "gtkd",
    "qt5-styleplugins",
    "urxvt-resize-font-git",
    "i3lock-color",
    "raw-thumbnailer",
    "gsconnect",
    "tamzen-font",
    "betterlockscreen",
    "nordic-theme",
#    "nordic-darker-theme",
    "geany-nord-theme",
    "geany-plugin-preview",
    "nordzy-icon-theme",
    "oh-my-posh-bin",
    "fish-done",
#    "tilix-git",
    "find-the-command",
    "p7zip-gui",
    "qownnotes",
    "xorg-font-utils",
    "xnviewmp",
    "simplescreenrecorder",
    "gtkhash-thunar",
    "a4tech-bloody-driver-git",
#    "nordic-bluish-accent-theme",
#    "nordic-bluish-accent-standard-buttons-theme",
#    "nordic-polar-standard-buttons-theme",
#    "nordic-standard-buttons-theme",
#    "nordic-darker-standard-buttons-theme"
]

# Optionally, you can also define package groups or categories
PACKAGE_CATEGORIES = {
    "desktop": ["awesome-freedesktop-git", "lain-git", "awesome-rofi"],
    "themes": ["nordic-theme", "nordic-darker-theme", "nordic-bluish-accent-theme"],
    "fonts": ["tamzen-font", "ttf-font-awesome-5"],
    "tools": ["libinput-gestures", "betterlockscreen", "simplescreenrecorder"],
    "drivers": ["nvidia-driver-assistant", "a4tech-bloody-driver-git"]
}

--- FILE: .github/scripts/builder.py ---
#!/usr/bin/env python3
"""
Manjaro Package Builder - Refactored Modular Architecture with Zero-Residue Policy
Main entry point for the refactored modular system
"""

import os
import sys
import traceback

# Add the modules directory to sys.path
script_dir = os.path.dirname(os.path.abspath(__file__))
modules_dir = os.path.join(script_dir, "modules")
sys.path.insert(0, modules_dir)

# Try to import from the new modular structure
try:
    from modules.orchestrator.package_builder import PackageBuilder
    print(">>> DEBUG: Successfully imported PackageBuilder from modular system")
    MODULES_LOADED = True
except ImportError as e:
    print(f"âŒ CRITICAL: Failed to import PackageBuilder: {e}")
    print(f"âŒ Please ensure modules are in: {modules_dir}/")
    print(f"âŒ Current sys.path: {sys.path}")
    MODULES_LOADED = False
    sys.exit(1)
except Exception as e:
    print(f"âŒ CRITICAL: Error importing PackageBuilder: {e}")
    MODULES_LOADED = False
    sys.exit(1)


def main() -> int:
    """
    Main entry point for the package builder
    
    Returns:
        Exit code (0 for success, 1 for failure)
    """
    try:
        if not MODULES_LOADED:
            print("âŒ Modules not loaded, cannot proceed")
            return 1
        
        print(">>> DEBUG: Starting PackageBuilder.run()")
        builder = PackageBuilder()
        return builder.run()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Build interrupted by user")
        return 130  # Standard exit code for Ctrl+C
    except SystemExit as e:
        # Re-raise system exit to preserve exit code
        raise e
    except Exception as e:
        print(f"\nâŒ UNEXPECTED ERROR in main(): {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
--- FILE: .github/scripts/rsync_upload.py ---
#!/usr/bin/env python3
"""
RSYNC Upload Test - Python Version
Ez a szkript teszteli a fÃ¡jlfeltÃ¶ltÃ©st RSYNC-vel egy tÃ¡voli szerverre.
"""

import os
import sys
import time
import subprocess
import tarfile
from pathlib import Path
from datetime import datetime
from typing import Tuple, Optional, List, Dict
import shutil
import stat

# === KONSTANSOK ===
OUTPUT_DIR = Path("/home/builder/built_packages")
TEST_PREFIX = f"github_test_{int(time.time())}"

# === KONFIGURÃCIÃ“ ===
class Config:
    """KonfigurÃ¡ciÃ³s osztÃ¡ly"""
    def __init__(self):
        self.remote_dir = os.environ.get("REMOTE_DIR", "/var/www/repo")
        self.vps_user = os.environ.get("VPS_USER", "root")
        self.vps_host = os.environ.get("VPS_HOST", "")
        self.test_size_mb = int(os.environ.get("TEST_SIZE_MB", "10"))
        
        # EllenÅ‘rizzÃ¼k a kÃ¶telezÅ‘ vÃ¡ltozÃ³kat
        if not self.vps_host:
            raise ValueError("VPS_HOST nincs beÃ¡llÃ­tva!")
        
        # SSH utasÃ­tÃ¡s
        self.ssh_cmd = ["ssh", "-o", "StrictHostKeyChecking=no", 
                       "-o", "ConnectTimeout=30", "-o", "BatchMode=yes"]

# === LOGOLÃS ===
class Logger:
    """LogolÃ³ osztÃ¡ly"""
    
    @staticmethod
    def log(level: str, message: str):
        timestamp = datetime.now().strftime("%H:%M:%S")
        level_icons = {
            "INFO": "â„¹ï¸",
            "SUCCESS": "âœ…",
            "ERROR": "âŒ",
            "WARNING": "âš ï¸"
        }
        icon = level_icons.get(level, "")
        print(f"[{timestamp}] {icon} {message}")
    
    @staticmethod
    def info(message: str):
        Logger.log("INFO", message)
    
    @staticmethod
    def success(message: str):
        Logger.log("SUCCESS", message)
    
    @staticmethod
    def error(message: str):
        Logger.log("ERROR", message)
    
    @staticmethod
    def warning(message: str):
        Logger.log("WARNING", message)

# === FÅ OSZTÃLY ===
class RsyncUploadTester:
    """RSYNC feltÃ¶ltÃ©s tesztelÅ‘"""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = Logger()
        self.test_files: List[Path] = []
        
        # Kimeneti kÃ¶nyvtÃ¡r lÃ©trehozÃ¡sa
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        # JogosultsÃ¡gok beÃ¡llÃ­tÃ¡sa
        os.chmod(OUTPUT_DIR, stat.S_IRWXU | stat.S_IRWXG | stat.S_IROTH | stat.S_IXOTH)
    
    def run_command(self, cmd: List[str], check: bool = True, 
                    capture: bool = False, shell: bool = False) -> Tuple[int, str, str]:
        """Parancs futtatÃ¡sa"""
        try:
            self.logger.info(f"FuttatÃ¡s: {' '.join(cmd) if not shell else cmd}")
            
            if shell and isinstance(cmd, list):
                cmd = " ".join(cmd)
            
            result = subprocess.run(
                cmd, 
                check=check, 
                capture_output=capture,
                text=True,
                shell=shell
            )
            return (
                result.returncode,
                result.stdout if capture else "",
                result.stderr if capture else ""
            )
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Parancs hibÃ¡san fejezÅ‘dÃ¶tt be: {e}")
            if capture:
                return (e.returncode, e.stdout, e.stderr)
            if check:
                raise
            return (e.returncode, "", str(e))
        except Exception as e:
            self.logger.error(f"Parancs futtatÃ¡si hiba: {e}")
            if check:
                raise
            return (1, "", str(e))
    
    def ssh_command(self, remote_cmd: str, check: bool = True) -> Tuple[int, str, str]:
        """SSH parancs futtatÃ¡sa"""
        full_cmd = self.config.ssh_cmd + [
            f"{self.config.vps_user}@{self.config.vps_host}",
            remote_cmd
        ]
        return self.run_command(full_cmd, check=check, capture=True)
    
    def test_ssh_connection(self) -> bool:
        """SSH kapcsolat tesztelÃ©se"""
        self.logger.info("1. SSH kapcsolat teszt...")
        
        try:
            returncode, stdout, stderr = self.ssh_command("echo 'SSH OK' && hostname")
            if returncode == 0:
                self.logger.success(f"SSH kapcsolat rendben - {stdout.strip()}")
                return True
            else:
                self.logger.error(f"SSH kapcsolat sikertelen: {stderr}")
                return False
        except Exception as e:
            self.logger.error(f"SSH kapcsolat hiba: {e}")
            return False
    
    def test_remote_directory(self) -> bool:
        """TÃ¡voli kÃ¶nyvtÃ¡r ellenÅ‘rzÃ©se"""
        self.logger.info("2. TÃ¡voli kÃ¶nyvtÃ¡r ellenÅ‘rzÃ©se...")
        
        remote_dir = self.config.remote_dir
        returncode, stdout, stderr = self.ssh_command(
            f"if [ -d '{remote_dir}' ]; then "
            f"echo 'KÃ¶nyvtÃ¡r lÃ©tezik' && ls -ld '{remote_dir}'; "
            f"else echo 'KÃ¶nyvtÃ¡r nem lÃ©tezik, lÃ©trehozom...' && "
            f"sudo mkdir -p '{remote_dir}' && sudo chmod 755 '{remote_dir}'; fi"
        )
        
        if returncode == 0:
            self.logger.success(f"KÃ¶nyvtÃ¡r rendben: {stdout.splitlines()[0] if stdout else 'OK'}")
            return True
        else:
            self.logger.error(f"KÃ¶nyvtÃ¡r problÃ©ma: {stderr}")
            return False
    
    def create_dummy_file(self, path: Path, size_mb: int) -> bool:
        """Dummy fÃ¡jl lÃ©trehozÃ¡sa"""
        try:
            # MB-ban megadott mÃ©ret byte-okra konvertÃ¡lÃ¡sa
            size_bytes = size_mb * 1024 * 1024
            
            # VÃ©letlenszerÅ± adatokkal feltÃ¶ltÃ©s
            with open(path, 'wb') as f:
                # 1MB-os blokkokban Ã­runk a hatÃ©konysÃ¡g Ã©rdekÃ©ben
                block_size = 1024 * 1024  # 1MB
                blocks = size_mb
                remaining = size_bytes % block_size
                
                for i in range(blocks):
                    f.write(os.urandom(block_size))
                
                if remaining > 0:
                    f.write(os.urandom(remaining))
            
            # JogosultsÃ¡gok beÃ¡llÃ­tÃ¡sa
            os.chmod(path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)
            return True
            
        except Exception as e:
            self.logger.error(f"Hiba a fÃ¡jl lÃ©trehozÃ¡sakor {path}: {e}")
            return False
    
    def create_test_files(self) -> bool:
        """TesztfÃ¡jlok lÃ©trehozÃ¡sa"""
        self.logger.info("3. TesztfÃ¡jlok lÃ©trehozÃ¡sa...")
        
        try:
            # TÃ¶rÃ¶ljÃ¼k a rÃ©gi fÃ¡jlokat
            for f in OUTPUT_DIR.glob("*"):
                try:
                    f.unlink()
                except:
                    pass
            
            # FÃ¡jlmÃ©retek - VALÃ“DI PKG NEVEKKEL
            file_specs = [
                ("awesome-git-4.0.r123.gabc123def-1-x86_64.pkg.tar.zst", 5),
                ("nvidia-driver-470.199.02-1-x86_64.pkg.tar.zst", 190),
                (f"custom-package-1.0.{self.config.test_size_mb}-1-x86_64.pkg.tar.zst", self.config.test_size_mb),
            ]
            
            # FÃ¡jlok lÃ©trehozÃ¡sa
            for filename, size_mb in file_specs:
                self.logger.info(f"  - {filename} ({size_mb}MB)...")
                filepath = OUTPUT_DIR / filename
                
                if self.create_dummy_file(filepath, size_mb):
                    self.test_files.append(filepath)
                else:
                    self.logger.error(f"Nem sikerÃ¼lt lÃ©trehozni: {filename}")
                    return False
            
            # AdatbÃ¡zis fÃ¡jl lÃ©trehozÃ¡sa (tar.gz)
            self.logger.info("  - AdatbÃ¡zis fÃ¡jl...")
            db_filename = OUTPUT_DIR / "test-repo.db.tar.gz"
            
            try:
                import gzip
                import io
                
                # EgyszerÅ± tar.gz fÃ¡jl lÃ©trehozÃ¡sa
                with tarfile.open(db_filename, "w:gz") as tar:
                    for test_file in self.test_files:
                        tar.add(test_file, arcname=test_file.name)
                
                self.test_files.append(db_filename)
                
            except Exception as e:
                self.logger.warning(f"AdatbÃ¡zis fÃ¡jl lÃ©trehozÃ¡sa nem sikerÃ¼lt: {e}")
                # LÃ©trehozunk egy Ã¼res adatbÃ¡zis fÃ¡jlt
                with open(db_filename, 'wb') as f:
                    f.write(b"dummy repo database")
                self.test_files.append(db_filename)
            
            # FÃ¡jlinformÃ¡ciÃ³k
            self.logger.info("FÃ¡jlok elkÃ©szÃ¼ltek:")
            total_size = 0
            for f in self.test_files:
                size = f.stat().st_size
                size_mb = size / (1024 * 1024)
                total_size += size_mb
                self.logger.info(f"    {f.name} - {size_mb:.1f}MB")
            
            self.logger.info(f"    Ã–sszesen: {total_size:.1f}MB")
            return True
            
        except Exception as e:
            self.logger.error(f"FÃ¡jl lÃ©trehozÃ¡si hiba: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_rsync_upload(self) -> bool:
        """RSYNC feltÃ¶ltÃ©s futtatÃ¡sa"""
        self.logger.info("4. RSYNC feltÃ¶ltÃ©s indÃ­tÃ¡sa...")
        self.logger.info(f"  ForrÃ¡s: {OUTPUT_DIR}/")
        self.logger.info(f"  CÃ©l: {self.config.vps_user}@{self.config.vps_host}:{self.config.remote_dir}/")
        
        # EllenÅ‘rizzÃ¼k, vannak-e fÃ¡jlok
        if not self.test_files:
            self.logger.error("Nincsenek feltÃ¶lthetÅ‘ fÃ¡jlok!")
            return False
        
        # GyÅ±jtsÃ¼k Ã¶ssze a fÃ¡jlokat
        file_patterns = [
            str(OUTPUT_DIR / "*.pkg.tar.zst"),
            str(OUTPUT_DIR / "*.db.tar.gz")
        ]
        
        # Shell glob hasznÃ¡lata a fÃ¡jlok keresÃ©sÃ©re
        import glob
        files_to_upload = []
        for pattern in file_patterns:
            files_to_upload.extend(glob.glob(pattern))
        
        if not files_to_upload:
            self.logger.error("Nem talÃ¡lhatÃ³k fÃ¡jlok a glob pattern alapjÃ¡n!")
            self.logger.info(f"Glob pattern: {file_patterns}")
            self.logger.info(f"OUTPUT_DIR tartalma: {list(OUTPUT_DIR.iterdir())}")
            return False
        
        self.logger.info(f"  FeltÃ¶ltendÅ‘ fÃ¡jlok ({len(files_to_upload)} db):")
        for f in files_to_upload:
            size_mb = os.path.getsize(f) / (1024 * 1024)
            self.logger.info(f"    - {os.path.basename(f)} ({size_mb:.1f}MB)")
        
        # RSYNC parancs Ã¶sszeÃ¡llÃ­tÃ¡sa - SHELL MODBAN!
        rsync_cmd = f"""
        rsync -avz \
          --progress \
          --stats \
          --chmod=0644 \
          -e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=30 -o BatchMode=yes" \
          {" ".join(f"'{f}'" for f in files_to_upload)} \
          '{self.config.vps_user}@{self.config.vps_host}:{self.config.remote_dir}/'
        """
        
        start_time = time.time()
        
        try:
            self.logger.info("RSYNC futtatÃ¡sa...")
            
            # RSYNC futtatÃ¡sa shell mÃ³dban
            returncode, stdout, stderr = self.run_command(
                rsync_cmd,
                check=False,
                capture=True,
                shell=True
            )
            
            # Kimenet kiÃ­rÃ¡sa
            if stdout:
                for line in stdout.splitlines():
                    if line.strip():
                        print(f"    {line}")
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            if returncode == 0:
                self.logger.success(f"RSYNC sikeres! ({duration} mÃ¡sodperc)")
                
                # StatisztikÃ¡k kinyerÃ©se
                if "sent" in stdout.lower():
                    for line in stdout.splitlines():
                        if "sent" in line.lower() and "received" in line.lower():
                            self.logger.info(f"    Ãtvitel: {line.strip()}")
                
                # FÃ¡jlok ellenÅ‘rzÃ©se
                self.verify_remote_files()
                return True
            else:
                self.logger.error(f"RSYNC sikertelen! (return code: {returncode})")
                if stderr:
                    self.logger.error(f"RSYNC hiba: {stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"RSYNC futtatÃ¡si hiba: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def verify_remote_files(self):
        """TÃ¡voli fÃ¡jlok ellenÅ‘rzÃ©se"""
        self.logger.info("5. FÃ¡jlok ellenÅ‘rzÃ©se a szerveren...")
        
        remote_cmd = f"""
        echo "=== SZERVER FÃJLOK ==="
        ls -la "{self.config.remote_dir}/" 2>/dev/null | head -20
        echo ""
        echo "=== PKG FÃJLOK ==="
        ls -lh "{self.config.remote_dir}/"*.pkg.tar.* 2>/dev/null || echo "Nincsenek .pkg.tar fÃ¡jlok"
        echo ""
        echo "=== DB FÃJL ==="
        ls -lh "{self.config.remote_dir}/"*.db.tar.gz 2>/dev/null || echo "Nincs .db.tar.gz fÃ¡jl"
        echo ""
        echo "=== HELY FOGYASZTÃS ==="
        du -sh "{self.config.remote_dir}/" 2>/dev/null || echo "Nem elÃ©rhetÅ‘"
        """
        
        returncode, stdout, stderr = self.ssh_command(remote_cmd, check=False)
        
        if returncode == 0 and stdout:
            for line in stdout.splitlines():
                if line.strip():
                    print(f"    {line}")
        elif stderr:
            self.logger.warning(f"EllenÅ‘rzÃ©s hibÃ¡ja: {stderr}")
    
    def cleanup(self):
        """TakarÃ­tÃ¡s"""
        self.logger.info("6. TakarÃ­tÃ¡s...")
        
        # LokÃ¡lis fÃ¡jlok tÃ¶rlÃ©se
        try:
            # TÃ¶rÃ¶ljÃ¼k a teljes OUTPUT_DIR tartalmÃ¡t
            for item in OUTPUT_DIR.iterdir():
                try:
                    if item.is_file():
                        item.unlink()
                    elif item.is_dir():
                        shutil.rmtree(item)
                except Exception as e:
                    self.logger.warning(f"Nem sikerÃ¼lt tÃ¶rÃ¶lni {item}: {e}")
            
            self.logger.success("LokÃ¡lis fÃ¡jlok tÃ¶rÃ¶lve")
        except Exception as e:
            self.logger.error(f"LokÃ¡lis tÃ¶rlÃ©s hiba: {e}")
        
        # TÃ¡voli tesztfÃ¡jlok tÃ¶rlÃ©se
        try:
            # Csak a mai tesztfÃ¡jlokat tÃ¶rÃ¶ljÃ¼k
            remote_cmd = f"""
            echo "TÃ¡voli tesztfÃ¡jlok tÃ¶rlÃ©se..."
            # TÃ¶rÃ¶ljÃ¼k az Ã¶sszes .pkg.tar.zst fÃ¡jlt
            rm -f "{self.config.remote_dir}/"*.pkg.tar.zst 2>/dev/null
            # TÃ¶rÃ¶ljÃ¼k az Ã¶sszes .db.tar.gz fÃ¡jlt
            rm -f "{self.config.remote_dir}/"*.db.tar.gz 2>/dev/null
            echo "âœ… TÃ¡voli tesztfÃ¡jlok tÃ¶rÃ¶lve"
            """
            
            returncode, stdout, stderr = self.ssh_command(remote_cmd, check=False)
            if returncode == 0:
                if stdout:
                    self.logger.success(stdout.splitlines()[-1] if stdout else "TÃ¶rÃ¶lve")
            else:
                self.logger.warning(f"TÃ¡voli tÃ¶rlÃ©s figyelmeztetÃ©s: {stderr}")
        except Exception as e:
            self.logger.warning(f"TÃ¡voli tÃ¶rlÃ©s hiba: {e}")
    
    def run(self) -> bool:
        """FÅ‘ teszt futtatÃ¡sa"""
        self.logger.info("=== RSYNC FELTÃ–LTÃ‰S TESZT (Python) ===")
        self.logger.info(f"Host: {self.config.vps_host}")
        self.logger.info(f"User: {self.config.vps_user}")
        self.logger.info(f"Remote: {self.config.remote_dir}")
        self.logger.info(f"File size: {self.config.test_size_mb}MB")
        print()
        
        # LÃ©pÃ©sek
        steps = [
            ("SSH kapcsolat", self.test_ssh_connection),
            ("KÃ¶nyvtÃ¡r ellenÅ‘rzÃ©s", self.test_remote_directory),
            ("FÃ¡jlok lÃ©trehozÃ¡sa", self.create_test_files),
        ]
        
        success = True
        for step_name, step_func in steps:
            if not step_func():
                self.logger.error(f"{step_name} sikertelen!")
                success = False
                break
        
        # RSYNC feltÃ¶ltÃ©s csak ha minden elÅ‘zÅ‘ lÃ©pÃ©s sikeres
        rsync_success = False
        if success:
            rsync_success = self.run_rsync_upload()
        
        # TakarÃ­tÃ¡s mindig
        self.cleanup()
        
        # Ã–sszefoglalÃ³
        self.print_summary(success and rsync_success)
        
        return success and rsync_success
    
    def print_summary(self, overall_success: bool):
        """Ã–sszefoglalÃ³ kiÃ­rÃ¡sa"""
        print()
        print("=" * 50)
        self.logger.info("=== TESZT VÃ‰GE ===")
        print()
        
        if overall_success:
            self.logger.success("ðŸŽ‰ RSYNC MÅ°KÃ–DIK!")
            print()
            print("âœ… Az eredeti CI script RSYNC-re Ã¡tÃ­rhatÃ³.")
            print()
            print("ðŸ“‹ Javasolt RSYNC konfigurÃ¡ciÃ³ az eredeti CI-hez:")
            print()
            print('''
            # Az eredeti scriptben cserÃ©ld le az scp rÃ©szt:
            
            # RÃ‰GI (SCP):
            # scp $SSH_OPTS $OUTPUT_DIR/* $VPS_USER@$VPS_HOST:$REMOTE_DIR/
            
            # ÃšJ (RSYNC):
            log_info "FÃ¡jlok feltÃ¶ltÃ©se RSYNC-kel..."
            rsync -avz \\
                --progress \\
                --stats \\
                --chmod=0644 \\
                -e "ssh $SSH_OPTS" \\
                "$OUTPUT_DIR/"*.pkg.tar.* \\
                "$VPS_USER@$VPS_HOST:$REMOTE_DIR/"
            ''')
        else:
            self.logger.error("RSYNC SIKERTELEN")
            print()
            print("ðŸ”§ HibaelhÃ¡rÃ­tÃ¡s:")
            print("1. EllenÅ‘rizd az SSH kulcs jogosultsÃ¡gokat")
            print("2. EllenÅ‘rizd a tÃ¡voli kÃ¶nyvtÃ¡r Ã­rÃ¡si jogosultsÃ¡gait")
            print("3. EllenÅ‘rizd a tÅ±zfal beÃ¡llÃ­tÃ¡sokat (port 22)")
            print("4. EllenÅ‘rizd, hogy a szerver elÃ©rhetÅ‘-e a kontÃ©nerbÅ‘l")
            print("5. SSH kapcsolat tesztelÃ©se kÃ©zzel:")
            print(f"   ssh -i /home/builder/.ssh/id_ed25519 {self.config.vps_user}@{self.config.vps_host}")
        
        print()
        print(f"ðŸ•’ Teszt idÅ‘pont: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 50)

# === FÅ PROGRAM ===
def main():
    """FÅ‘ program"""
    try:
        # KonfigurÃ¡ciÃ³ betÃ¶ltÃ©se
        config = Config()
        
        # TesztelÅ‘ lÃ©trehozÃ¡sa Ã©s futtatÃ¡sa
        tester = RsyncUploadTester(config)
        success = tester.run()
        
        # KilÃ©pÃ©si kÃ³d
        sys.exit(0 if success else 1)
        
    except ValueError as e:
        Logger.error(f"KonfigurÃ¡ciÃ³s hiba: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        Logger.info("Teszt megszakÃ­tva")
        sys.exit(130)
    except Exception as e:
        Logger.error(f"VÃ¡ratlan hiba: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()