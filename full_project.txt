
--- FILE: .github/scripts/checker.py ---
#!/usr/bin/env python3

import os
import sys
import py_compile
import yaml

def find_files_by_extension(root_dir, extensions):
    """Find all files with given extensions recursively"""
    matched_files = []
    for root, dirs, files in os.walk(root_dir):
        for file in files:
            if any(file.endswith(ext) for ext in extensions):
                matched_files.append(os.path.join(root, file))
    return matched_files

def check_python_file(file_path):
    """Check Python file syntax using py_compile"""
    try:
        py_compile.compile(file_path, doraise=True)
        print(f"[PASS] Python syntax: {file_path}")
        return True
    except py_compile.PyCompileError as e:
        print(f"[FAIL] Python syntax: {file_path} - {e}")
        return False
    except FileNotFoundError:
        print(f"[FAIL] Python syntax: {file_path} - File not found")
        return False
    except Exception as e:
        print(f"[FAIL] Python syntax: {file_path} - Unexpected error: {e}")
        return False

def check_yaml_file(file_path):
    """Basic YAML syntax check"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            yaml.safe_load(f)
        print(f"[PASS] YAML syntax: {file_path}")
        return True
    except yaml.YAMLError as e:
        print(f"[FAIL] YAML syntax: {file_path} - {e}")
        return False
    except FileNotFoundError:
        print(f"[FAIL] YAML syntax: {file_path} - File not found")
        return False
    except Exception as e:
        print(f"[FAIL] YAML syntax: {file_path} - Unexpected error: {e}")
        return False

def check_env_vars(vars_list):
    """Check that environment variables are not empty"""
    all_passed = True
    for var in vars_list:
        value = os.getenv(var, '')
        if value and value.strip():
            print(f"[PASS] ENV variable: {var}")
        else:
            print(f"[FAIL] ENV variable: {var} - Empty or not set")
            all_passed = False
    return all_passed

def main():
    print("=== Running Preflight Checker ===")
    
    # Track overall status
    all_checks_passed = True
    
    # Check Python files in .github/scripts/ and subdirectories
    scripts_dir = '.github/scripts'
    if os.path.exists(scripts_dir):
        python_files = find_files_by_extension(scripts_dir, ['.py'])
        
        if python_files:
            print(f"\nChecking {len(python_files)} Python file(s) in '{scripts_dir}' and subdirectories:")
            for py_file in python_files:
                if not check_python_file(py_file):
                    all_checks_passed = False
        else:
            print(f"[INFO] No Python files found in '{scripts_dir}'")
    else:
        print(f"[WARNING] Directory '{scripts_dir}' does not exist")
    
    # Check YAML files in .github/workflows/
    workflows_dir = '.github/workflows'
    if os.path.exists(workflows_dir):
        yaml_files = find_files_by_extension(workflows_dir, ['.yaml', '.yml', '.bckp'])
        
        if yaml_files:
            print(f"\nChecking {len(yaml_files)} YAML file(s) in '{workflows_dir}' and subdirectories:")
            for yaml_file in yaml_files:
                if not check_yaml_file(yaml_file):
                    all_checks_passed = False
        else:
            print(f"[INFO] No YAML files found in '{workflows_dir}'")
    else:
        print(f"[WARNING] Directory '{workflows_dir}' does not exist")
    
    # Check required environment variables
    print("\nChecking environment variables:")
    required_vars = ['VPS_USER', 'VPS_HOST', 'VPS_SSH_KEY', 'REPO_SERVER_URL']
    if not check_env_vars(required_vars):
        all_checks_passed = False
    
    print("\n" + "=" * 30)
    
    if all_checks_passed:
        print("âœ… All preflight checks passed")
        sys.exit(0)
    else:
        print("âŒ One or more preflight checks failed")
        sys.exit(1)

if __name__ == '__main__':
    main()
--- FILE: .github/scripts/modules/__init__.py ---
"""
Package Builder Modules
"""

# This file makes the modules directory a Python package
--- FILE: .github/scripts/modules/common/__init__.py ---
"""
Common utilities for the package builder system
"""

from .logging_utils import setup_logging, get_logger
from .shell_executor import ShellExecutor
from .environment import EnvironmentValidator
from .config_loader import ConfigLoader

__all__ = [
    'setup_logging',
    'get_logger',
    'ShellExecutor',
    'EnvironmentValidator',
    'ConfigLoader'
]
--- FILE: .github/scripts/modules/common/logging_utils.py ---
"""
Logging utilities for the package builder system
Handles logging configuration with support for debug mode
"""

import logging
import os
from typing import Optional


def setup_logging(debug_mode: bool = False, log_file: str = 'builder.log') -> None:
    """
    Configure logging for the application
    
    Args:
        debug_mode: If True, sets log level to DEBUG and adds more verbose output
        log_file: Path to log file
    """
    # Determine log level
    log_level = logging.DEBUG if debug_mode else logging.INFO
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    
    # Remove existing handlers to avoid duplication
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Create formatters
    console_formatter = logging.Formatter(
        '[%(asctime)s] %(levelname)s: %(message)s',
        datefmt='%H:%M:%S'
    )
    
    file_formatter = logging.Formatter(
        '[%(asctime)s] %(levelname)s - %(name)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler
    try:
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO if debug_mode else logging.WARNING)
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    except (IOError, PermissionError) as e:
        # Log but don't fail if we can't create log file
        root_logger.warning(f"Could not create log file {log_file}: {e}")
    
    # Suppress overly verbose logs from dependencies
    logging.getLogger('paramiko').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('git').setLevel(logging.WARNING)


def get_logger(name: Optional[str] = None) -> logging.Logger:
    """
    Get a logger instance
    
    Args:
        name: Logger name (usually __name__ of the calling module)
    
    Returns:
        Configured logger instance
    """
    return logging.getLogger(name)
--- FILE: .github/scripts/modules/common/shell_executor.py ---
"""
Shell command execution with comprehensive logging, timeout, and debug mode support
Extracted from PackageBuilder._run_cmd with enhanced features
"""

import os
import subprocess
import logging
from pathlib import Path
from typing import Dict, List, Optional, Union, Any


class ShellExecutor:
    """
    Executes shell commands with comprehensive logging, timeout handling,
    and optional debug mode for CI/CD output
    """
    
    def __init__(self, debug_mode: bool = False, default_timeout: int = 1800):
        """
        Initialize ShellExecutor
        
        Args:
            debug_mode: If True, bypass logger and print directly to stdout for CI/CD visibility
            default_timeout: Default command timeout in seconds
        """
        self.debug_mode = debug_mode
        self.default_timeout = default_timeout
        self.logger = logging.getLogger(__name__)
    
    def run(
        self,
        cmd: Union[str, List[str]],
        cwd: Optional[Union[str, Path]] = None,
        capture: bool = True,
        check: bool = True,
        shell: bool = True,
        user: Optional[str] = None,
        log_cmd: bool = False,
        timeout: Optional[int] = None,
        extra_env: Optional[Dict[str, str]] = None,
        **kwargs  # Accept any additional kwargs like 'text', 'capture_output', etc.
    ) -> subprocess.CompletedProcess:
        """
        Run command with comprehensive logging and timeout
        
        Args:
            cmd: Command to execute (string or list)
            cwd: Working directory
            capture: Capture stdout/stderr (alias for capture_output)
            check: Raise CalledProcessError on non-zero exit code
            shell: Use shell execution
            user: Run as specified user (requires sudo)
            log_cmd: Log command details
            timeout: Command timeout in seconds (defaults to self.default_timeout)
            extra_env: Additional environment variables
            **kwargs: Additional arguments passed to subprocess.run (text, capture_output, etc.)
        
        Returns:
            subprocess.CompletedProcess with decoded string output
        
        Raises:
            subprocess.TimeoutExpired: Command timed out
            subprocess.CalledProcessError: Command failed and check=True
        """
        if timeout is None:
            timeout = self.default_timeout
        
        # Convert cmd to string if it's a list (for logging)
        if isinstance(cmd, list):
            cmd_str = ' '.join(cmd)
            # For list commands, use shell=False unless explicitly overridden
            if 'shell' not in kwargs:
                shell = False
        else:
            cmd_str = cmd
            # For string commands, use shell=True by default
            if 'shell' not in kwargs:
                shell = True
        
        # Log command if requested
        if log_cmd or self.debug_mode:
            self._log_command(cmd_str, log_cmd)
        
        # Prepare working directory
        cwd_path = Path(cwd) if cwd else Path.cwd()
        
        # Prepare environment
        env = os.environ.copy()
        env['LC_ALL'] = 'C'  # Ensure consistent locale for command output
        
        if extra_env:
            env.update(extra_env)
        
        # Handle capture_output parameter (Python 3.7+)
        subprocess_kwargs = {
            'cwd': cwd_path,
            'shell': shell,
            'check': check,
            'env': env,
            'timeout': timeout,
            'text': True,  # Always return strings, not bytes
            'encoding': 'utf-8',
            'errors': 'ignore'
        }
        
        # Add any additional kwargs passed by caller
        subprocess_kwargs.update(kwargs)
        
        # Handle capture parameter (map to capture_output)
        if 'capture_output' not in subprocess_kwargs:
            subprocess_kwargs['capture_output'] = capture
        
        # Ensure we return text, not bytes
        if 'text' not in subprocess_kwargs or subprocess_kwargs.get('text') is False:
            subprocess_kwargs['text'] = True
            subprocess_kwargs['encoding'] = 'utf-8'
            subprocess_kwargs['errors'] = 'ignore'
        
        # Prepare command based on user
        if user:
            return self._run_as_user(cmd, user, subprocess_kwargs, log_cmd)
        else:
            return self._run_direct(cmd, subprocess_kwargs, log_cmd)
    
    def _log_command(self, cmd: str, log_cmd: bool) -> None:
        """Log command execution details"""
        if self.debug_mode:
            print(f"ðŸ”§ [DEBUG] RUNNING COMMAND: {cmd}", flush=True)
        elif log_cmd:
            self.logger.info(f"RUNNING COMMAND: {cmd}")
    
    def _log_output(self, result: subprocess.CompletedProcess, log_cmd: bool) -> None:
        """Log command output based on debug mode"""
        # Always decode to string if needed
        stdout = self._ensure_string(result.stdout)
        stderr = self._ensure_string(result.stderr)
        
        if self.debug_mode:
            if stdout:
                print(f"ðŸ”§ [DEBUG] STDOUT:\n{stdout}", flush=True)
            
            if stderr:
                print(f"ðŸ”§ [DEBUG] STDERR:\n{stderr}", flush=True)
            
            print(f"ðŸ”§ [DEBUG] EXIT CODE: {result.returncode}", flush=True)
        elif log_cmd:
            if stdout:
                self.logger.info(f"STDOUT: {stdout[:500]}")
            
            if stderr:
                self.logger.info(f"STDERR: {stderr[:500]}")
            
            self.logger.info(f"EXIT CODE: {result.returncode}")
        
        # Critical: If command failed and we're in debug mode, print full output
        if result.returncode != 0 and self.debug_mode:
            cmd_str = result.cmd if hasattr(result, 'cmd') else 'unknown'
            print(f"âŒ [DEBUG] COMMAND FAILED: {cmd_str}", flush=True)
            
            if stdout and len(stdout) > 500:
                print(f"âŒ [DEBUG] FULL STDOUT (truncated):\n{stdout[:2000]}", flush=True)
            
            if stderr and len(stderr) > 500:
                print(f"âŒ [DEBUG] FULL STDERR (truncated):\n{stderr[:2000]}", flush=True)
    
    def _ensure_string(self, value: Any) -> str:
        """Ensure value is a string, decoding bytes if necessary"""
        if value is None:
            return ""
        elif isinstance(value, str):
            # Clean string: strip whitespace and ensure no binary data
            cleaned = value.strip()
            # Remove any null bytes or control characters that might indicate binary data
            cleaned = ''.join(char for char in cleaned if ord(char) >= 32 or char == '\n' or char == '\t')
            return cleaned
        elif isinstance(value, bytes):
            try:
                decoded = value.decode('utf-8', errors='ignore')
                return decoded.strip()
            except:
                # If decoding fails, return safe string representation
                return str(value)[:1000].strip()
        else:
            return str(value).strip()
    
    def _run_as_user(
        self,
        cmd: Union[str, List[str]],
        user: str,
        subprocess_kwargs: Dict[str, Any],
        log_cmd: bool
    ) -> subprocess.CompletedProcess:
        """Run command as specified user"""
        # Set up environment for the user
        env = subprocess_kwargs.get('env', os.environ.copy())
        env['HOME'] = f'/home/{user}'
        env['USER'] = user
        subprocess_kwargs['env'] = env
        
        try:
            # Build sudo command
            shell = subprocess_kwargs.get('shell', True)
            
            if isinstance(cmd, list):
                # For list commands, build sudo command with list
                sudo_cmd = ['sudo', '-u', user] + cmd
                subprocess_kwargs['shell'] = False
            else:
                # For string commands, use shell execution
                sudo_cmd = ['sudo', '-u', user, 'bash', '-c', cmd]
                subprocess_kwargs['shell'] = True
            
            result = subprocess.run(sudo_cmd, **subprocess_kwargs)
            
            # Ensure stdout/stderr are clean strings
            if hasattr(result, 'stdout') and result.stdout is not None:
                result.stdout = self._ensure_string(result.stdout)
            
            if hasattr(result, 'stderr') and result.stderr is not None:
                result.stderr = self._ensure_string(result.stderr)
            
            if log_cmd or self.debug_mode:
                self._log_output(result, log_cmd)
            
            return result
            
        except subprocess.TimeoutExpired as e:
            error_msg = f"âš ï¸ Command timed out after {subprocess_kwargs.get('timeout', 1800)} seconds: {cmd}"
            if self.debug_mode:
                print(f"âŒ [DEBUG] {error_msg}", flush=True)
            self.logger.error(error_msg)
            raise
        except subprocess.CalledProcessError as e:
            # Ensure stdout/stderr are clean strings for the exception
            if hasattr(e, 'stdout') and e.stdout is not None:
                e.stdout = self._ensure_string(e.stdout)
            
            if hasattr(e, 'stderr') and e.stderr is not None:
                e.stderr = self._ensure_string(e.stderr)
            
            if log_cmd or self.debug_mode:
                error_msg = f"Command failed: {cmd}"
                if self.debug_mode:
                    print(f"âŒ [DEBUG] {error_msg}", flush=True)
                    if hasattr(e, 'stdout') and e.stdout:
                        print(f"âŒ [DEBUG] EXCEPTION STDOUT:\n{e.stdout}", flush=True)
                    if hasattr(e, 'stderr') and e.stderr:
                        print(f"âŒ [DEBUG] EXCEPTION STDERR:\n{e.stderr}", flush=True)
                else:
                    self.logger.error(error_msg)
            
            if subprocess_kwargs.get('check', True):
                raise
            
            # Create a CompletedProcess with decoded strings
            return subprocess.CompletedProcess(
                args=[],
                returncode=e.returncode,
                stdout=self._ensure_string(getattr(e, 'stdout', '')),
                stderr=self._ensure_string(getattr(e, 'stderr', ''))
            )
    
    def _run_direct(
        self,
        cmd: Union[str, List[str]],
        subprocess_kwargs: Dict[str, Any],
        log_cmd: bool
    ) -> subprocess.CompletedProcess:
        """Run command directly (no user switch)"""
        try:
            result = subprocess.run(cmd, **subprocess_kwargs)
            
            # Ensure stdout/stderr are clean strings
            if hasattr(result, 'stdout') and result.stdout is not None:
                result.stdout = self._ensure_string(result.stdout)
            
            if hasattr(result, 'stderr') and result.stderr is not None:
                result.stderr = self._ensure_string(result.stderr)
            
            if log_cmd or self.debug_mode:
                self._log_output(result, log_cmd)
            
            return result
            
        except subprocess.TimeoutExpired as e:
            error_msg = f"âš ï¸ Command timed out after {subprocess_kwargs.get('timeout', 1800)} seconds: {cmd}"
            if self.debug_mode:
                print(f"âŒ [DEBUG] {error_msg}", flush=True)
            self.logger.error(error_msg)
            raise
        except subprocess.CalledProcessError as e:
            # Ensure stdout/stderr are clean strings for the exception
            if hasattr(e, 'stdout') and e.stdout is not None:
                e.stdout = self._ensure_string(e.stdout)
            
            if hasattr(e, 'stderr') and e.stderr is not None:
                e.stderr = self._ensure_string(e.stderr)
            
            if log_cmd or self.debug_mode:
                error_msg = f"Command failed: {cmd}"
                if self.debug_mode:
                    print(f"âŒ [DEBUG] {error_msg}", flush=True)
                    if hasattr(e, 'stdout') and e.stdout:
                        print(f"âŒ [DEBUG] EXCEPTION STDOUT:\n{e.stdout}", flush=True)
                    if hasattr(e, 'stderr') and e.stderr:
                        print(f"âŒ [DEBUG] EXCEPTION STDERR:\n{e.stderr}", flush=True)
                else:
                    self.logger.error(error_msg)
            
            if subprocess_kwargs.get('check', True):
                raise
            
            # Create a CompletedProcess with decoded strings
            return subprocess.CompletedProcess(
                args=[],
                returncode=e.returncode,
                stdout=self._ensure_string(getattr(e, 'stdout', '')),
                stderr=self._ensure_string(getattr(e, 'stderr', ''))
            )
    
    def simple_run(self, cmd: str, check: bool = True, **kwargs) -> subprocess.CompletedProcess:
        """
        Simplified run command for common use cases
        
        Args:
            cmd: Command to execute
            check: Raise on error
            **kwargs: Additional arguments passed to run()
        
        Returns:
            subprocess.CompletedProcess
        """
        return self.run(cmd, check=check, log_cmd=False, **kwargs)
--- FILE: .github/scripts/modules/common/environment.py ---
"""
Environment validation and system path utilities
Extracted from PackageBuilder._validate_env and _get_repo_root
"""

import os
import re
import sys
import logging
from pathlib import Path
from typing import List, Tuple, Optional


class EnvironmentValidator:
    """Validates environment and provides system path utilities"""
    
    # Required environment variables
    REQUIRED_VARS = [
        'REPO_NAME',
        'VPS_HOST',
        'VPS_USER',
        'VPS_SSH_KEY',
        'REMOTE_DIR',
    ]
    
    # Optional but recommended variables
    OPTIONAL_VARS = [
        'REPO_SERVER_URL',
        'GPG_KEY_ID',
        'GPG_PRIVATE_KEY',
        'PACKAGER_ENV',
    ]
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        Initialize EnvironmentValidator
        
        Args:
            logger: Optional logger instance (creates one if not provided)
        """
        self.logger = logger or logging.getLogger(__name__)
    
    def validate(self) -> bool:
        """
        Comprehensive pre-flight environment validation
        
        Returns:
            True if validation passed, False otherwise
        
        Note:
            Exits with sys.exit(1) on critical failures
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("PRE-FLIGHT ENVIRONMENT VALIDATION")
        self.logger.info("=" * 60)
        
        # Check required variables
        missing_vars = []
        for var in self.REQUIRED_VARS:
            value = os.getenv(var)
            if not value or value.strip() == '':
                missing_vars.append(var)
                self.logger.error(f"[ERROR] Variable {var} is empty! Ensure it is set in GitHub Secrets.")
        
        if missing_vars:
            self.logger.error(f"Missing required variables: {', '.join(missing_vars)}")
            sys.exit(1)
        
        # Check optional variables and warn if missing
        for var in self.OPTIONAL_VARS:
            value = os.getenv(var)
            if not value or value.strip() == '':
                self.logger.warning(f"âš ï¸ Optional variable {var} is empty")
        
        # âœ… SECURITY FIX: DO NOT display secret information!
        self.logger.info("âœ… Environment validation passed:")
        for var in self.REQUIRED_VARS + self.OPTIONAL_VARS:
            value = os.getenv(var)
            if value and value.strip() != '':
                self.logger.info(f"   {var}: [LOADED]")
            else:
                self.logger.info(f"   {var}: [MISSING]")
        
        # Validate REPO_NAME for pacman.conf
        repo_name = os.getenv('REPO_NAME')
        if repo_name:
            if not re.match(r'^[a-zA-Z0-9_-]+$', repo_name):
                self.logger.error(f"[ERROR] Invalid REPO_NAME '{repo_name}'. Must contain only letters, numbers, hyphens, and underscores.")
                sys.exit(1)
            if len(repo_name) > 50:
                self.logger.error(f"[ERROR] REPO_NAME '{repo_name}' is too long (max 50 characters).")
                sys.exit(1)
        
        return True
    
    def get_repo_root(self) -> Path:
        """
        Get the repository root directory reliably
        
        Returns:
            Path to repository root
        """
        # Check GITHUB_WORKSPACE first (GitHub Actions)
        github_workspace = os.getenv('GITHUB_WORKSPACE')
        if github_workspace:
            workspace_path = Path(github_workspace)
            if workspace_path.exists():
                self.logger.info(f"Using GITHUB_WORKSPACE: {workspace_path}")
                return workspace_path
        
        # Check container workspace (Docker/container specific)
        container_workspace = Path('/__w/manjaro-awesome/manjaro-awesome')
        if container_workspace.exists():
            self.logger.info(f"Using container workspace: {container_workspace}")
            return container_workspace
        
        # Get script directory and go up to repo root
        script_path = Path(__file__).resolve()
        
        # Navigate up: modules/common/environment.py -> modules/common -> modules -> scripts -> .github -> repo root
        # That's 6 levels up from this file
        potential_roots = [
            script_path.parent.parent.parent.parent.parent.parent,  # modules/common -> repo root
            script_path.parent.parent.parent.parent.parent,          # Alternative path
            Path.cwd(),                                              # Current directory
        ]
        
        for repo_root in potential_roots:
            if repo_root.exists():
                # Check for typical repository markers
                if (repo_root / '.github').exists() or (repo_root / 'README.md').exists():
                    self.logger.info(f"Using repository root: {repo_root}")
                    return repo_root
        
        # Fallback to current directory
        current_dir = Path.cwd()
        self.logger.info(f"Using current directory: {current_dir}")
        return current_dir
    
    def get_required_env(self, var_name: str, default: Optional[str] = None) -> str:
        """
        Get required environment variable
        
        Args:
            var_name: Environment variable name
            default: Default value (if not required)
        
        Returns:
            Environment variable value
        
        Raises:
            ValueError: If variable is not set and no default provided
        """
        value = os.getenv(var_name)
        if value is None or value.strip() == '':
            if default is not None:
                return default
            raise ValueError(f"Required environment variable {var_name} is not set")
        return value
    
    def get_optional_env(self, var_name: str, default: str = '') -> str:
        """
        Get optional environment variable
        
        Args:
            var_name: Environment variable name
            default: Default value if not set
        
        Returns:
            Environment variable value or default
        """
        value = os.getenv(var_name)
        if value is None or value.strip() == '':
            return default
        return value
    
    def validate_repo_name(self, repo_name: str) -> bool:
        """
        Validate repository name format
        
        Args:
            repo_name: Repository name to validate
        
        Returns:
            True if valid, False otherwise
        """
        if not repo_name:
            return False
        
        # Check format
        if not re.match(r'^[a-zA-Z0-9_-]+$', repo_name):
            self.logger.error(f"Invalid REPO_NAME '{repo_name}'. Must contain only letters, numbers, hyphens, and underscores.")
            return False
        
        # Check length
        if len(repo_name) > 50:
            self.logger.error(f"REPO_NAME '{repo_name}' is too long (max 50 characters).")
            return False
        
        return True
    
    def check_environment(self) -> Tuple[bool, List[str]]:
        """
        Check environment without exiting
        
        Returns:
            Tuple of (is_valid, missing_vars)
        """
        missing_vars = []
        
        for var in self.REQUIRED_VARS:
            value = os.getenv(var)
            if not value or value.strip() == '':
                missing_vars.append(var)
        
        return (len(missing_vars) == 0, missing_vars)
--- FILE: .github/scripts/modules/common/config_loader.py ---
"""
Configuration loading from environment, config files, and package definitions
Extracted from PackageBuilder._load_config and get_package_lists
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging


class ConfigLoader:
    """
    Loads configuration from environment variables, config files,
    and package definitions
    """
    
    def __init__(self, repo_root: Path, logger: Optional[logging.Logger] = None):
        """
        Initialize ConfigLoader
        
        Args:
            repo_root: Repository root directory
            logger: Optional logger instance
        """
        self.repo_root = repo_root
        self.logger = logger or logging.getLogger(__name__)
        
        # Initialize config state
        self._config_cache: Dict[str, Any] = {}
        self._packages_cache: Optional[Tuple[List[str], List[str]]] = None
        
        # Try to import config files
        self.has_config_files = self._try_import_config()
    
    def _try_import_config(self) -> bool:
        """Try to import config and packages modules"""
        try:
            # Add the script directory to sys.path for imports
            script_dir = self.repo_root / ".github" / "scripts"
            if script_dir.exists():
                sys.path.insert(0, str(script_dir))
            
            # Try to import config
            try:
                import config
                self._config_cache['config_module'] = config
                self.logger.debug("âœ… Config module imported")
            except ImportError:
                self.logger.warning("âš ï¸ Could not import config module")
                self._config_cache['config_module'] = None
            
            # Try to import packages
            try:
                import packages
                self._config_cache['packages_module'] = packages
                self.logger.debug("âœ… Packages module imported")
            except ImportError:
                self.logger.warning("âš ï¸ Could not import packages module")
                self._config_cache['packages_module'] = None
            
            return bool(self._config_cache.get('config_module'))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error importing config files: {e}")
            return False
    
    def load_config(self) -> Dict[str, Any]:
        """
        Load configuration from environment and config files
        
        Returns:
            Dictionary with configuration values
        """
        config = {}
        
        # Required environment variables (secrets)
        config['vps_user'] = os.getenv('VPS_USER', '')
        config['vps_host'] = os.getenv('VPS_HOST', '')
        config['ssh_key'] = os.getenv('VPS_SSH_KEY', '')
        
        # Optional environment variables (overrides)
        config['repo_server_url'] = os.getenv('REPO_SERVER_URL', '')
        config['remote_dir'] = os.getenv('REMOTE_DIR', '')
        
        # Repository name from environment
        config['repo_name'] = os.getenv('REPO_NAME', '')
        
        # Load from config.py if available
        if self.has_config_files and self._config_cache.get('config_module'):
            config_module = self._config_cache['config_module']
            
            # Directories
            config['output_dir'] = getattr(config_module, 'OUTPUT_DIR', 'built_packages')
            config['build_tracking_dir'] = getattr(config_module, 'BUILD_TRACKING_DIR', '.build_tracking')
            config['mirror_temp_dir'] = Path(getattr(config_module, 'MIRROR_TEMP_DIR', '/tmp/repo_mirror'))
            config['sync_clone_dir'] = Path(getattr(config_module, 'SYNC_CLONE_DIR', '/tmp/manjaro-awesome-gitclone'))
            config['aur_build_dir'] = self.repo_root / getattr(config_module, 'AUR_BUILD_DIR', 'build_aur')
            
            # AUR and SSH
            config['aur_urls'] = getattr(config_module, 'AUR_URLS', [
                "https://aur.archlinux.org/{pkg_name}.git",
                "git://aur.archlinux.org/{pkg_name}.git"
            ])
            config['ssh_options'] = getattr(config_module, 'SSH_OPTIONS', [
                "-o", "StrictHostKeyChecking=no",
                "-o", "ConnectTimeout=30",
                "-o", "BatchMode=yes"
            ])
            
            # GitHub and packager
            config['github_repo'] = os.getenv('GITHUB_REPO', 
                                            getattr(config_module, 'GITHUB_REPO', 'megvadulthangya/manjaro-awesome.git'))
            config['packager_id'] = getattr(config_module, 'PACKAGER_ID', 'Maintainer <no-reply@gshoots.hu>')
            
            # Debug mode
            config['debug_mode'] = getattr(config_module, 'DEBUG_MODE', False)
        else:
            # Default values if config.py not available
            config['output_dir'] = 'built_packages'
            config['build_tracking_dir'] = '.build_tracking'
            config['mirror_temp_dir'] = Path('/tmp/repo_mirror')
            config['sync_clone_dir'] = Path('/tmp/manjaro-awesome-gitclone')
            config['aur_build_dir'] = self.repo_root / 'build_aur'
            config['aur_urls'] = [
                "https://aur.archlinux.org/{pkg_name}.git",
                "git://aur.archlinux.org/{pkg_name}.git"
            ]
            config['ssh_options'] = [
                "-o", "StrictHostKeyChecking=no",
                "-o", "ConnectTimeout=30",
                "-o", "BatchMode=yes"
            ]
            config['github_repo'] = os.getenv('GITHUB_REPO', 'megvadulthangya/manjaro-awesome.git')
            config['packager_id'] = 'Maintainer <no-reply@gshoots.hu>'
            config['debug_mode'] = False
        
        # Convert string paths to Path objects
        config['output_dir'] = self.repo_root / config['output_dir']
        config['build_tracking_dir'] = self.repo_root / config['build_tracking_dir']
        
        # Log configuration (without secrets)
        self.logger.info("ðŸ”§ Configuration loaded:")
        self.logger.info(f"   SSH user: {config['vps_user']}")
        self.logger.info(f"   VPS host: {config['vps_host']}")
        self.logger.info(f"   Remote directory: {config['remote_dir']}")
        self.logger.info(f"   Repository name: {config['repo_name']}")
        if config['repo_server_url']:
            self.logger.info(f"   Repository URL: {config['repo_server_url']}")
        self.logger.info(f"   Config files loaded: {self.has_config_files}")
        self.logger.info(f"   Debug mode: {config['debug_mode']}")
        
        return config
    
    def get_package_lists(self) -> Tuple[List[str], List[str]]:
        """
        Get package lists from packages.py or exit if not available
        
        Returns:
            Tuple of (local_packages_list, aur_packages_list)
        
        Raises:
            SystemExit: If package lists cannot be loaded
        """
        if self._packages_cache is not None:
            return self._packages_cache
        
        packages_module = self._config_cache.get('packages_module')
        
        if packages_module and hasattr(packages_module, 'LOCAL_PACKAGES') and hasattr(packages_module, 'AUR_PACKAGES'):
            self.logger.info("ðŸ“¦ Using package lists from packages.py")
            local_packages_list = packages_module.LOCAL_PACKAGES
            aur_packages_list = packages_module.AUR_PACKAGES
            
            total_packages = len(local_packages_list) + len(aur_packages_list)
            self.logger.debug(f">>> DEBUG: Found {total_packages} packages to check")
            
            self._packages_cache = (local_packages_list, aur_packages_list)
            return self._packages_cache
        else:
            self.logger.error("âŒ Cannot load package lists from packages.py")
            self.logger.error("Please ensure packages.py exists and contains LOCAL_PACKAGES and AUR_PACKAGES lists")
            sys.exit(1)
    
    def get_packager_id(self) -> str:
        """
        Get PACKAGER_ID from config or environment
        
        Returns:
            PACKAGER_ID string
        """
        # Try environment variable first
        packager_env = os.getenv('PACKAGER_ENV')
        if packager_env:
            return packager_env
        
        # Fall back to config
        if self.has_config_files and self._config_cache.get('config_module'):
            config_module = self._config_cache['config_module']
            return getattr(config_module, 'PACKAGER_ID', 'Maintainer <no-reply@gshoots.hu>')
        
        return 'Maintainer <no-reply@gshoots.hu>'
    
    def get_config_value(self, key: str, default: Any = None) -> Any:
        """
        Get a specific configuration value
        
        Args:
            key: Configuration key
            default: Default value if key not found
        
        Returns:
            Configuration value or default
        """
        # Try config module first
        if self.has_config_files and self._config_cache.get('config_module'):
            config_module = self._config_cache['config_module']
            if hasattr(config_module, key):
                return getattr(config_module, key)
        
        # Fall back to environment
        env_value = os.getenv(key.upper())
        if env_value is not None:
            return env_value
        
        return default
--- FILE: .github/scripts/modules/build/__init__.py ---
"""
Package building modules
"""

# Placeholder for build modules - will be implemented in Phase 2
--- FILE: .github/scripts/modules/build/version_manager.py ---
"""
Version management for package building
Handles version extraction, comparison, and SRCINFO parsing
Extracted from build_engine.py with enhanced functionality
"""

import re
import subprocess
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any


class VersionManager:
    """Manages package version extraction, comparison, and parsing"""
    
    def __init__(self, shell_executor, logger: Optional[logging.Logger] = None):
        """
        Initialize VersionManager
        
        Args:
            shell_executor: ShellExecutor instance for command execution
            logger: Optional logger instance
        """
        self.shell_executor = shell_executor
        self.logger = logger or logging.getLogger(__name__)
    
    def extract_version_from_srcinfo(self, pkg_dir: Path) -> Tuple[str, str, Optional[str]]:
        """Extract pkgver, pkgrel, and epoch from .SRCINFO or makepkg --printsrcinfo output"""
        srcinfo_path = pkg_dir / ".SRCINFO"
        
        # First try to read existing .SRCINFO
        if srcinfo_path.exists():
            try:
                with open(srcinfo_path, 'r') as f:
                    srcinfo_content = f.read()
                return self._parse_srcinfo_content(srcinfo_content)
            except Exception as e:
                self.logger.warning(f"Failed to parse existing .SRCINFO: {e}")
        
        # Generate .SRCINFO using makepkg --printsrcinfo
        try:
            result = self.shell_executor.run(
                ['makepkg', '--printsrcinfo'],
                cwd=pkg_dir,
                capture=True,
                text=True,
                check=False,
                log_cmd=False
            )
            
            if result.returncode == 0 and result.stdout:
                # Also write to .SRCINFO for future use
                with open(srcinfo_path, 'w') as f:
                    f.write(result.stdout)
                return self._parse_srcinfo_content(result.stdout)
            else:
                self.logger.warning(f"makepkg --printsrcinfo failed: {result.stderr}")
                raise RuntimeError(f"Failed to generate .SRCINFO: {result.stderr}")
                
        except Exception as e:
            self.logger.error(f"Error running makepkg --printsrcinfo: {e}")
            raise
    
    def _parse_srcinfo_content(self, srcinfo_content: str) -> Tuple[str, str, Optional[str]]:
        """Parse SRCINFO content to extract version information"""
        pkgver = None
        pkgrel = None
        epoch = None
        
        lines = srcinfo_content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Handle key-value pairs
            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip()
                
                if key == 'pkgver':
                    pkgver = value
                elif key == 'pkgrel':
                    pkgrel = value
                elif key == 'epoch':
                    epoch = value
        
        if not pkgver or not pkgrel:
            raise ValueError("Could not extract pkgver and pkgrel from .SRCINFO")
        
        return pkgver, pkgrel, epoch
    
    def get_full_version_string(self, pkgver: str, pkgrel: str, epoch: Optional[str]) -> str:
        """Construct full version string from components"""
        if epoch and epoch != '0':
            return f"{epoch}:{pkgver}-{pkgrel}"
        return f"{pkgver}-{pkgrel}"
    
    def compare_versions(self, remote_version: Optional[str], pkgver: str, pkgrel: str, 
                        epoch: Optional[str]) -> bool:
        """
        Compare versions using vercmp-style logic
        
        Returns:
            True if AUR_VERSION > REMOTE_VERSION (should build), False otherwise
        """
        # If no remote version exists, we should build
        if not remote_version:
            self.logger.debug(f"[DEBUG] Comparing Package: Remote(NONE) vs New({pkgver}-{pkgrel}) -> BUILD TRIGGERED (no remote)")
            return True
        
        # Parse remote version
        remote_epoch = None
        remote_pkgver = None
        remote_pkgrel = None
        
        # Check if remote has epoch
        if ':' in remote_version:
            remote_epoch_str, rest = remote_version.split(':', 1)
            remote_epoch = remote_epoch_str
            if '-' in rest:
                remote_pkgver, remote_pkgrel = rest.split('-', 1)
            else:
                remote_pkgver = rest
                remote_pkgrel = "1"
        else:
            if '-' in remote_version:
                remote_pkgver, remote_pkgrel = remote_version.split('-', 1)
            else:
                remote_pkgver = remote_version
                remote_pkgrel = "1"
        
        # Build version strings for comparison
        new_version_str = f"{epoch or '0'}:{pkgver}-{pkgrel}"
        remote_version_str = f"{remote_epoch or '0'}:{remote_pkgver}-{remote_pkgrel}"
        
        # Use vercmp for proper version comparison
        try:
            result = self.shell_executor.run(
                ['vercmp', new_version_str, remote_version_str], 
                capture=True,
                text=True,
                check=False,
                log_cmd=False
            )
            
            if result.returncode == 0:
                cmp_result = int(result.stdout.strip())
                
                if cmp_result > 0:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({pkgver}-{pkgrel}) -> BUILD TRIGGERED (new version is newer)")
                    return True
                elif cmp_result == 0:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({pkgver}-{pkgrel}) -> SKIP (versions identical)")
                    return False
                else:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({pkgver}-{pkgrel}) -> SKIP (remote version is newer)")
                    return False
            else:
                # Fallback to simple comparison if vercmp fails
                self.logger.warning("vercmp failed, using fallback comparison")
                return self._fallback_version_comparison(remote_version, pkgver, pkgrel, epoch)
                
        except Exception as e:
            self.logger.warning(f"vercmp comparison failed: {e}, using fallback")
            return self._fallback_version_comparison(remote_version, pkgver, pkgrel, epoch)
    
    def _fallback_version_comparison(self, remote_version: str, pkgver: str, pkgrel: str, 
                                   epoch: Optional[str]) -> bool:
        """Fallback version comparison when vercmp is not available"""
        # Parse remote version
        remote_epoch = None
        remote_pkgver = None
        remote_pkgrel = None
        
        if ':' in remote_version:
            remote_epoch_str, rest = remote_version.split(':', 1)
            remote_epoch = remote_epoch_str
            if '-' in rest:
                remote_pkgver, remote_pkgrel = rest.split('-', 1)
            else:
                remote_pkgver = rest
                remote_pkgrel = "1"
        else:
            if '-' in remote_version:
                remote_pkgver, remote_pkgrel = remote_version.split('-', 1)
            else:
                remote_pkgver = remote_version
                remote_pkgrel = "1"
        
        # Compare epochs first
        if epoch != remote_epoch:
            try:
                epoch_int = int(epoch or 0)
                remote_epoch_int = int(remote_epoch or 0)
                if epoch_int > remote_epoch_int:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> BUILD TRIGGERED (epoch {epoch_int} > {remote_epoch_int})")
                    return True
                else:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (epoch {epoch_int} <= {remote_epoch_int})")
                    return False
            except ValueError:
                if epoch != remote_epoch:
                    self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (epoch string mismatch)")
                    return False
        
        # Compare pkgver
        if pkgver != remote_pkgver:
            self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> BUILD TRIGGERED (pkgver different)")
            return True
        
        # Compare pkgrel
        try:
            remote_pkgrel_int = int(remote_pkgrel)
            pkgrel_int = int(pkgrel)
            if pkgrel_int > remote_pkgrel_int:
                self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> BUILD TRIGGERED (pkgrel {pkgrel_int} > {remote_pkgrel_int})")
                return True
            else:
                self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (pkgrel {pkgrel_int} <= {remote_pkgrel_int})")
                return False
        except ValueError:
            if pkgrel != remote_pkgrel:
                self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (pkgrel string mismatch)")
                return False
        
        # Versions are identical
        self.logger.debug(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (versions identical)")
        return False
    
    def parse_package_filename(self, filename: str) -> Optional[Tuple[str, str]]:
        """Parse package filename to extract name and version"""
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            if len(parts) < 4:
                return None
            
            # Try to find where package name ends
            for i in range(len(parts) - 3, 0, -1):
                potential_name = '-'.join(parts[:i])
                remaining = parts[i:]
                
                if len(remaining) >= 3:
                    # Check for epoch format (e.g., "2-26.1.9-1")
                    if remaining[0].isdigit() and '-' in '-'.join(remaining[1:]):
                        epoch = remaining[0]
                        version_part = remaining[1]
                        release_part = remaining[2]
                        version_str = f"{epoch}:{version_part}-{release_part}"
                        return potential_name, version_str
                    # Standard format (e.g., "26.1.9-1")
                    elif any(c.isdigit() for c in remaining[0]) and remaining[1].isdigit():
                        version_part = remaining[0]
                        release_part = remaining[1]
                        version_str = f"{version_part}-{release_part}"
                        return potential_name, version_str
        
        except Exception as e:
            self.logger.debug(f"Could not parse filename {filename}: {e}")
        
        return None
--- FILE: .github/scripts/modules/build/aur_builder.py ---
"""
AUR package builder with SRCINFO-based version comparison and dependency fallback
Extracted from PackageBuilder._build_aur_package
"""

import os
import shutil
import time
import re
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List

from modules.common.shell_executor import ShellExecutor
from modules.build.version_manager import VersionManager


class AURBuilder:
    """Handles AUR package building with version comparison and dependency resolution"""
    
    def __init__(self, config: Dict[str, Any], shell_executor: ShellExecutor,
                 version_manager: VersionManager, version_tracker,
                 build_state, logger: Optional[logging.Logger] = None):
        """
        Initialize AURBuilder
        
        Args:
            config: Configuration dictionary
            shell_executor: ShellExecutor instance
            version_manager: VersionManager instance
            version_tracker: VersionTracker instance
            build_state: BuildState instance
            logger: Optional logger instance
        """
        self.config = config
        self.shell_executor = shell_executor
        self.version_manager = version_manager
        self.version_tracker = version_tracker
        self.build_state = build_state
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract configuration
        self.repo_root = Path(config.get('repo_root', '.'))
        self.output_dir = Path(config.get('output_dir', 'built_packages'))
        self.aur_build_dir = Path(config.get('aur_build_dir', 'build_aur'))
        self.aur_urls = config.get('aur_urls', [])
        self.packager_id = config.get('packager_id', 'Maintainer <no-reply@gshoots.hu>')
        self.repo_name = config.get('repo_name', '')
    
    def build(self, pkg_name: str, remote_version: Optional[str] = None) -> bool:
        """
        Build AUR package with SRCINFO-based version comparison and dependency fallback
        
        Args:
            pkg_name: Package name
            remote_version: Optional remote version for comparison
        
        Returns:
            True if build successful or skipped (up-to-date), False on failure
        """
        aur_dir = self.aur_build_dir
        aur_dir.mkdir(exist_ok=True)
        
        pkg_dir = aur_dir / pkg_name
        if pkg_dir.exists():
            shutil.rmtree(pkg_dir, ignore_errors=True)
        
        self.logger.info(f"Cloning {pkg_name} from AUR...")
        
        # Try different AUR URLs from config (ALWAYS FRESH CLONE)
        clone_success = False
        for aur_url_template in self.aur_urls:
            aur_url = aur_url_template.format(pkg_name=pkg_name)
            self.logger.info(f"Trying AUR URL: {aur_url}")
            result = self.shell_executor.run(
                f"git clone --depth 1 {aur_url} {pkg_dir}",
                check=False,
                log_cmd=False
            )
            if result and result.returncode == 0:
                clone_success = True
                self.logger.info(f"Successfully cloned {pkg_name} from {aur_url}")
                break
            else:
                if pkg_dir.exists():
                    shutil.rmtree(pkg_dir, ignore_errors=True)
                self.logger.warning(f"Failed to clone from {aur_url}")
        
        if not clone_success:
            self.logger.error(f"Failed to clone {pkg_name} from any AUR URL")
            return False
        
        # Set correct permissions
        self.shell_executor.run(f"chown -R builder:builder {pkg_dir}", check=False, log_cmd=False)
        
        pkgbuild = pkg_dir / "PKGBUILD"
        if not pkgbuild.exists():
            self.logger.error(f"No PKGBUILD found for {pkg_name}")
            shutil.rmtree(pkg_dir, ignore_errors=True)
            return False
        
        # Extract version info from SRCINFO (not regex)
        try:
            pkgver, pkgrel, epoch = self.version_manager.extract_version_from_srcinfo(pkg_dir)
            version = self.version_manager.get_full_version_string(pkgver, pkgrel, epoch)
            
            # DECISION LOGIC: Only build if AUR_VERSION > REMOTE_VERSION
            if remote_version and not self.version_manager.compare_versions(remote_version, pkgver, pkgrel, epoch):
                self.logger.info(f"âœ… {pkg_name} already up to date on server ({remote_version}) - skipping")
                self.build_state.add_skipped(pkg_name, version, is_aur=True, reason="up-to-date")
                
                # ZERO-RESIDUE FIX: Register the target version for this skipped package
                self.version_tracker.register_skipped_package(pkg_name, remote_version)
                
                shutil.rmtree(pkg_dir, ignore_errors=True)
                return True  # Skipped is considered successful for workflow
            
            if remote_version:
                self.logger.info(f"â„¹ï¸  {pkg_name}: remote has {remote_version}, building {version}")
                
                # PRE-BUILD PURGE: Remove old version files BEFORE building new version
                self._pre_build_purge(pkg_name, remote_version)
            else:
                self.logger.info(f"â„¹ï¸  {pkg_name}: not on server, building {version}")
                
        except Exception as e:
            self.logger.error(f"Failed to extract version for {pkg_name}: {e}")
            version = "unknown"
        
        # Attempt build
        build_success = self._execute_build(pkg_name, pkg_dir, version)
        
        if build_success:
            # ZERO-RESIDUE FIX: Register the target version for this built package
            self.version_tracker.register_built_package(pkg_name, version)
            self.build_state.add_built(pkg_name, version, is_aur=True)
        
        return build_success
    
    def _pre_build_purge(self, pkg_name: str, old_version: str):
        """Purge old version before building new one"""
        # This is a placeholder - actual cleanup is handled by CleanupManager
        self.logger.debug(f"Would purge old version {old_version} of {pkg_name}")
    
    def _execute_build(self, pkg_name: str, pkg_dir: Path, version: str) -> bool:
        """Execute the actual build process"""
        try:
            self.logger.info(f"Building {pkg_name} ({version})...")
            
            # Clean workspace before building
            self._clean_workspace(pkg_dir)
            
            self.logger.info("Downloading sources...")
            source_result = self.shell_executor.run(
                f"makepkg -od --noconfirm",
                cwd=pkg_dir,
                check=False,
                capture=True,
                timeout=600,
                extra_env={"PACKAGER": self.packager_id},
                log_cmd=False
            )
            
            if source_result.returncode != 0:
                self.logger.error(f"Failed to download sources for {pkg_name}")
                shutil.rmtree(pkg_dir, ignore_errors=True)
                return False
            
            # CRITICAL FIX: Install dependencies with AUR fallback
            self.logger.info("Installing dependencies with AUR fallback...")
            
            # First attempt: Standard makepkg -si
            self.logger.info("Building package (first attempt)...")
            build_result = self.shell_executor.run(
                f"makepkg -si --noconfirm --clean --nocheck",
                cwd=pkg_dir,
                capture=True,
                check=False,
                timeout=3600,
                extra_env={"PACKAGER": self.packager_id},
                log_cmd=True
            )
            
            # If first attempt fails, try yay fallback for missing dependencies
            if build_result.returncode != 0:
                self.logger.warning(f"First build attempt failed for {pkg_name}, trying AUR dependency fallback...")
                
                # Extract missing dependencies from error output
                error_output = build_result.stderr if build_result.stderr else build_result.stdout
                missing_deps = self._extract_missing_dependencies(error_output)
                
                if missing_deps:
                    self.logger.info(f"Found missing dependencies: {missing_deps}")
                    
                    # Try to install missing dependencies with yay
                    deps_str = ' '.join(missing_deps)
                    yay_cmd = f"LC_ALL=C yay -S --needed --noconfirm {deps_str}"
                    yay_result = self.shell_executor.run(
                        yay_cmd,
                        log_cmd=True,
                        check=False,
                        user="builder",
                        timeout=1800
                    )
                    
                    if yay_result.returncode == 0:
                        self.logger.info("âœ… Missing dependencies installed via yay, retrying build...")
                        
                        # Retry the build
                        build_result = self.shell_executor.run(
                            f"makepkg -si --noconfirm --clean --nocheck",
                            cwd=pkg_dir,
                            capture=True,
                            check=False,
                            timeout=3600,
                            extra_env={"PACKAGER": self.packager_id},
                            log_cmd=True
                        )
                    else:
                        self.logger.error(f"âŒ Failed to install missing dependencies with yay")
                        shutil.rmtree(pkg_dir, ignore_errors=True)
                        return False
            
            if build_result.returncode == 0:
                moved = False
                for pkg_file in pkg_dir.glob("*.pkg.tar.*"):
                    dest = self.output_dir / pkg_file.name
                    shutil.move(str(pkg_file), str(dest))
                    self.logger.info(f"âœ… Built: {pkg_file.name}")
                    moved = True
                
                shutil.rmtree(pkg_dir, ignore_errors=True)
                
                if moved:
                    return True
                else:
                    self.logger.error(f"No package files created for {pkg_name}")
                    return False
            else:
                self.logger.error(f"Failed to build {pkg_name}")
                shutil.rmtree(pkg_dir, ignore_errors=True)
                return False
                
        except Exception as e:
            self.logger.error(f"Error building {pkg_name}: {e}")
            if pkg_dir.exists():
                shutil.rmtree(pkg_dir, ignore_errors=True)
            return False
    
    def _clean_workspace(self, pkg_dir: Path):
        """Clean workspace before building to avoid contamination"""
        self.logger.info(f"ðŸ§¹ Cleaning workspace for {pkg_dir.name}...")
        
        # Clean src/ directory if exists
        src_dir = pkg_dir / "src"
        if src_dir.exists():
            try:
                shutil.rmtree(src_dir, ignore_errors=True)
                self.logger.info(f"  Cleaned src/ directory")
            except Exception as e:
                self.logger.warning(f"  Could not clean src/: {e}")
        
        # Clean pkg/ directory if exists
        pkg_build_dir = pkg_dir / "pkg"
        if pkg_build_dir.exists():
            try:
                shutil.rmtree(pkg_build_dir, ignore_errors=True)
                self.logger.info(f"  Cleaned pkg/ directory")
            except Exception as e:
                self.logger.warning(f"  Could not clean pkg/: {e}")
        
        # Clean any leftover .tar.* files
        for leftover in pkg_dir.glob("*.pkg.tar.*"):
            try:
                leftover.unlink()
                self.logger.info(f"  Removed leftover package: {leftover.name}")
            except Exception as e:
                self.logger.warning(f"  Could not remove {leftover}: {e}")
    
    def _extract_missing_dependencies(self, error_output: str) -> List[str]:
        """Extract missing dependencies from error output"""
        missing_deps = []
        
        # Look for patterns like "error: target not found: <package>"
        missing_patterns = [
            r"error: target not found: (\S+)",
            r"Could not find all required packages:",
            r":: Unable to find (\S+)",
        ]
        
        for pattern in missing_patterns:
            matches = re.findall(pattern, error_output)
            if matches:
                missing_deps.extend(matches)
        
        # Also look for specific makepkg dependency errors
        if "makepkg: cannot find the" in error_output:
            lines = error_output.split('\n')
            for line in lines:
                if "makepkg: cannot find the" in line:
                    # Extract package name from line like "makepkg: cannot find the 'gcc14' package"
                    dep_match = re.search(r"cannot find the '([^']+)'", line)
                    if dep_match:
                        missing_deps.append(dep_match.group(1))
        
        # Remove duplicates
        missing_deps = list(set(missing_deps))
        return missing_deps
--- FILE: .github/scripts/modules/build/local_builder.py ---
"""
Local package builder with SRCINFO-based version comparison and dependency fallback
Extracted from PackageBuilder._build_local_package
"""

import os
import shutil
import re
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List

from modules.common.shell_executor import ShellExecutor
from modules.build.version_manager import VersionManager


class LocalBuilder:
    """Handles local package building with version comparison and dependency resolution"""
    
    def __init__(self, config: Dict[str, Any], shell_executor: ShellExecutor,
                 version_manager: VersionManager, version_tracker,
                 build_state, logger: Optional[logging.Logger] = None):
        """
        Initialize LocalBuilder
        
        Args:
            config: Configuration dictionary
            shell_executor: ShellExecutor instance
            version_manager: VersionManager instance
            version_tracker: VersionTracker instance
            build_state: BuildState instance
            logger: Optional logger instance
        """
        self.config = config
        self.shell_executor = shell_executor
        self.version_manager = version_manager
        self.version_tracker = version_tracker
        self.build_state = build_state
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract configuration
        self.repo_root = Path(config.get('repo_root', '.'))
        self.output_dir = Path(config.get('output_dir', 'built_packages'))
        self.packager_id = config.get('packager_id', 'Maintainer <no-reply@gshoots.hu>')
        self.repo_name = config.get('repo_name', '')
        
        # HOKIBOT data collection
        self.hokibot_data = []
    
    def build(self, pkg_name: str, remote_version: Optional[str] = None) -> bool:
        """
        Build local package with SRCINFO-based version comparison and dependency fallback
        
        Args:
            pkg_name: Package name
            remote_version: Optional remote version for comparison
        
        Returns:
            True if build successful or skipped (up-to-date), False on failure
        """
        pkg_dir = self.repo_root / pkg_name
        if not pkg_dir.exists():
            self.logger.error(f"Package directory not found: {pkg_name}")
            return False
        
        pkgbuild = pkg_dir / "PKGBUILD"
        if not pkgbuild.exists():
            self.logger.error(f"No PKGBUILD found for {pkg_name}")
            return False
        
        # Extract version info from SRCINFO (not regex)
        try:
            pkgver, pkgrel, epoch = self.version_manager.extract_version_from_srcinfo(pkg_dir)
            version = self.version_manager.get_full_version_string(pkgver, pkgrel, epoch)
            
            # DECISION LOGIC: Only build if AUR_VERSION > REMOTE_VERSION
            if remote_version and not self.version_manager.compare_versions(remote_version, pkgver, pkgrel, epoch):
                self.logger.info(f"âœ… {pkg_name} already up to date on server ({remote_version}) - skipping")
                self.build_state.add_skipped(pkg_name, version, is_aur=False, reason="up-to-date")
                
                # ZERO-RESIDUE FIX: Register the target version for this skipped package
                self.version_tracker.register_skipped_package(pkg_name, remote_version)
                
                return True  # Skipped is considered successful for workflow
            
            if remote_version:
                self.logger.info(f"â„¹ï¸  {pkg_name}: remote has {remote_version}, building {version}")
                
                # PRE-BUILD PURGE: Remove old version files BEFORE building new version
                self._pre_build_purge(pkg_name, remote_version)
            else:
                self.logger.info(f"â„¹ï¸  {pkg_name}: not on server, building {version}")
                
        except Exception as e:
            self.logger.error(f"Failed to extract version for {pkg_name}: {e}")
            version = "unknown"
        
        # Attempt build
        build_success = self._execute_build(pkg_name, pkg_dir, version, pkgver, pkgrel, epoch)
        
        if build_success:
            # ZERO-RESIDUE FIX: Register the target version for this built package
            self.version_tracker.register_built_package(pkg_name, version)
            self.build_state.add_built(pkg_name, version, is_aur=False)
        
        return build_success
    
    def _pre_build_purge(self, pkg_name: str, old_version: str):
        """Purge old version before building new one"""
        # This is a placeholder - actual cleanup is handled by CleanupManager
        self.logger.debug(f"Would purge old version {old_version} of {pkg_name}")
    
    def _execute_build(self, pkg_name: str, pkg_dir: Path, version: str,
                      pkgver: str, pkgrel: str, epoch: Optional[str]) -> bool:
        """Execute the actual build process"""
        try:
            self.logger.info(f"Building {pkg_name} ({version})...")
            
            # Clean workspace before building
            self._clean_workspace(pkg_dir)
            
            self.logger.info("Downloading sources...")
            source_result = self.shell_executor.run(
                f"makepkg -od --noconfirm",
                cwd=pkg_dir,
                check=False,
                capture=True,
                timeout=600,
                extra_env={"PACKAGER": self.packager_id},
                log_cmd=False
            )
            
            if source_result.returncode != 0:
                self.logger.error(f"Failed to download sources for {pkg_name}")
                return False
            
            # CRITICAL FIX: Install dependencies with AUR fallback
            self.logger.info("Installing dependencies with AUR fallback...")
            
            # First attempt: Standard makepkg with appropriate flags
            makepkg_flags = "-si --noconfirm --clean"
            if pkg_name == "gtk2":
                makepkg_flags += " --nocheck"
                self.logger.info("GTK2: Skipping check step (long)")
            
            self.logger.info("Building package (first attempt)...")
            build_result = self.shell_executor.run(
                f"makepkg {makepkg_flags}",
                cwd=pkg_dir,
                capture=True,
                check=False,
                timeout=3600,
                extra_env={"PACKAGER": self.packager_id},
                log_cmd=True
            )
            
            # If first attempt fails, try yay fallback for missing dependencies
            if build_result.returncode != 0:
                self.logger.warning(f"First build attempt failed for {pkg_name}, trying AUR dependency fallback...")
                
                # Extract missing dependencies from error output
                error_output = build_result.stderr if build_result.stderr else build_result.stdout
                missing_deps = self._extract_missing_dependencies(error_output)
                
                if missing_deps:
                    self.logger.info(f"Found missing dependencies: {missing_deps}")
                    
                    # Try to install missing dependencies with yay
                    deps_str = ' '.join(missing_deps)
                    yay_cmd = f"LC_ALL=C yay -S --needed --noconfirm {deps_str}"
                    yay_result = self.shell_executor.run(
                        yay_cmd,
                        log_cmd=True,
                        check=False,
                        user="builder",
                        timeout=1800
                    )
                    
                    if yay_result.returncode == 0:
                        self.logger.info("âœ… Missing dependencies installed via yay, retrying build...")
                        
                        # Retry the build
                        build_result = self.shell_executor.run(
                            f"makepkg {makepkg_flags}",
                            cwd=pkg_dir,
                            capture=True,
                            check=False,
                            timeout=3600,
                            extra_env={"PACKAGER": self.packager_id},
                            log_cmd=True
                        )
                    else:
                        self.logger.error(f"âŒ Failed to install missing dependencies with yay")
                        return False
            
            if build_result.returncode == 0:
                moved = False
                built_files = []
                for pkg_file in pkg_dir.glob("*.pkg.tar.*"):
                    dest = self.output_dir / pkg_file.name
                    shutil.move(str(pkg_file), str(dest))
                    self.logger.info(f"âœ… Built: {pkg_file.name}")
                    moved = True
                    built_files.append(str(dest))
                
                if moved:
                    # Collect metadata for hokibot
                    if built_files:
                        # Simplified metadata extraction
                        self.hokibot_data.append({
                            'name': pkg_name,
                            'built_version': version,
                            'pkgver': pkgver,
                            'pkgrel': pkgrel,
                            'epoch': epoch
                        })
                        self.logger.info(f"ðŸ“ HOKIBOT observed: {pkg_name} -> {version}")
                    
                    return True
                else:
                    self.logger.error(f"No package files created for {pkg_name}")
                    return False
            else:
                self.logger.error(f"Failed to build {pkg_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"Error building {pkg_name}: {e}")
            return False
    
    def _clean_workspace(self, pkg_dir: Path):
        """Clean workspace before building to avoid contamination"""
        self.logger.info(f"ðŸ§¹ Cleaning workspace for {pkg_dir.name}...")
        
        # Clean src/ directory if exists
        src_dir = pkg_dir / "src"
        if src_dir.exists():
            try:
                shutil.rmtree(src_dir, ignore_errors=True)
                self.logger.info(f"  Cleaned src/ directory")
            except Exception as e:
                self.logger.warning(f"  Could not clean src/: {e}")
        
        # Clean pkg/ directory if exists
        pkg_build_dir = pkg_dir / "pkg"
        if pkg_build_dir.exists():
            try:
                shutil.rmtree(pkg_build_dir, ignore_errors=True)
                self.logger.info(f"  Cleaned pkg/ directory")
            except Exception as e:
                self.logger.warning(f"  Could not clean pkg/: {e}")
        
        # Clean any leftover .tar.* files
        for leftover in pkg_dir.glob("*.pkg.tar.*"):
            try:
                leftover.unlink()
                self.logger.info(f"  Removed leftover package: {leftover.name}")
            except Exception as e:
                self.logger.warning(f"  Could not remove {leftover}: {e}")
    
    def _extract_missing_dependencies(self, error_output: str) -> List[str]:
        """Extract missing dependencies from error output"""
        missing_deps = []
        
        # Look for patterns like "error: target not found: <package>"
        missing_patterns = [
            r"error: target not found: (\S+)",
            r"Could not find all required packages:",
            r":: Unable to find (\S+)",
        ]
        
        for pattern in missing_patterns:
            matches = re.findall(pattern, error_output)
            if matches:
                missing_deps.extend(matches)
        
        # Also look for specific makepkg dependency errors
        if "makepkg: cannot find the" in error_output:
            lines = error_output.split('\n')
            for line in lines:
                if "makepkg: cannot find the" in line:
                    # Extract package name from line like "makepkg: cannot find the 'gcc14' package"
                    dep_match = re.search(r"cannot find the '([^']+)'", line)
                    if dep_match:
                        missing_deps.append(dep_match.group(1))
        
        # Remove duplicates
        missing_deps = list(set(missing_deps))
        return missing_deps
    
    def get_hokibot_data(self):
        """Get collected hokibot data"""
        return self.hokibot_data
--- FILE: .github/scripts/modules/repo/__init__.py ---
"""
Repository management modules
"""

# Placeholder for repository modules - will be implemented in Phase 3
--- FILE: .github/scripts/modules/repo/version_tracker.py ---
"""
Version tracking for Zero-Residue cleanup policy - SERVER-FIRST ARCHITECTURE
Enhanced with VCS version resolution and protected artifacts
"""

import os
import json
import re
import subprocess
import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set, Any
from datetime import datetime

from modules.vps.ssh_client import SSHClient


class VersionTracker:
    """
    Tracks package versions with VCS resolution and protected artifacts
    """
    
    def __init__(self, repo_root: Path, ssh_client: SSHClient, logger: Optional[logging.Logger] = None):
        """
        Initialize VersionTracker
        
        Args:
            repo_root: Repository root directory
            ssh_client: SSHClient instance for remote operations
            logger: Optional logger instance
        """
        self.repo_root = repo_root
        self.ssh_client = ssh_client
        self.logger = logger or logging.getLogger(__name__)
        
        # VCS cache for git package versions
        self._vcs_version_cache: Dict[str, str] = {}
        
        # Protected artifacts registry (multi-package PKGBUILD outputs)
        self._protected_files: Set[str] = set()
        
        # Pending deletions queue
        self._pending_deletions: List[str] = []
        
        # Target versions: {pkg_name: target_version} - versions we want to keep
        self._target_versions: Dict[str, str] = {}
        
        # Skipped packages: {pkg_name: remote_version} - packages skipped as up-to-date
        self._skipped_packages: Dict[str, str] = {}
        
        # Built packages: {pkg_name: built_version} - packages we just built
        self._built_packages: Dict[str, str] = {}
        
        # JSON state file
        self.state_file = self._get_state_path()
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        self.state: Dict[str, Any] = self._load_state()
        
        # Load remote inventory at initialization
        self._load_remote_inventory()
    
    def _get_state_path(self) -> Path:
        """
        Get platform-appropriate state file path
        
        Returns:
            Path to state file
        """
        # Priority 1: GitHub Actions workspace
        github_workspace = os.getenv('GITHUB_WORKSPACE')
        if github_workspace:
            workspace_path = Path(github_workspace)
            if workspace_path.exists():
                state_path = workspace_path / ".build_tracking" / "vps_state.json"
                self.logger.info(f"Using GitHub workspace state path: {state_path}")
                return state_path
        
        # Priority 2: Repository root
        repo_state_path = self.repo_root / ".build_tracking" / "vps_state.json"
        if self.repo_root.exists():
            self.logger.info(f"Using repository state path: {repo_state_path}")
            return repo_state_path
        
        # Priority 3: User home directory
        home_dir = Path.home()
        home_state_path = home_dir / ".build_tracking" / "vps_state.json"
        home_state_path.parent.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"Using home directory state path: {home_state_path}")
        return home_state_path
    
    def _load_state(self) -> Dict[str, Any]:
        """Load state from JSON file"""
        try:
            if self.state_file.exists():
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                self.logger.info(f"Loaded state from {self.state_file}")
                return state
            else:
                self.logger.info(f"State file {self.state_file} does not exist, creating new")
                return {"packages": {}, "metadata": {"created": datetime.now().isoformat()}}
        except Exception as e:
            self.logger.error(f"Failed to load state file: {e}")
            return {"packages": {}, "metadata": {"created": datetime.now().isoformat()}}
    
    def _load_remote_inventory(self):
        """Load remote inventory at initialization"""
        self.logger.info("ðŸ” Loading remote inventory...")
        remote_files = self.ssh_client.get_cached_inventory(force_refresh=True)
        
        self.logger.info(f"ðŸ“‹ Remote inventory loaded: {len(remote_files)} files")
    
    def resolve_vcs_version(self, pkg_name: str, pkg_dir: Path, force_refresh: bool = False) -> Optional[Tuple[str, str, str]]:
        """
        Resolve VCS package version by running makepkg --printsrcinfo
        
        Args:
            pkg_name: Package name
            pkg_dir: Directory containing PKGBUILD
            force_refresh: If True, ignore cache and refresh
        
        Returns:
            Tuple of (pkgver, pkgrel, epoch) or None if failed
        """
        # Check cache first
        cache_key = f"{pkg_name}_{pkg_dir}"
        if not force_refresh and cache_key in self._vcs_version_cache:
            version_str = self._vcs_version_cache[cache_key]
            return self._parse_version_string(version_str)
        
        # Determine if this is a VCS package
        is_vcs_package = self._is_vcs_package(pkg_name, pkg_dir)
        
        try:
            if is_vcs_package:
                self.logger.info(f"ðŸ” Resolving VCS version for {pkg_name}...")
                
                # For VCS packages, we need to run makepkg --printsrcinfo
                # This will resolve git commit hashes to actual versions
                result = subprocess.run(
                    ['makepkg', '--printsrcinfo'],
                    cwd=pkg_dir,
                    capture_output=True,
                    text=True,
                    check=False,
                    timeout=300
                )
                
                if result.returncode == 0 and result.stdout:
                    version_info = self._extract_version_from_srcinfo(result.stdout)
                    if version_info:
                        pkgver, pkgrel, epoch = version_info
                        version_str = self._format_version_string(pkgver, pkgrel, epoch)
                        self._vcs_version_cache[cache_key] = version_str
                        self.logger.info(f"âœ… VCS version resolved: {version_str}")
                        return version_info
                    else:
                        self.logger.warning(f"Could not parse version from SRCINFO for {pkg_name}")
                else:
                    self.logger.warning(f"makepkg --printsrcinfo failed for {pkg_name}: {result.stderr}")
            
            # Fall back to standard .SRCINFO parsing
            return self._extract_version_from_srcinfo_file(pkg_dir)
            
        except subprocess.TimeoutExpired:
            self.logger.error(f"Timeout resolving VCS version for {pkg_name}")
            return None
        except Exception as e:
            self.logger.error(f"Error resolving VCS version for {pkg_name}: {e}")
            return None
    
    def _is_vcs_package(self, pkg_name: str, pkg_dir: Path) -> bool:
        """
        Check if package is a VCS package
        
        Args:
            pkg_name: Package name
            pkg_dir: Package directory
        
        Returns:
            True if VCS package
        """
        # Check package name patterns
        vcs_patterns = ['-git', '_git', '-svn', '_svn', '-hg', '_hg', '-bzr', '_bzr']
        if any(pattern in pkg_name.lower() for pattern in vcs_patterns):
            return True
        
        # Check PKGBUILD content
        pkgbuild_path = pkg_dir / "PKGBUILD"
        if pkgbuild_path.exists():
            try:
                with open(pkgbuild_path, 'r') as f:
                    content = f.read()
                
                # Look for VCS URLs
                vcs_url_patterns = [
                    r'git\+https?://',
                    r'https?://.*\.git',
                    r'svn\+https?://',
                    r'hg\+https?://',
                    r'bzr\+https?://'
                ]
                
                for pattern in vcs_url_patterns:
                    if re.search(pattern, content):
                        return True
            except Exception:
                pass
        
        return False
    
    def _extract_version_from_srcinfo(self, srcinfo_content: str) -> Optional[Tuple[str, str, Optional[str]]]:
        """Parse SRCINFO content to extract version information"""
        pkgver = None
        pkgrel = None
        epoch = None
        
        lines = srcinfo_content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Handle key-value pairs
            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip()
                
                if key == 'pkgver':
                    pkgver = value
                elif key == 'pkgrel':
                    pkgrel = value
                elif key == 'epoch':
                    epoch = value
        
        if not pkgver or not pkgrel:
            return None
        
        return pkgver, pkgrel, epoch
    
    def _extract_version_from_srcinfo_file(self, pkg_dir: Path) -> Optional[Tuple[str, str, Optional[str]]]:
        """Extract version from .SRCINFO file"""
        srcinfo_path = pkg_dir / ".SRCINFO"
        
        if srcinfo_path.exists():
            try:
                with open(srcinfo_path, 'r') as f:
                    content = f.read()
                return self._extract_version_from_srcinfo(content)
            except Exception as e:
                self.logger.warning(f"Failed to read .SRCINFO: {e}")
        
        return None
    
    def _parse_version_string(self, version_str: str) -> Optional[Tuple[str, str, Optional[str]]]:
        """Parse version string into components"""
        if ':' in version_str:
            # Has epoch
            epoch_part, rest = version_str.split(':', 1)
            epoch = epoch_part
            if '-' in rest:
                pkgver, pkgrel = rest.split('-', 1)
            else:
                pkgver = rest
                pkgrel = "1"
        else:
            # No epoch
            epoch = None
            if '-' in version_str:
                pkgver, pkgrel = version_str.split('-', 1)
            else:
                pkgver = version_str
                pkgrel = "1"
        
        return pkgver, pkgrel, epoch
    
    def _format_version_string(self, pkgver: str, pkgrel: str, epoch: Optional[str]) -> str:
        """Format version components into string"""
        if epoch and epoch != '0':
            return f"{epoch}:{pkgver}-{pkgrel}"
        return f"{pkgver}-{pkgrel}"
    
    def register_protected_files(self, pkg_name: str, files: List[str]):
        """
        Register protected files that should not be deleted
        
        Args:
            pkg_name: Package name
            files: List of filenames to protect
        """
        protected_count = 0
        for filename in files:
            if filename not in self._protected_files:
                self._protected_files.add(filename)
                protected_count += 1
                self.logger.debug(f"ðŸ”’ Protected file: {filename}")
        
        if protected_count > 0:
            self.logger.info(f"ðŸ”’ Registered {protected_count} protected files for {pkg_name}")
    
    def is_protected(self, filename: str) -> bool:
        """
        Check if a file is protected from deletion
        
        Args:
            filename: Filename to check
        
        Returns:
            True if file is protected
        """
        return filename in self._protected_files
    
    def get_protected_files(self) -> Set[str]:
        """Get all protected files"""
        return self._protected_files.copy()
    
    def clear_protected_files(self):
        """Clear protected files registry"""
        self._protected_files.clear()
        self.logger.debug("Cleared protected files registry")
    
    def queue_deletion(self, remote_path: str):
        """
        Queue a file for batch deletion
        
        Args:
            remote_path: Full remote path to delete
        """
        filename = Path(remote_path).name
        
        # Don't queue protected files
        if self.is_protected(filename):
            self.logger.debug(f"Skipping protected file: {filename}")
            return
        
        self._pending_deletions.append(remote_path)
        self.logger.debug(f"Queued for deletion: {filename}")
    
    def commit_queued_deletions(self) -> bool:
        """
        Execute all queued deletions via SSH client
        
        Returns:
            True if successful
        """
        if not self._pending_deletions:
            return True
        
        self.logger.info(f"ðŸ”§ Committing {len(self._pending_deletions)} queued deletions...")
        success = self.ssh_client.commit_queued_deletions()
        
        if success:
            self._pending_deletions.clear()
        
        return success
    
    def get_pending_deletions(self) -> List[str]:
        """Get list of pending deletions"""
        return self._pending_deletions.copy()
    
    def clear_pending_deletions(self):
        """Clear pending deletions queue"""
        self._pending_deletions.clear()
        self.logger.debug("Cleared pending deletions queue")
    
    def save_state(self) -> bool:
        """Save state to JSON file"""
        try:
            self.state["metadata"]["last_updated"] = datetime.now().isoformat()
            self.state["metadata"]["protected_files"] = list(self._protected_files)
            
            with open(self.state_file, 'w') as f:
                json.dump(self.state, f, indent=2)
            
            self.logger.debug(f"Saved state to {self.state_file}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to save state: {e}")
            return False
    
    def is_package_on_remote(self, pkg_name: str, version: str) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        SERVER-FIRST: Check if package with specific version exists on remote server
        
        Args:
            pkg_name: Package name (e.g., 'libinput-gestures')
            version: Package version (e.g., '2.81-1')
        
        Returns:
            Tuple of (found, remote_version, remote_hash) or (False, None, None)
        """
        self.logger.debug(f"Checking if {pkg_name} version {version} exists on remote...")
        
        # Get cached inventory
        remote_inventory = self.ssh_client.get_cached_inventory()
        
        # Normalize package name for matching (case-insensitive)
        pkg_name_lower = pkg_name.lower()
        
        for filename, file_path in remote_inventory.items():
            # Parse filename to extract name, version, and architecture
            parsed = self._parse_package_filename_with_arch(filename)
            if not parsed:
                continue
            
            remote_pkg_name, remote_version, architecture = parsed
            remote_pkg_name_lower = remote_pkg_name.lower()
            
            # Check if package name matches (case-insensitive)
            if remote_pkg_name_lower == pkg_name_lower:
                # Check if version matches (including epoch handling)
                if self._versions_match(remote_version, version):
                    self.logger.info(f"âœ… Found matching package on remote: {pkg_name} {version}")
                    
                    # Get hash from remote file
                    remote_hash = self.ssh_client.get_remote_hash(file_path)
                    
                    return True, remote_version, remote_hash
        
        self.logger.debug(f"Package {pkg_name} version {version} not found in remote files")
        return False, None, None
    
    def _versions_match(self, version1: str, version2: str) -> bool:
        """Check if two version strings match (handles epoch and architecture)"""
        # Normalize versions by removing epoch if it's 0
        def normalize_version(v: str) -> str:
            if ':' in v:
                epoch, rest = v.split(':', 1)
                if epoch == '0':
                    return rest
            return v
        
        v1_norm = normalize_version(version1)
        v2_norm = normalize_version(version2)
        
        return v1_norm == v2_norm
    
    def discover_and_adopt_remote_packages(self, pkg_name: str) -> Optional[Tuple[str, Optional[str]]]:
        """
        Enhanced adoption logic: Check remote server for package and adopt if found
        
        Args:
            pkg_name: Package name to search for
        
        Returns:
            Tuple of (version, hash) or None if not found
        """
        self.logger.info(f"ðŸ” Searching for {pkg_name} on remote server...")
        
        # Get cached inventory
        remote_inventory = self.ssh_client.get_cached_inventory()
        
        # Case-insensitive matching with architecture suffix handling
        pkg_name_lower = pkg_name.lower()
        
        for filename, file_path in remote_inventory.items():
            self.logger.debug(f"Checking file: {filename}")
            
            # Parse package name and version from filename
            parsed = self._parse_package_filename_with_arch(filename)
            if not parsed:
                continue
            
            remote_pkg_name, version, architecture = parsed
            remote_pkg_name_lower = remote_pkg_name.lower()
            
            # Case-insensitive comparison with architecture suffix handling
            if remote_pkg_name_lower == pkg_name_lower:
                self.logger.info(f"âœ… Found {pkg_name} on remote server: {filename}")
                
                # Get hash from remote file
                remote_hash = self.ssh_client.get_remote_hash(file_path)
                
                # Update state
                self.state["packages"][pkg_name] = {
                    "version": version,
                    "hash": remote_hash,
                    "last_updated": datetime.now().isoformat(),
                    "source": "adopted",
                    "filename": filename,
                    "architecture": architecture
                }
                
                # Save state immediately
                self.save_state()
                
                # Update target versions
                self._target_versions[pkg_name] = version
                self._skipped_packages[pkg_name] = version
                
                self.logger.info(f"ðŸ“¥ Adopted {pkg_name} version {version} from remote server")
                return version, remote_hash
        
        self.logger.debug(f"Package {pkg_name} not found in remote files")
        return None
    
    def _parse_package_filename_with_arch(self, filename: str) -> Optional[Tuple[str, str, str]]:
        """
        Parse package filename to extract name, version, and architecture
        
        Args:
            filename: Package filename (e.g., 'qownnotes-26.1.9-1-x86_64.pkg.tar.zst')
        
        Returns:
            Tuple of (package_name, version_string, architecture) or None
        """
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            if len(parts) < 4:
                return None
            
            # Try to find where package name ends and architecture begins
            # Architecture is usually the last part (x86_64, any, etc.)
            # Version is usually the 2 or 3 parts before architecture
            
            # Start from the end and work backwards
            for i in range(len(parts) - 2, 0, -1):
                # Check if the remaining parts look like version-release-architecture
                remaining = parts[i:]
                
                if len(remaining) >= 3:
                    # Check for architecture suffix (x86_64, any, etc.)
                    arch = remaining[-1]
                    
                    # Check for epoch format (e.g., "2-26.1.9-1-x86_64")
                    if remaining[0].isdigit() and len(remaining) >= 4:
                        epoch = remaining[0]
                        version_part = remaining[1]
                        release_part = remaining[2]
                        version_str = f"{epoch}:{version_part}-{release_part}"
                        package_name = '-'.join(parts[:i])
                        return package_name, version_str, arch
                    # Standard format (e.g., "26.1.9-1-x86_64")
                    elif any(c.isdigit() for c in remaining[0]) and remaining[1].isdigit():
                        version_part = remaining[0]
                        release_part = remaining[1]
                        version_str = f"{version_part}-{release_part}"
                        package_name = '-'.join(parts[:i])
                        return package_name, version_str, arch
        
        except Exception as e:
            self.logger.debug(f"Could not parse filename {filename}: {e}")
        
        return None
    
    def register_built_package(self, pkg_name: str, version: str, hash_value: Optional[str] = None) -> None:
        """
        Register a package that was just built or adopted from VPS
        
        Args:
            pkg_name: Package name
            version: Package version
            hash_value: Optional hash value for verification
        """
        # Update built packages registry
        self._built_packages[pkg_name] = version
        self._target_versions[pkg_name] = version
        
        # Update JSON state
        if "packages" not in self.state:
            self.state["packages"] = {}
        
        source = "adopted" if hash_value is not None else "built"
        
        self.state["packages"][pkg_name] = {
            "version": version,
            "hash": hash_value,
            "last_updated": datetime.now().isoformat(),
            "source": source
        }
        
        # Save state
        self.save_state()
        
        self.logger.info(f"ðŸ“ Registered {source} package: {pkg_name} ({version})")
    
    def register_target_version(self, pkg_name: str, target_version: str) -> None:
        """
        Register the target version for a package
        
        Args:
            pkg_name: Package name
            target_version: The version we want to keep (either built or latest from server)
        """
        self._target_versions[pkg_name] = target_version
        self.logger.info(f"ðŸ“ Registered target version for {pkg_name}: {target_version}")
    
    def register_skipped_package(self, pkg_name: str, remote_version: str) -> None:
        """
        Register a package that was skipped because it's up-to-date
        
        Args:
            pkg_name: Package name
            remote_version: The remote version that should be kept (not deleted)
        """
        # Store in skipped registry
        self._skipped_packages[pkg_name] = remote_version
        
        # ðŸš¨ CRITICAL: Explicitly set target version to remote version
        self._target_versions[pkg_name] = remote_version
        
        # Update JSON state
        if "packages" not in self.state:
            self.state["packages"] = {}
        
        self.state["packages"][pkg_name] = {
            "version": remote_version,
            "hash": None,
            "last_updated": datetime.now().isoformat(),
            "source": "skipped"
        }
        
        # Save state
        self.save_state()
        
        self.logger.info(f"ðŸ“ Registered SKIPPED package: {pkg_name} (remote: {remote_version}, target: {remote_version})")
    
    def get_target_version(self, pkg_name: str) -> Optional[str]:
        """
        Get target version for a package
        
        Args:
            pkg_name: Package name
        
        Returns:
            Target version or None if not registered
        """
        return self._target_versions.get(pkg_name)
    
    def has_target_version(self, pkg_name: str) -> bool:
        """
        Check if a package has a registered target version
        
        Args:
            pkg_name: Package name
        
        Returns:
            True if target version exists
        """
        return pkg_name in self._target_versions
    
    def is_skipped(self, pkg_name: str) -> bool:
        """
        Check if a package was skipped
        
        Args:
            pkg_name: Package name
        
        Returns:
            True if package was skipped
        """
        return pkg_name in self._skipped_packages
    
    def is_built(self, pkg_name: str) -> bool:
        """
        Check if a package was built in this run
        
        Args:
            pkg_name: Package name
        
        Returns:
            True if package was built
        """
        return pkg_name in self._built_packages
    
    def set_remote_inventory(self, remote_files: Dict[str, str]) -> None:
        """
        Set remote inventory from VPS
        
        Args:
            remote_files: Dictionary of {filename: full_path} from VPS
        """
        self.logger.info(f"ðŸ“‹ Remote inventory updated: {len(remote_files)} files")
    
    def get_remote_inventory(self) -> Dict[str, str]:
        """
        Get current remote inventory from cache
        
        Returns:
            Dictionary of {filename: full_path}
        """
        return self.ssh_client.get_cached_inventory()
    
    def get_files_to_keep(self) -> Set[str]:
        """
        Determine which files should be kept based on target versions
        
        Returns:
            Set of filenames that match target versions
        """
        files_to_keep = set()
        remote_inventory = self.get_remote_inventory()
        
        for filename in remote_inventory:
            # Parse filename to extract package name and version
            parsed = self._parse_package_filename(filename)
            if not parsed:
                # Can't parse, keep it to be safe
                files_to_keep.add(filename)
                continue
            
            pkg_name, version_str = parsed
            
            # Check if this package has a target version
            if pkg_name in self._target_versions:
                target_version = self._target_versions[pkg_name]
                if version_str == target_version:
                    # This is the version we want to keep
                    files_to_keep.add(filename)
                    self.logger.debug(f"âœ… Keeping {filename} (matches target version {target_version})")
                else:
                    self.logger.debug(f"ðŸ—‘ï¸ Marking for deletion: {filename} (target is {target_version})")
            else:
                # No target version registered - keep to be safe
                files_to_keep.add(filename)
                self.logger.debug(f"âš ï¸ Keeping unknown package: {filename} (not in target versions)")
        
        return files_to_keep
    
    def get_files_to_delete(self) -> List[str]:
        """
        Determine which files should be deleted based on target versions
        
        Returns:
            List of full paths to delete
        """
        files_to_delete = []
        remote_inventory = self.get_remote_inventory()
        files_to_keep = self.get_files_to_keep()
        
        for filename, full_path in remote_inventory.items():
            if filename not in files_to_keep and not self.is_protected(filename):
                files_to_delete.append(full_path)
        
        return files_to_delete
    
    def _parse_package_filename(self, filename: str) -> Optional[Tuple[str, str]]:
        """
        Parse package filename to extract name and version (without architecture)
        
        Args:
            filename: Package filename (e.g., 'qownnotes-26.1.9-1-x86_64.pkg.tar.zst')
        
        Returns:
            Tuple of (package_name, version_string) or None
        """
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            if len(parts) < 4:
                return None
            
            # Try to find where package name ends
            for i in range(len(parts) - 3, 0, -1):
                potential_name = '-'.join(parts[:i])
                remaining = parts[i:]
                
                if len(remaining) >= 3:
                    # Check for epoch format (e.g., "2-26.1.9-1")
                    if remaining[0].isdigit() and '-' in '-'.join(remaining[1:]):
                        epoch = remaining[0]
                        version_part = remaining[1]
                        release_part = remaining[2]
                        version_str = f"{epoch}:{version_part}-{release_part}"
                        return potential_name, version_str
                    # Standard format (e.g., "26.1.9-1")
                    elif any(c.isdigit() for c in remaining[0]) and remaining[1].isdigit():
                        version_part = remaining[0]
                        release_part = remaining[1]
                        version_str = f"{version_part}-{release_part}"
                        return potential_name, version_str
        
        except Exception as e:
            self.logger.debug(f"Could not parse filename {filename}: {e}")
        
        return None
    
    def clear_remote_inventory(self) -> None:
        """Clear remote inventory cache"""
        self.ssh_client.clear_cache()
        self.logger.debug("Cleared remote inventory cache")
    
    def get_target_packages(self) -> Dict[str, str]:
        """Get all target packages"""
        return self._target_versions.copy()
    
    def get_skipped_packages_dict(self) -> Dict[str, str]:
        """Get all skipped packages"""
        return self._skipped_packages.copy()
    
    def get_built_packages_dict(self) -> Dict[str, str]:
        """Get all built packages"""
        return self._built_packages.copy()
    
    def has_packages(self) -> bool:
        """Check if any packages are registered"""
        return bool(self._target_versions)
    
    def get_state_summary(self) -> Dict[str, Any]:
        """Get state summary for logging"""
        return {
            "total_packages": len(self.state.get("packages", {})),
            "last_updated": self.state.get("metadata", {}).get("last_updated"),
            "protected_files": len(self._protected_files),
            "pending_deletions": len(self._pending_deletions),
        }
--- FILE: .github/scripts/modules/repo/database_manager.py ---
"""
Repository database management
Handles database generation, verification, and VPS operations
Extracted from RepoManager with enhanced functionality
"""

import os
import subprocess
import shutil
import logging
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

from modules.vps.ssh_client import SSHClient
from modules.vps.rsync_client import RsyncClient


class DatabaseManager:
    """Manages repository database operations and VPS synchronization"""
    
    def __init__(self, config: Dict[str, Any], ssh_client: SSHClient,
                 rsync_client: RsyncClient, logger: Optional[logging.Logger] = None):
        """
        Initialize DatabaseManager
        
        Args:
            config: Configuration dictionary
            ssh_client: SSHClient instance
            rsync_client: RsyncClient instance
            logger: Optional logger instance
        """
        self.config = config
        self.ssh_client = ssh_client
        self.rsync_client = rsync_client
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract configuration
        self.repo_name = config.get('repo_name', '')
        self.output_dir = Path(config.get('output_dir', 'built_packages'))
        self.remote_dir = config.get('remote_dir', '')
    
    def generate_database(self) -> bool:
        """
        Generate repository database from ALL locally available packages
        
        Returns:
            True if database generation successful
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("PHASE: Repository Database Generation")
        self.logger.info("=" * 60)
        
        # Get all package files from local output directory
        all_packages = self._get_all_local_packages()
        
        if not all_packages:
            self.logger.info("No packages available for database generation")
            return False
        
        self.logger.info(f"Generating database with {len(all_packages)} packages...")
        self.logger.info(f"Packages: {', '.join(all_packages[:10])}{'...' if len(all_packages) > 10 else ''}")
        
        old_cwd = os.getcwd()
        os.chdir(self.output_dir)
        
        try:
            db_file = f"{self.repo_name}.db.tar.gz"
            
            # Clean old database files
            for f in [f"{self.repo_name}.db", f"{self.repo_name}.db.tar.gz", 
                      f"{self.repo_name}.files", f"{self.repo_name}.files.tar.gz"]:
                if os.path.exists(f):
                    os.remove(f)
            
            # Verify each package file exists locally before database generation
            missing_packages = []
            valid_packages = []
            
            for pkg_filename in all_packages:
                if Path(pkg_filename).exists():
                    valid_packages.append(pkg_filename)
                else:
                    missing_packages.append(pkg_filename)
            
            if missing_packages:
                self.logger.error(f"âŒ CRITICAL: {len(missing_packages)} packages missing locally:")
                for pkg in missing_packages[:5]:
                    self.logger.error(f"   - {pkg}")
                if len(missing_packages) > 5:
                    self.logger.error(f"   ... and {len(missing_packages) - 5} more")
                return False
            
            if not valid_packages:
                self.logger.error("No valid package files found for database generation")
                return False
            
            self.logger.info(f"âœ… All {len(valid_packages)} package files verified locally")
            
            # Generate database with repo-add using shell=True for wildcard expansion
            cmd = f"repo-add {db_file} *.pkg.tar.zst"
            
            self.logger.info(f"Running repo-add with shell=True to include ALL packages...")
            self.logger.info(f"Command: {cmd}")
            self.logger.info(f"Current directory: {os.getcwd()}")
            
            result = subprocess.run(
                cmd,
                shell=True,  # CRITICAL: Use shell=True for wildcard expansion
                capture_output=True,
                text=True,
                check=False
            )
            
            if result.returncode == 0:
                self.logger.info("âœ… Database created successfully")
                
                # Verify the database was created
                db_path = Path(db_file)
                if db_path.exists():
                    size_mb = db_path.stat().st_size / (1024 * 1024)
                    self.logger.info(f"Database size: {size_mb:.2f} MB")
                    
                    # CRITICAL: Verify database entries BEFORE upload
                    self.logger.info("ðŸ” Verifying database entries before upload...")
                    list_cmd = ["tar", "-tzf", db_file]
                    list_result = subprocess.run(list_cmd, capture_output=True, text=True, check=False)
                    if list_result.returncode == 0:
                        db_entries = [line for line in list_result.stdout.split('\n') if line.endswith('/desc')]
                        self.logger.info(f"âœ… Database contains {len(db_entries)} package entries")
                        if len(db_entries) == 0:
                            self.logger.error("âŒâŒâŒ DATABASE IS EMPTY! This is the root cause of the issue.")
                            return False
                        else:
                            self.logger.info(f"Sample database entries: {db_entries[:5]}")
                    else:
                        self.logger.warning(f"Could not list database contents: {list_result.stderr}")
                
                return True
            else:
                self.logger.error(f"repo-add failed with exit code {result.returncode}:")
                if result.stdout:
                    self.logger.error(f"STDOUT: {result.stdout[:500]}")
                if result.stderr:
                    self.logger.error(f"STDERR: {result.stderr[:500]}")
                return False
                
        finally:
            os.chdir(old_cwd)
    
    def check_database_files(self) -> Tuple[List[str], List[str]]:
        """
        Check if repository database files exist on server
        
        Returns:
            Tuple of (existing_files, missing_files)
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("STEP 2: Checking existing database files on server")
        self.logger.info("=" * 60)
        
        db_files = [
            f"{self.repo_name}.db",
            f"{self.repo_name}.db.tar.gz",
            f"{self.repo_name}.files",
            f"{self.repo_name}.files.tar.gz"
        ]
        
        existing_files = []
        missing_files = []
        
        for db_file in db_files:
            remote_cmd = f"test -f {self.remote_dir}/{db_file} && echo 'EXISTS' || echo 'MISSING'"
            
            ssh_cmd = [
                "ssh",
                f"{self.config.get('vps_user')}@{self.config.get('vps_host')}",
                remote_cmd
            ]
            
            try:
                result = subprocess.run(
                    ssh_cmd,
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                if result.returncode == 0 and "EXISTS" in result.stdout:
                    existing_files.append(db_file)
                    self.logger.info(f"âœ… Database file exists: {db_file}")
                else:
                    missing_files.append(db_file)
                    self.logger.info(f"â„¹ï¸ Database file missing: {db_file}")
                    
            except Exception as e:
                self.logger.warning(f"Could not check {db_file}: {e}")
                missing_files.append(db_file)
        
        if existing_files:
            self.logger.info(f"Found {len(existing_files)} database files on server")
        else:
            self.logger.info("No database files found on server")
        
        return existing_files, missing_files
    
    def fetch_existing_database(self, existing_files: List[str]) -> bool:
        """
        Fetch existing database files from server
        
        Args:
            existing_files: List of database files to fetch
        
        Returns:
            True if all files fetched successfully
        """
        if not existing_files:
            return True
        
        self.logger.info("\nðŸ“¥ Fetching existing database files from server...")
        
        success = True
        for db_file in existing_files:
            remote_path = f"{self.remote_dir}/{db_file}"
            local_path = self.output_dir / db_file
            
            # Remove local copy if exists
            if local_path.exists():
                local_path.unlink()
            
            ssh_cmd = [
                "scp",
                "-o", "StrictHostKeyChecking=no",
                "-o", "ConnectTimeout=30",
                f"{self.config.get('vps_user')}@{self.config.get('vps_host')}:{remote_path}",
                str(local_path)
            ]
            
            try:
                result = subprocess.run(
                    ssh_cmd,
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                if result.returncode == 0 and local_path.exists():
                    size_mb = local_path.stat().st_size / (1024 * 1024)
                    self.logger.info(f"âœ… Fetched: {db_file} ({size_mb:.2f} MB)")
                else:
                    self.logger.warning(f"âš ï¸ Could not fetch {db_file}")
                    success = False
            except Exception as e:
                self.logger.warning(f"Could not fetch {db_file}: {e}")
                success = False
        
        return success
    
    def upload_database(self) -> bool:
        """
        Upload database files to server
        
        Returns:
            True if upload successful
        """
        # Get all database files and signatures
        db_files = list(self.output_dir.glob(f"{self.repo_name}.*"))
        
        if not db_files:
            self.logger.warning("No database files to upload")
            return False
        
        files_to_upload = [str(f) for f in db_files]
        return self.rsync_client.upload(files_to_upload, self.output_dir)
    
    def _get_all_local_packages(self) -> List[str]:
        """Get ALL package files from local output directory (mirrored + newly built)"""
        self.logger.info("\nðŸ” Getting complete package list from local directory...")
        
        local_files = list(self.output_dir.glob("*.pkg.tar.*"))
        
        if not local_files:
            self.logger.info("â„¹ï¸ No package files found locally")
            return []
        
        local_filenames = [f.name for f in local_files]
        
        self.logger.info(f"ðŸ“Š Local package count: {len(local_filenames)}")
        self.logger.info(f"Sample packages: {local_filenames[:10]}")
        
        return local_filenames
    
    def get_database_files(self) -> List[Path]:
        """Get list of generated database files"""
        db_files = []
        patterns = [
            f"{self.repo_name}.db",
            f"{self.repo_name}.db.tar.gz",
            f"{self.repo_name}.files",
            f"{self.repo_name}.files.tar.gz",
            f"{self.repo_name}.db.sig",
            f"{self.repo_name}.db.tar.gz.sig",
            f"{self.repo_name}.files.sig",
            f"{self.repo_name}.files.tar.gz.sig",
        ]
        
        for pattern in patterns:
            for file_path in self.output_dir.glob(pattern):
                if file_path.exists():
                    db_files.append(file_path)
        
        return db_files
    
    def cleanup_old_databases(self) -> None:
        """Clean up old database files from output directory"""
        patterns = [
            f"{self.repo_name}.db",
            f"{self.repo_name}.db.tar.gz",
            f"{self.repo_name}.files",
            f"{self.repo_name}.files.tar.gz",
        ]
        
        for pattern in patterns:
            for file_path in self.output_dir.glob(pattern):
                try:
                    if file_path.exists():
                        file_path.unlink()
                        self.logger.debug(f"Removed old database file: {file_path.name}")
                except Exception as e:
                    self.logger.warning(f"Could not remove {file_path}: {e}")
--- FILE: .github/scripts/modules/repo/cleanup_manager.py ---
"""
Cleanup manager for Zero-Residue policy
Handles surgical removal of old package versions from local and remote systems
Extracted from RepoManager with enhanced precision
"""

import os
import shutil
import subprocess
import re
import logging
from pathlib import Path
from typing import Dict, Any, List, Set, Optional, Tuple

from modules.repo.version_tracker import VersionTracker
from modules.vps.ssh_client import SSHClient
from modules.vps.rsync_client import RsyncClient


class CleanupManager:
    """Manages Zero-Residue cleanup operations for local and remote systems"""
    
    def __init__(self, config: Dict[str, Any], version_tracker: VersionTracker,
                 ssh_client: SSHClient, rsync_client: RsyncClient,
                 logger: Optional[logging.Logger] = None):
        """
        Initialize CleanupManager
        
        Args:
            config: Configuration dictionary
            version_tracker: VersionTracker instance
            ssh_client: SSHClient instance
            rsync_client: RsyncClient instance
            logger: Optional logger instance
        """
        self.config = config
        self.version_tracker = version_tracker
        self.ssh_client = ssh_client
        self.rsync_client = rsync_client
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract configuration
        self.repo_name = config.get('repo_name', '')
        self.output_dir = Path(config.get('output_dir', 'built_packages'))
        self.remote_dir = config.get('remote_dir', '')
        
        # Upload success flag for safety valve
        self._upload_successful = False
    
    def set_upload_successful(self, successful: bool):
        """Set the upload success flag for safety valve"""
        self._upload_successful = successful
    
    def purge_old_local(self, pkg_name: str, old_version: str, target_version: Optional[str] = None):
        """
        ðŸš¨ ZERO-RESIDUE POLICY: Surgical old version removal BEFORE building
        
        Removes old versions from local output directory before new build.
        
        Args:
            pkg_name: Package name
            old_version: Version to potentially delete
            target_version: Version we want to keep (None if building new)
        """
        # If we have a registered target version, use it
        if target_version is None:
            target_version = self.version_tracker.get_target_version(pkg_name)
        
        if target_version and old_version == target_version:
            # This is the version we want to keep
            self.logger.info(f"âœ… No pre-build purge needed: {pkg_name} version {old_version} is target version")
            return
        
        # Delete old version from output directory
        self._delete_specific_version_local(pkg_name, old_version)
    
    def _delete_specific_version_local(self, pkg_name: str, version_to_delete: str):
        """Delete a specific version of a package from local output_dir"""
        patterns = self._version_to_patterns(pkg_name, version_to_delete)
        deleted_count = 0
        
        for pattern in patterns:
            for old_file in self.output_dir.glob(pattern):
                try:
                    # Verify this is actually the version we want to delete
                    extracted_version = self._extract_version_from_filename(old_file.name, pkg_name)
                    if extracted_version == version_to_delete:
                        old_file.unlink()
                        self.logger.info(f"ðŸ—‘ï¸ Surgically removed local {old_file.name}")
                        deleted_count += 1
                        
                        # Also remove signature
                        sig_file = old_file.with_suffix(old_file.suffix + '.sig')
                        if sig_file.exists():
                            sig_file.unlink()
                            self.logger.info(f"ðŸ—‘ï¸ Removed local signature {sig_file.name}")
                except Exception as e:
                    self.logger.warning(f"Could not delete local {old_file}: {e}")
        
        if deleted_count > 0:
            self.logger.info(f"âœ… Removed {deleted_count} local files for {pkg_name} version {version_to_delete}")
    
    def validate_output_dir(self):
        """
        ðŸ”¥ ZOMBIE PROTECTION: Final validation before database generation
        
        Enhanced to recognize skipped packages as legitimate (not zombies)
        
        Scans output_dir and ensures:
        1. Only one version per package exists
        2. If multiple versions exist, keep only the target version
        3. Delete any "zombie" files (old versions that shouldn't be there)
        
        This is the LAST CHANCE to clean up before repo-add runs.
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ðŸš¨ FINAL VALIDATION: Removing zombie packages from output_dir")
        self.logger.info("=" * 60)
        
        # Get all package files in output_dir
        package_files = list(self.output_dir.glob("*.pkg.tar.*"))
        
        if not package_files:
            self.logger.info("â„¹ï¸ No package files in output_dir to validate")
            return
        
        self.logger.info(f"ðŸ” Validating {len(package_files)} package files in output_dir...")
        
        # Group files by package name
        packages_dict: Dict[str, List[Tuple[str, Path]]] = {}
        
        for pkg_file in package_files:
            # Extract package name and version from filename
            extracted = self._parse_package_filename(pkg_file.name)
            if extracted:
                pkg_name, version_str = extracted
                if pkg_name not in packages_dict:
                    packages_dict[pkg_name] = []
                packages_dict[pkg_name].append((version_str, pkg_file))
        
        # Process each package
        total_deleted = 0
        
        for pkg_name, files in packages_dict.items():
            if len(files) > 1:
                self.logger.warning(f"âš ï¸ Multiple versions found for {pkg_name}: {[v[0] for v in files]}")
                
                # Check if we have a registered target version
                target_version = self.version_tracker.get_target_version(pkg_name)
                
                if target_version:
                    # Keep only the target version
                    kept = False
                    for version_str, file_path in files:
                        if version_str == target_version:
                            self.logger.info(f"âœ… Keeping target version: {file_path.name} ({version_str})")
                            kept = True
                        else:
                            try:
                                file_path.unlink()
                                self.logger.info(f"ðŸ—‘ï¸ Removing non-target version: {file_path.name}")
                                total_deleted += 1
                            except Exception as e:
                                self.logger.warning(f"Could not delete {file_path}: {e}")
                    
                    if not kept:
                        self.logger.error(f"âŒ Target version {target_version} for {pkg_name} not found in output_dir!")
                else:
                    # No target version registered, keep the latest
                    self.logger.warning(f"âš ï¸ No target version registered for {pkg_name}, using version comparison")
                    latest_version = self._find_latest_version([v[0] for v in files])
                    for version_str, file_path in files:
                        if version_str == latest_version:
                            self.logger.info(f"âœ… Keeping latest version: {file_path.name} ({version_str})")
                        else:
                            try:
                                file_path.unlink()
                                self.logger.info(f"ðŸ—‘ï¸ Removing older version: {file_path.name}")
                                total_deleted += 1
                            except Exception as e:
                                self.logger.warning(f"Could not delete {file_path}: {e}")
        
        if total_deleted > 0:
            self.logger.info(f"ðŸŽ¯ Final validation: Removed {total_deleted} zombie package files")
        else:
            self.logger.info("âœ… Output_dir validation passed - no zombie packages found")
    
    def cleanup_server(self):
        """
        ðŸš¨ ZERO-RESIDUE SERVER CLEANUP: Remove zombie packages from VPS 
        using TARGET VERSIONS as SOURCE OF TRUTH.
        
        Only keeps packages that match registered target versions.
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ðŸ”’ ZERO-RESIDUE SERVER CLEANUP: Target Versions are Source of Truth")
        self.logger.info("=" * 60)
        
        # VALVE: Check if we have any target versions registered
        if not self.version_tracker.has_packages():
            self.logger.warning("âš ï¸ No target versions registered - skipping server cleanup")
            return
        
        self.logger.info(f"ðŸ”„ Zero-Residue cleanup initiated with {len(self.version_tracker.get_target_packages())} target versions")
        
        # STEP 1: Get ALL files from VPS
        vps_files = self.ssh_client.get_file_inventory()
        if not vps_files:
            self.logger.info("â„¹ï¸ No files found on VPS - nothing to clean up")
            return
        
        # Update version tracker with remote inventory
        self.version_tracker.set_remote_inventory(vps_files)
        
        # STEP 2: Determine files to delete based on target versions
        files_to_delete = self.version_tracker.get_files_to_delete()
        
        if not files_to_delete:
            self.logger.info("âœ… No zombie packages found on VPS")
            return
        
        self.logger.warning(f"ðŸš¨ Identified {len(files_to_delete)} zombie packages for deletion")
        
        # STEP 3: Execute deletion
        deleted_count = 0
        batch_size = 50
        
        for i in range(0, len(files_to_delete), batch_size):
            batch = files_to_delete[i:i + batch_size]
            if self.ssh_client.delete_remote_files(batch):
                deleted_count += len(batch)
        
        self.logger.info(f"ðŸ“Š Server cleanup complete: Deleted {deleted_count} zombie packages")
    
    def _parse_package_filename(self, filename: str) -> Optional[Tuple[str, str]]:
        """Parse package filename to extract name and version"""
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            # The package name is everything before the last 3 parts (version-release-arch)
            # or last 4 parts (epoch-version-release-arch)
            if len(parts) >= 4:
                # Try to find where package name ends
                for i in range(len(parts) - 3, 0, -1):
                    potential_name = '-'.join(parts[:i])
                    
                    # Check if remaining parts look like version-release-arch
                    remaining = parts[i:]
                    if len(remaining) >= 3:
                        # Check for epoch format (e.g., "2-26.1.9-1-x86_64")
                        if remaining[0].isdigit() and '-' in '-'.join(remaining[1:]):
                            epoch = remaining[0]
                            version_part = remaining[1]
                            release_part = remaining[2]
                            version_str = f"{epoch}:{version_part}-{release_part}"
                            return potential_name, version_str
                        # Standard format (e.g., "26.1.9-1-x86_64")
                        elif any(c.isdigit() for c in remaining[0]) and remaining[1].isdigit():
                            version_part = remaining[0]
                            release_part = remaining[1]
                            version_str = f"{version_part}-{release_part}"
                            return potential_name, version_str
        except Exception as e:
            self.logger.debug(f"Could not parse filename {filename}: {e}")
        
        return None
    
    def _version_to_patterns(self, pkg_name: str, version: str) -> List[str]:
        """Convert version string to filename patterns"""
        patterns = []
        
        if ':' in version:
            # Version with epoch: "2:26.1.9-1" -> "2-26.1.9-1-*.pkg.tar.*"
            epoch, rest = version.split(':', 1)
            patterns.append(f"{pkg_name}-{epoch}-{rest}-*.pkg.tar.*")
        else:
            # Standard version: "26.1.9-1" -> "*26.1.9-1-*.pkg.tar.*"
            patterns.append(f"{pkg_name}-{version}-*.pkg.tar.*")
        
        return patterns
    
    def _extract_version_from_filename(self, filename: str, pkg_name: str) -> Optional[str]:
        """
        Extract version from package filename
        
        Args:
            filename: Package filename (e.g., 'qownnotes-26.1.9-1-x86_64.pkg.tar.zst')
            pkg_name: Package name (e.g., 'qownnotes')
        
        Returns:
            Version string (e.g., '26.1.9-1') or None if cannot parse
        """
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            # Find where package name ends
            for i in range(len(parts) - 2, 0, -1):
                possible_name = '-'.join(parts[:i])
                if possible_name == pkg_name or possible_name.startswith(pkg_name + '-'):
                    # Remaining parts: version-release-architecture
                    if len(parts) >= i + 3:
                        version_part = parts[i]
                        release_part = parts[i+1]
                        
                        # Check for epoch (e.g., "2-26.1.9-1" -> "2:26.1.9-1")
                        if i + 2 < len(parts) and parts[i].isdigit():
                            epoch_part = parts[i]
                            version_part = parts[i+1]
                            release_part = parts[i+2]
                            return f"{epoch_part}:{version_part}-{release_part}"
                        else:
                            return f"{version_part}-{release_part}"
        except Exception as e:
            self.logger.debug(f"Could not extract version from {filename}: {e}")
        
        return None
    
    def _find_latest_version(self, versions: List[str]) -> str:
        """
        Find the latest version from a list using vercmp
        
        Args:
            versions: List of version strings
        
        Returns:
            The latest version string
        """
        if not versions:
            return ""
        
        if len(versions) == 1:
            return versions[0]
        
        # Try to use vercmp for accurate comparison
        try:
            latest = versions[0]
            for i in range(1, len(versions)):
                result = subprocess.run(
                    ['vercmp', versions[i], latest],
                    capture_output=True,
                    text=True,
                    check=False
                )
                if result.returncode == 0:
                    cmp_result = int(result.stdout.strip())
                    if cmp_result > 0:
                        latest = versions[i]
            
            return latest
        except Exception as e:
            # Fallback: use string comparison (less accurate but works for simple cases)
            self.logger.warning(f"vercmp failed, using fallback version comparison: {e}")
            return max(versions)
    
    def cleanup_temp_directories(self):
        """Clean up temporary directories"""
        temp_dirs = [
            Path(self.config.get('mirror_temp_dir', '/tmp/repo_mirror')),
            Path(self.config.get('sync_clone_dir', '/tmp/manjaro-awesome-gitclone')),
        ]
        
        for temp_dir in temp_dirs:
            if temp_dir.exists():
                try:
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    self.logger.debug(f"Cleaned up temporary directory: {temp_dir}")
                except Exception as e:
                    self.logger.warning(f"Could not clean up {temp_dir}: {e}")
--- FILE: .github/scripts/modules/vps/__init__.py ---
"""
VPS and remote operations modules
"""

# Placeholder for VPS modules - will be implemented in Phase 4
--- FILE: .github/scripts/modules/vps/ssh_client.py ---
"""
SSH client for remote VPS operations
Handles SSH connections, file operations, and remote command execution
"""

import os
import shutil
import time
import logging
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any

from modules.common.shell_executor import ShellExecutor


class SSHClient:
    """Handles SSH connections and remote operations on VPS with caching"""
    
    def __init__(self, config: Dict[str, Any], shell_executor: ShellExecutor,
                 logger: Optional[logging.Logger] = None):
        """
        Initialize SSHClient
        
        Args:
            config: Configuration dictionary with VPS settings
            shell_executor: ShellExecutor instance for command execution
            logger: Optional logger instance
        """
        self.config = config
        self.shell_executor = shell_executor
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract VPS configuration
        self.vps_user = config.get('vps_user', '')
        self.vps_host = config.get('vps_host', '')
        self.remote_dir = config.get('remote_dir', '')
        self.ssh_options = config.get('ssh_options', [])
        self.repo_name = config.get('repo_name', '')
        
        # SSH key path
        self.ssh_key_path = Path("/home/builder/.ssh/id_ed25519")
        
        # Add quiet flag to SSH options
        self.ssh_options_with_quiet = self.ssh_options + ["-q"]
        
        # Cache for remote operations
        self._remote_inventory_cache: Optional[Dict[str, str]] = None
        self._cache_timestamp: float = 0
        self._cache_ttl = 300  # 5 minutes
        
        # Pending operations
        self._pending_deletions: List[str] = []
    
    def get_cached_inventory(self, force_refresh: bool = False) -> Dict[str, str]:
        """
        Get cached remote inventory with TTL
        
        Args:
            force_refresh: If True, ignore cache and refresh
            
        Returns:
            Dictionary of {filename: full_path}
        """
        current_time = time.time()
        
        if (force_refresh or 
            not self._remote_inventory_cache or 
            current_time - self._cache_timestamp > self._cache_ttl):
            
            self.logger.info("ðŸ” Refreshing remote inventory cache...")
            self._remote_inventory_cache = self._get_remote_file_list_optimized()
            self._cache_timestamp = current_time
            self.logger.info(f"ðŸ“‹ Cache updated: {len(self._remote_inventory_cache)} files")
        else:
            cache_age = int(current_time - self._cache_timestamp)
            self.logger.debug(f"ðŸ“‹ Using cached inventory ({cache_age}s old)")
        
        return self._remote_inventory_cache.copy()
    
    def _get_remote_file_list_optimized(self) -> Dict[str, str]:
        """
        Get optimized list of package files from remote server
        
        Returns:
            Dictionary of {filename: full_path}
        """
        remote_cmd = f"cd {self.remote_dir} && ls -1 *.pkg.tar.* 2>/dev/null || echo ''"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                files = {}
                for line in result.stdout.strip().splitlines():
                    line = line.strip()
                    if line and not line.startswith("Welcome") and not line.startswith("Last login"):
                        full_path = f"{self.remote_dir}/{line}"
                        files[line] = full_path
                
                return files
            else:
                self.logger.warning(f"âš ï¸ Failed to list remote files: {result.stderr[:200]}")
                return {}
                
        except Exception as e:
            self.logger.error(f"âŒ Error listing remote files: {e}")
            return {}
    
    def batch_delete(self, file_paths: List[str], batch_size: int = 100) -> bool:
        """
        Delete multiple files in batches via single SSH session
        
        Args:
            file_paths: List of full remote paths to delete
            batch_size: Maximum files per batch to avoid command line limits
            
        Returns:
            True if all deletions successful, False otherwise
        """
        if not file_paths:
            self.logger.debug("No files to delete")
            return True
        
        self.logger.info(f"ðŸ—‘ï¸ Batch deleting {len(file_paths)} remote file(s)...")
        
        # Extract just filenames from full paths
        all_filenames = []
        for file_path in file_paths:
            filename = Path(file_path).name
            all_filenames.append(filename)
        
        # Process in batches
        success = True
        total_batches = (len(all_filenames) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            batch_start = batch_num * batch_size
            batch_end = batch_start + batch_size
            batch_filenames = all_filenames[batch_start:batch_end]
            
            batch_num_display = batch_num + 1
            
            # Build delete command with proper escaping
            filenames_str = ' '.join([f"'{f}'" for f in batch_filenames])
            remote_cmd = f"cd {self.remote_dir} && rm -f {filenames_str} && echo 'BATCH_DELETE_SUCCESS_{batch_num_display}'"
            
            # Use string command with shell=True
            ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
            
            try:
                result = self.shell_executor.run(
                    ssh_cmd,
                    capture=True,
                    check=False,
                    shell=True,
                    log_cmd=False
                )
                
                if result.returncode == 0 and f"BATCH_DELETE_SUCCESS_{batch_num_display}" in result.stdout:
                    self.logger.info(f"âœ… Batch {batch_num_display}/{total_batches}: Deleted {len(batch_filenames)} files")
                    
                    # Update cache by removing deleted files
                    if self._remote_inventory_cache:
                        for filename in batch_filenames:
                            self._remote_inventory_cache.pop(filename, None)
                else:
                    self.logger.warning(f"âš ï¸ Batch {batch_num_display} failed: {result.stderr[:200]}")
                    success = False
                    
            except Exception as e:
                self.logger.error(f"âŒ Error in batch delete {batch_num_display}: {e}")
                success = False
        
        if success:
            self.logger.info(f"âœ… All {len(file_paths)} files deleted successfully")
        else:
            self.logger.warning(f"âš ï¸ Some batch deletions failed")
        
        return success
    
    def atomic_command_sequence(self, commands: List[str], timeout: int = 60) -> Tuple[bool, str]:
        """
        Execute multiple remote commands in a single SSH session using heredoc
        
        Args:
            commands: List of shell commands to execute
            timeout: Total timeout in seconds
            
        Returns:
            Tuple of (success, combined_output)
        """
        if not commands:
            return True, "No commands to execute"
        
        self.logger.info(f"ðŸ”§ Executing {len(commands)} commands atomically...")
        
        # Build heredoc script
        heredoc_script = "set -e\n"  # Exit on error
        heredoc_script += f"cd {self.remote_dir}\n"
        
        for i, cmd in enumerate(commands):
            heredoc_script += f"echo '>>> COMMAND {i+1}: {cmd}'\n"
            heredoc_script += f"{cmd}\n"
            heredoc_script += "echo '>>> SUCCESS'\n"
        
        heredoc_script += "echo 'ATOMIC_SEQUENCE_COMPLETE'\n"
        
        # Escape for SSH
        escaped_script = heredoc_script.replace('"', '\\"').replace('$', '\\$')
        
        # Use heredoc via bash -s
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"bash -s\" << 'EOF'\n{heredoc_script}\nEOF"
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                timeout=timeout,
                shell=True,
                log_cmd=False
            )
            
            combined_output = ""
            if result.stdout:
                combined_output += result.stdout
            
            if result.returncode == 0 and "ATOMIC_SEQUENCE_COMPLETE" in result.stdout:
                self.logger.info("âœ… Atomic command sequence completed successfully")
                return True, combined_output
            else:
                error_msg = result.stderr[:500] if result.stderr else "No error output"
                self.logger.error(f"âŒ Atomic command sequence failed: {error_msg}")
                return False, combined_output + f"\nERROR: {error_msg}"
                
        except Exception as e:
            self.logger.error(f"âŒ Atomic command sequence exception: {e}")
            return False, str(e)
    
    def queue_deletion(self, remote_path: str):
        """
        Queue a file for batch deletion
        
        Args:
            remote_path: Full remote path to delete
        """
        self._pending_deletions.append(remote_path)
    
    def commit_queued_deletions(self) -> bool:
        """
        Execute all queued deletions
        
        Returns:
            True if successful
        """
        if not self._pending_deletions:
            return True
        
        self.logger.info(f"ðŸ”§ Committing {len(self._pending_deletions)} queued deletions...")
        success = self.batch_delete(self._pending_deletions)
        
        if success:
            self._pending_deletions.clear()
        
        return success
    
    def clear_cache(self):
        """Clear the remote inventory cache"""
        self._remote_inventory_cache = None
        self._cache_timestamp = 0
        self.logger.debug("Cleared remote inventory cache")
    
    def invalidate_cache_for_file(self, filename: str):
        """Remove a specific file from cache"""
        if self._remote_inventory_cache and filename in self._remote_inventory_cache:
            del self._remote_inventory_cache[filename]
            self.logger.debug(f"Invalidated cache entry for: {filename}")
    
    # Existing methods remain unchanged but use cache where appropriate
    
    def delete_remote_files(self, file_list: List[str]) -> bool:
        """Legacy method - delegates to batch_delete"""
        return self.batch_delete(file_list)
    
    def setup_ssh_config(self, ssh_key: Optional[str] = None) -> bool:
        """
        Setup SSH config file for builder user
        
        Args:
            ssh_key: Optional SSH private key content
        
        Returns:
            True if setup successful
        """
        try:
            ssh_dir = Path("/home/builder/.ssh")
            ssh_dir.mkdir(exist_ok=True, mode=0o700)
            
            # Write SSH config file
            config_content = f"""Host {self.vps_host}
  HostName {self.vps_host}
  User {self.vps_user}
  IdentityFile ~/.ssh/id_ed25519
  StrictHostKeyChecking no
  ConnectTimeout 30
  ServerAliveInterval 15
  ServerAliveCountMax 3
"""
            
            config_file = ssh_dir / "config"
            with open(config_file, "w") as f:
                f.write(config_content)
            
            config_file.chmod(0o600)
            
            # Ensure SSH key exists and has correct permissions
            if not self.ssh_key_path.exists() and ssh_key:
                with open(self.ssh_key_path, "w") as f:
                    f.write(ssh_key)
                self.ssh_key_path.chmod(0o600)
            
            # Set ownership to builder
            try:
                shutil.chown(ssh_dir, "builder", "builder")
                for item in ssh_dir.iterdir():
                    shutil.chown(item, "builder", "builder")
            except Exception as e:
                self.logger.warning(f"Could not change SSH dir ownership: {e}")
            
            self.logger.info("âœ… SSH configuration setup complete")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ SSH configuration failed: {e}")
            return False
    
    def test_connection(self) -> bool:
        """
        Test SSH connection to VPS
        
        Returns:
            True if connection successful
        """
        self.logger.info("ðŸ” Testing SSH connection to VPS...")
        
        ssh_test_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"cd {self.remote_dir} && echo SSH_TEST_SUCCESS\""
        
        try:
            result = self.shell_executor.run(
                ssh_test_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result and result.returncode == 0 and "SSH_TEST_SUCCESS" in result.stdout:
                self.logger.info("âœ… SSH connection successful")
                return True
            else:
                error_msg = result.stderr[:100] if result and result.stderr else 'No output'
                self.logger.warning(f"âš ï¸ SSH connection failed: {error_msg}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ SSH test exception: {e}")
            return False
    
    def get_remote_file_list(self) -> List[str]:
        """
        Get explicit list of package files from remote server (legacy)
        
        Returns:
            List of package filenames
        """
        inventory = self.get_cached_inventory()
        return list(inventory.values())
    
    def file_exists(self, remote_path: str) -> bool:
        """
        Check if a file exists on remote server
        
        Args:
            remote_path: Full remote path to check
        
        Returns:
            True if file exists
        """
        # Extract filename from path
        filename = Path(remote_path).name
        
        # Use cache first
        inventory = self.get_cached_inventory()
        if filename in inventory:
            return True
        
        # Fallback to direct check
        remote_cmd = f"cd {self.remote_dir} && test -f \"{filename}\" && echo EXISTS || echo NOT_EXISTS"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0 and "EXISTS" in result.stdout:
                # Update cache
                if self._remote_inventory_cache is not None:
                    self._remote_inventory_cache[filename] = remote_path
                return True
            return False
        except Exception as e:
            self.logger.warning(f"Could not check file existence {remote_path}: {e}")
            return False
    
    def get_remote_hash(self, remote_path: str) -> Optional[str]:
        """
        Get SHA256 hash of remote file
        
        Args:
            remote_path: Full remote path to file
        
        Returns:
            SHA256 hash string or None if failed
        """
        # Extract filename from path
        filename = Path(remote_path).name
        
        remote_cmd = f"cd {self.remote_dir} && sha256sum \"{filename}\" 2>/dev/null | cut -d' ' -f1"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0 and result.stdout.strip():
                hash_value = result.stdout.strip()
                if len(hash_value) == 64:  # SHA256 hash length
                    return hash_value
                else:
                    self.logger.warning(f"Invalid hash format for {remote_path}")
            return None
        except Exception as e:
            self.logger.warning(f"Could not get hash for {remote_path}: {e}")
            return None
    
    def ensure_directory(self) -> bool:
        """
        Ensure remote directory exists and has correct permissions
        
        Returns:
            True if directory exists or was created successfully
        """
        self.logger.info("ðŸ”§ Ensuring remote directory exists...")
        
        remote_cmd = f"""
        # Check if directory exists
        if [ ! -d "{self.remote_dir}" ]; then
            echo "Creating directory {self.remote_dir}"
            sudo mkdir -p "{self.remote_dir}"
            sudo chown -R {self.vps_user}:www-data "{self.remote_dir}"
            sudo chmod -R 755 "{self.remote_dir}"
            echo "âœ… Directory created and permissions set"
        else
            echo "âœ… Directory exists"
            # Ensure correct permissions
            sudo chown -R {self.vps_user}:www-data "{self.remote_dir}"
            sudo chmod -R 755 "{self.remote_dir}"
            echo "âœ… Permissions verified"
        fi
        """
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                self.logger.info("âœ… Remote directory verified")
                for line in result.stdout.splitlines():
                    if line.strip():
                        self.logger.debug(f"REMOTE DIR: {line}")
                return True
            else:
                self.logger.warning(f"âš ï¸ Could not ensure remote directory: {result.stderr[:200]}")
                return False
                
        except Exception as e:
            self.logger.warning(f"Could not ensure remote directory: {e}")
            return False
    
    def check_repository_exists(self) -> Tuple[bool, bool]:
        """
        Check if repository exists on VPS via SSH
        
        Returns:
            Tuple of (exists, has_packages)
        """
        self.logger.info("ðŸ” Checking if repository exists on VPS...")
        
        remote_cmd = f"""
        # Check for package files
        if cd "{self.remote_dir}" && find . -maxdepth 1 -name "*.pkg.tar.*" -type f 2>/dev/null | head -1 >/dev/null; then
            echo "REPO_EXISTS_WITH_PACKAGES"
        # Check for database files
        elif cd "{self.remote_dir}" && ([ -f "{self.repo_name}.db.tar.gz" ] || [ -f "{self.repo_name}.db" ]); then
            echo "REPO_EXISTS_WITH_DB"
        else
            echo "REPO_NOT_FOUND"
        fi
        """
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                timeout=30,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                output = result.stdout.strip()
                if "REPO_EXISTS_WITH_PACKAGES" in output:
                    self.logger.info("âœ… Repository exists on VPS (has package files)")
                    return True, True
                elif "REPO_EXISTS_WITH_DB" in output:
                    self.logger.info("âœ… Repository exists on VPS (has database)")
                    return True, False
                else:
                    self.logger.info("â„¹ï¸ Repository does not exist on VPS (first run)")
                    return False, False
            else:
                self.logger.warning(f"âš ï¸ Could not check repository existence: {result.stderr[:200]}")
                return False, False
                
        except Exception as e:
            self.logger.error(f"âŒ Error checking repository: {e}")
            return False, False
    
    def list_remote_files(self, pattern: str = "*.pkg.tar.*") -> List[str]:
        """
        List files on remote server matching pattern
        
        Args:
            pattern: File pattern to match
        
        Returns:
            List of remote file paths
        """
        if pattern == "*.pkg.tar.*":
            # Use cached inventory for common pattern
            inventory = self.get_cached_inventory()
            return list(inventory.values())
        
        # For other patterns, fall back to direct query
        remote_cmd = f"cd {self.remote_dir} && find . -maxdepth 1 -type f -name '{pattern}' 2>/dev/null | sed 's|^\./||'"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        self.logger.info(f"Listing remote files with pattern: {pattern}")
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                files = []
                for line in result.stdout.strip().splitlines():
                    line = line.strip()
                    if line and not line.startswith("Welcome") and not line.startswith("Last login"):
                        full_path = f"{self.remote_dir}/{line}"
                        files.append(full_path)
                
                self.logger.info(f"âœ… Found {len(files)} remote files")
                return files
            else:
                self.logger.warning(f"âš ï¸ Failed to list remote files: {result.stderr[:200]}")
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ Error listing remote files: {e}")
            return []
    
    def execute_remote_command(self, command: str, timeout: int = 30) -> Tuple[bool, str]:
        """
        Execute a command on remote server
        
        Args:
            command: Command to execute
            timeout: Command timeout in seconds
        
        Returns:
            Tuple of (success, output)
        """
        # Prepend cd to remote_dir
        remote_cmd = f"cd {self.remote_dir} && {command}"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                timeout=timeout,
                shell=True,
                log_cmd=False
            )
            
            if result.returncode == 0:
                return True, result.stdout.strip()
            else:
                return False, result.stderr.strip()
                
        except Exception as e:
            self.logger.error(f"âŒ Remote command execution failed: {e}")
            return False, str(e)
    
    def debug_remote_directory(self) -> bool:
        """
        Debug: List remote directory contents with full details
        
        Returns:
            True if command executed successfully
        """
        self.logger.info("ðŸ” DEBUG: Listing remote directory contents...")
        
        remote_cmd = f"cd {self.remote_dir} && pwd && echo '=== DIRECTORY CONTENTS ===' && ls -la && echo '=== PACKAGE FILES ===' && ls -la *.pkg.tar.* 2>/dev/null || echo 'No package files found'"
        
        ssh_cmd = f"ssh -q {self.vps_user}@{self.vps_host} \"{remote_cmd}\""
        
        self.logger.debug(f"DEBUG COMMAND: {ssh_cmd}")
        
        try:
            result = self.shell_executor.run(
                ssh_cmd,
                capture=True,
                check=False,
                shell=True,
                log_cmd=False
            )
            
            self.logger.info("[DEBUG] REMOTE DIR CONTENT:")
            self.logger.info("=" * 60)
            if result.stdout:
                for line in result.stdout.strip().splitlines():
                    self.logger.info(f"[DEBUG] {line}")
            self.logger.info("=" * 60)
            
            if result.stderr:
                self.logger.warning(f"[DEBUG] STDERR: {result.stderr[:200]}")
            
            return result.returncode == 0
        except Exception as e:
            self.logger.error(f"[DEBUG] Error listing remote directory: {e}")
            return False
--- FILE: .github/scripts/modules/vps/rsync_client.py ---
"""
Rsync client for efficient file transfer between local and remote systems
Handles uploads, downloads, and mirroring operations
"""

import os
import shutil
import time
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any

from modules.common.shell_executor import ShellExecutor


class RsyncClient:
    """Handles rsync operations for file transfer between local and remote"""
    
    def __init__(self, config: Dict[str, Any], shell_executor: ShellExecutor,
                 logger: Optional[logging.Logger] = None):
        """
        Initialize RsyncClient
        
        Args:
            config: Configuration dictionary with VPS settings
            shell_executor: ShellExecutor instance for command execution
            logger: Optional logger instance
        """
        self.config = config
        self.shell_executor = shell_executor
        self.logger = logger or logging.getLogger(__name__)
        
        # Extract VPS configuration
        self.vps_user = config.get('vps_user', '')
        self.vps_host = config.get('vps_host', '')
        self.remote_dir = config.get('remote_dir', '')
        self.ssh_options = config.get('ssh_options', [])
        
        # Add quiet flag to SSH options
        self.ssh_options_with_quiet = self.ssh_options + ["-q"]
        
        # Build SSH options string for rsync
        self.ssh_opts_str = ' '.join(self.ssh_options_with_quiet)
    
    def upload(self, local_files: List[str], local_base_dir: Optional[Path] = None) -> bool:
        """
        Upload files to server using RSYNC WITHOUT --delete flag
        
        Args:
            local_files: List of local file paths to upload
            local_base_dir: Base directory for relative paths
        
        Returns:
            True if upload successful
        """
        if not local_files:
            self.logger.warning("No files to upload")
            return False
        
        # Log files to upload (safe - only filenames, not paths)
        self.logger.info(f"Files to upload ({len(local_files)}):")
        for file_path in local_files:
            try:
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                filename = os.path.basename(file_path)
                file_type = "PACKAGE"
                if self.config.get('repo_name', '') in filename:
                    file_type = "DATABASE" if not file_path.endswith('.sig') else "SIGNATURE"
                self.logger.info(f"  - {filename} ({size_mb:.1f}MB) [{file_type}]")
            except Exception:
                self.logger.info(f"  - {os.path.basename(file_path)} [UNKNOWN SIZE]")
        
        # Build RSYNC command WITHOUT --delete
        rsync_cmd = self._build_rsync_command(local_files, local_base_dir, delete=False)
        
        self.logger.info(f"RUNNING RSYNC COMMAND WITHOUT --delete:")
        self.logger.info(rsync_cmd.strip())
        
        # FIRST ATTEMPT
        start_time = time.time()
        
        try:
            result = self.shell_executor.run(
                rsync_cmd,
                shell=True,
                capture=True,
                check=False,
                log_cmd=True
            )
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            self.logger.info(f"EXIT CODE (attempt 1): {result.returncode}")
            
            if result.returncode == 0:
                self._log_rsync_output(result, "RSYNC")
                self.logger.info(f"âœ… RSYNC upload successful! ({duration} seconds)")
                return True
            else:
                self.logger.warning(f"âš ï¸ First RSYNC attempt failed (code: {result.returncode})")
                
        except Exception as e:
            self.logger.error(f"RSYNC execution error: {e}")
        
        # SECOND ATTEMPT (with different SSH options)
        self.logger.info("âš ï¸ Retrying with different SSH options...")
        time.sleep(5)
        
        rsync_cmd_retry = self._build_rsync_command(
            local_files, 
            local_base_dir, 
            delete=False,
            enhanced_ssh=True
        )
        
        self.logger.info(f"RUNNING RSYNC RETRY COMMAND WITHOUT --delete:")
        self.logger.info(rsync_cmd_retry.strip())
        
        start_time = time.time()
        
        try:
            result = self.shell_executor.run(
                rsync_cmd_retry,
                shell=True,
                capture=True,
                check=False,
                log_cmd=True
            )
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            self.logger.info(f"EXIT CODE (attempt 2): {result.returncode}")
            
            if result.returncode == 0:
                self._log_rsync_output(result, "RSYNC RETRY")
                self.logger.info(f"âœ… RSYNC upload successful on retry! ({duration} seconds)")
                return True
            else:
                self.logger.error(f"âŒ RSYNC upload failed on both attempts!")
                return False
                
        except Exception as e:
            self.logger.error(f"RSYNC retry execution error: {e}")
            return False
    
    def _build_rsync_command(self, local_files: List[str], local_base_dir: Optional[Path],
                            delete: bool = False, enhanced_ssh: bool = False) -> str:
        """
        Build rsync command with syntax safety (ALWAYS prefixing with ./)
        
        Args:
            local_files: List of local file paths
            local_base_dir: Base directory for relative paths
            delete: Whether to add --delete flag
            enhanced_ssh: Whether to use enhanced SSH options
        
        Returns:
            Rsync command string
        """
        # Build file list with safety prefix - ALWAYS use ./ prefix
        if local_base_dir:
            # Use relative paths from base directory with ./ prefix
            file_args = []
            for file_path in local_files:
                rel_path = os.path.relpath(file_path, local_base_dir)
                # ALWAYS prefix with ./ to ensure rsync treats it as local file
                safe_path = f"./{rel_path}"
                file_args.append(f"'{safe_path}'")
        else:
            # Use absolute paths but extract filename and prefix with ./
            file_args = []
            for file_path in local_files:
                # Extract just the filename and ALWAYS prefix with ./
                filename = os.path.basename(file_path)
                safe_path = f"./{filename}"
                file_args.append(f"'{safe_path}'")
        
        file_args_str = ' '.join(file_args)
        
        # Build SSH options
        if enhanced_ssh:
            ssh_opts = '-e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=60 -o ServerAliveInterval=30 -o ServerAliveCountMax=3 -q"'
        else:
            ssh_opts = f'-e "ssh {self.ssh_opts_str}"' if self.ssh_opts_str else '-q'
        
        # Build delete flag
        delete_flag = '--delete' if delete else ''
        
        # Build command
        if local_base_dir:
            # Change to base directory and use relative paths with ./ prefix
            cmd = f"cd '{local_base_dir}' && rsync -avzq --progress --stats {delete_flag} {ssh_opts} {file_args_str} '{self.vps_user}@{self.vps_host}:{self.remote_dir}/'"
        else:
            # Use current directory and ./ prefix
            cmd = f"rsync -avzq --progress --stats {delete_flag} {ssh_opts} {file_args_str} '{self.vps_user}@{self.vps_host}:{self.remote_dir}/'"
        
        return cmd.strip()
    
    def mirror_remote(self, remote_pattern: str, local_dir: Path, 
                      temp_dir: Optional[Path] = None) -> bool:
        """
        Download remote files to local directory (mirror)
        
        Args:
            remote_pattern: Remote file pattern to download
            local_dir: Local directory to save files
            temp_dir: Temporary directory for download (optional)
        
        Returns:
            True if mirror successful
        """
        if temp_dir is None:
            temp_dir = Path("/tmp/repo_mirror")
        
        # Create temporary local repository directory
        if temp_dir.exists():
            shutil.rmtree(temp_dir, ignore_errors=True)
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"Created local mirror directory: {temp_dir}")
        
        # Use rsync to download files from server
        self.logger.info(f"ðŸ“¥ Downloading remote files to local mirror...")
        
        # Build rsync command for mirroring with safety prefix
        rsync_cmd = f"rsync -avzq --progress --stats -e \"ssh -o StrictHostKeyChecking=no -o ConnectTimeout=60 -q\" '{self.vps_user}@{self.vps_host}:{self.remote_dir}/{remote_pattern}' './' 2>/dev/null || true"
        
        self.logger.info(f"RUNNING RSYNC MIRROR COMMAND:")
        self.logger.info(rsync_cmd.strip())
        
        start_time = time.time()
        
        # Change to temp directory for safe rsync
        old_cwd = os.getcwd()
        os.chdir(temp_dir)
        
        try:
            result = self.shell_executor.run(
                rsync_cmd,
                shell=True,
                capture=True,
                text=True,
                check=False,
                log_cmd=True
            )
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            self.logger.info(f"EXIT CODE: {result.returncode}")
            self._log_rsync_output(result, "RSYNC MIRROR")
            
            # List downloaded files
            downloaded_files = list(temp_dir.glob("*"))
            file_count = len(downloaded_files)
            
            if file_count > 0:
                self.logger.info(f"âœ… Successfully mirrored {file_count} files ({duration} seconds)")
                self.logger.info(f"Sample mirrored files: {[f.name for f in downloaded_files[:5]]}")
                
                # Verify file integrity and copy to target directory
                valid_files = []
                for file_path in downloaded_files:
                    if file_path.stat().st_size > 0:
                        valid_files.append(file_path)
                    else:
                        self.logger.warning(f"âš ï¸ Empty file: {file_path.name}")
                
                self.logger.info(f"Valid mirrored files: {len(valid_files)}/{file_count}")
                
                # Copy mirrored files to target directory
                self.logger.info(f"ðŸ“‹ Copying {len(valid_files)} mirrored files to target directory...")
                copied_count = 0
                for file_path in valid_files:
                    dest = local_dir / file_path.name
                    if not dest.exists():  # Don't overwrite existing files
                        shutil.copy2(file_path, dest)
                        copied_count += 1
                
                self.logger.info(f"Copied {copied_count} mirrored files to target directory")
                
                # Clean up temporary directory
                shutil.rmtree(temp_dir, ignore_errors=True)
                
                return True
            else:
                self.logger.info("â„¹ï¸ No files were mirrored")
                shutil.rmtree(temp_dir, ignore_errors=True)
                return True
                
        except Exception as e:
            self.logger.error(f"RSYNC mirror execution error: {e}")
            if temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)
            return False
        finally:
            os.chdir(old_cwd)
    
    def _log_rsync_output(self, result: Any, prefix: str = "RSYNC") -> None:
        """Log rsync output"""
        if result.stdout:
            for line in result.stdout.splitlines():
                if line.strip():
                    self.logger.debug(f"{prefix}: {line}")
        if result.stderr:
            for line in result.stderr.splitlines():
                if line.strip() and "No such file or directory" not in line:
                    self.logger.error(f"{prefix} ERR: {line}")
    
    def sync_directories(self, local_dir: Path, remote_subdir: str = "", 
                        delete: bool = False) -> bool:
        """
        Sync entire directories with safety prefix
        
        Args:
            local_dir: Local directory to sync
            remote_subdir: Remote subdirectory (optional)
            delete: Whether to delete extra files on remote
        
        Returns:
            True if sync successful
        """
        remote_target = f"{self.remote_dir}/{remote_subdir}" if remote_subdir else self.remote_dir
        
        # Build rsync command for directory sync with safety prefix
        delete_flag = '--delete' if delete else ''
        
        # Use ./ prefix for directory sync
        rsync_cmd = f"rsync -avzq --progress --stats {delete_flag} -e \"ssh {self.ssh_opts_str}\" './' '{self.vps_user}@{self.vps_host}:{remote_target}/'"
        
        self.logger.info(f"Syncing directory {local_dir} to {remote_target}")
        self.logger.debug(f"RSYNC command: {rsync_cmd.strip()}")
        
        # Change to local directory before running rsync
        old_cwd = os.getcwd()
        os.chdir(local_dir)
        
        try:
            result = self.shell_executor.run(
                rsync_cmd,
                shell=True,
                capture=True,
                check=False,
                log_cmd=True
            )
            
            if result.returncode == 0:
                self.logger.info("âœ… Directory sync successful")
                return True
            else:
                self.logger.error(f"âŒ Directory sync failed: {result.stderr[:500]}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Directory sync error: {e}")
            return False
        finally:
            os.chdir(old_cwd)
--- FILE: .github/scripts/modules/orchestrator/__init__.py ---
"""
Orchestration and workflow management
"""

# Placeholder for orchestrator modules - will be implemented in Phase 5
--- FILE: .github/scripts/modules/orchestrator/state.py ---
"""
Build state tracking for package builder system
Tracks built, skipped, and failed packages with comprehensive statistics
"""

import time
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime
import logging


@dataclass
class PackageInfo:
    """Information about a specific package"""
    name: str
    version: str
    is_aur: bool
    timestamp: float = field(default_factory=time.time)
    build_duration: Optional[float] = None
    success: bool = True
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        return {
            'name': self.name,
            'version': self.version,
            'is_aur': self.is_aur,
            'timestamp': self.timestamp,
            'build_duration': self.build_duration,
            'success': self.success,
            'error_message': self.error_message
        }


class BuildState:
    """Tracks build state, statistics, and package outcomes"""
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        Initialize BuildState
        
        Args:
            logger: Optional logger instance
        """
        self.logger = logger or logging.getLogger(__name__)
        
        # Package tracking
        self._built_packages: List[PackageInfo] = []
        self._skipped_packages: List[PackageInfo] = []
        self._failed_packages: List[PackageInfo] = []
        
        # Statistics
        self.start_time = time.time()
        self.end_time: Optional[float] = None
        self.total_packages = 0
        
        # Detailed statistics
        self.stats = {
            'aur_success': 0,
            'local_success': 0,
            'aur_failed': 0,
            'local_failed': 0,
            'aur_skipped': 0,
            'local_skipped': 0,
        }
    
    def add_built(self, pkg_name: str, version: str, is_aur: bool = False, 
                  build_duration: Optional[float] = None) -> None:
        """
        Add a successfully built package
        
        Args:
            pkg_name: Package name
            version: Package version
            is_aur: Whether it's an AUR package
            build_duration: Build duration in seconds
        """
        pkg_info = PackageInfo(
            name=pkg_name,
            version=version,
            is_aur=is_aur,
            build_duration=build_duration,
            success=True
        )
        
        self._built_packages.append(pkg_info)
        
        if is_aur:
            self.stats['aur_success'] += 1
        else:
            self.stats['local_success'] += 1
        
        self.total_packages += 1
        self.logger.info(f"âœ… Built package registered: {pkg_name} ({version})")
    
    def add_skipped(self, pkg_name: str, version: str, is_aur: bool = False, 
                    reason: str = "up-to-date") -> None:
        """
        Add a skipped package
        
        Args:
            pkg_name: Package name
            version: Package version
            is_aur: Whether it's an AUR package
            reason: Reason for skipping
        """
        pkg_info = PackageInfo(
            name=pkg_name,
            version=version,
            is_aur=is_aur,
            success=True,
            error_message=f"Skipped: {reason}"
        )
        
        self._skipped_packages.append(pkg_info)
        
        if is_aur:
            self.stats['aur_skipped'] += 1
        else:
            self.stats['local_skipped'] += 1
        
        self.total_packages += 1
        self.logger.info(f"â­ï¸ Skipped package registered: {pkg_name} ({version}) - {reason}")
    
    def add_failed(self, pkg_name: str, version: str, is_aur: bool = False, 
                   error_message: str = "Build failed") -> None:
        """
        Add a failed package
        
        Args:
            pkg_name: Package name
            version: Package version
            is_aur: Whether it's an AUR package
            error_message: Error description
        """
        pkg_info = PackageInfo(
            name=pkg_name,
            version=version,
            is_aur=is_aur,
            success=False,
            error_message=error_message
        )
        
        self._failed_packages.append(pkg_info)
        
        if is_aur:
            self.stats['aur_failed'] += 1
        else:
            self.stats['local_failed'] += 1
        
        self.total_packages += 1
        self.logger.error(f"âŒ Failed package registered: {pkg_name} - {error_message}")
    
    def mark_complete(self) -> None:
        """Mark build as complete and calculate final statistics"""
        self.end_time = time.time()
    
    def get_duration(self) -> float:
        """Get total build duration in seconds"""
        end = self.end_time or time.time()
        return end - self.start_time
    
    def get_summary(self) -> Dict[str, Any]:
        """Get comprehensive build summary"""
        return {
            'total_packages': self.total_packages,
            'built': len(self._built_packages),
            'skipped': len(self._skipped_packages),
            'failed': len(self._failed_packages),
            'aur_success': self.stats['aur_success'],
            'local_success': self.stats['local_success'],
            'aur_failed': self.stats['aur_failed'],
            'local_failed': self.stats['local_failed'],
            'aur_skipped': self.stats['aur_skipped'],
            'local_skipped': self.stats['local_skipped'],
            'duration_seconds': self.get_duration(),
            'start_time': datetime.fromtimestamp(self.start_time).isoformat(),
            'end_time': datetime.fromtimestamp(self.end_time).isoformat() if self.end_time else None,
            'success_rate': (len(self._built_packages) / self.total_packages * 100) if self.total_packages > 0 else 0,
        }
    
    def get_built_packages(self) -> List[Dict[str, Any]]:
        """Get list of built packages as dictionaries"""
        return [pkg.to_dict() for pkg in self._built_packages]
    
    def get_skipped_packages(self) -> List[Dict[str, Any]]:
        """Get list of skipped packages as dictionaries"""
        return [pkg.to_dict() for pkg in self._skipped_packages]
    
    def get_failed_packages(self) -> List[Dict[str, Any]]:
        """Get list of failed packages as dictionaries"""
        return [pkg.to_dict() for pkg in self._failed_packages]
    
    def get_all_packages(self) -> List[Dict[str, Any]]:
        """Get all packages combined"""
        return (
            self.get_built_packages() +
            self.get_skipped_packages() +
            self.get_failed_packages()
        )
    
    def reset(self) -> None:
        """Reset build state for a new build"""
        self._built_packages.clear()
        self._skipped_packages.clear()
        self._failed_packages.clear()
        
        self.start_time = time.time()
        self.end_time = None
        self.total_packages = 0
        
        for key in self.stats:
            self.stats[key] = 0
        
        self.logger.info("Build state reset for new build")
--- FILE: .github/scripts/modules/orchestrator/package_builder.py ---
"""
Main orchestrator for package builder system - SERVER-FIRST ARCHITECTURE
VPS file list is the ONLY source of truth
"""

import os
import sys
import time
import re
import logging
import shutil
import subprocess
import tempfile
import glob
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Set

# Common utilities
from modules.common.logging_utils import setup_logging, get_logger
from modules.common.config_loader import ConfigLoader
from modules.common.environment import EnvironmentValidator
from modules.common.shell_executor import ShellExecutor

# State management
from modules.orchestrator.state import BuildState
from modules.repo.version_tracker import VersionTracker

# VPS communication
from modules.vps.ssh_client import SSHClient
from modules.vps.rsync_client import RsyncClient

# Build logic
from modules.build.version_manager import VersionManager
from modules.build.aur_builder import AURBuilder
from modules.build.local_builder import LocalBuilder

# Repository management
from modules.repo.database_manager import DatabaseManager
from modules.repo.cleanup_manager import CleanupManager

# GPG handling
from modules.gpg.gpg_handler import GPGHandler


class PackageBuilder:
    """Main orchestrator - SERVER-FIRST ARCHITECTURE"""
    
    def __init__(self):
        """Initialize PackageBuilder with server-first architecture"""
        self.logger = get_logger(__name__)
        
        # Phase 1: Common utilities
        self.env_validator = EnvironmentValidator(self.logger)
        self.env_validator.validate()
        
        self.repo_root = self.env_validator.get_repo_root()
        self.config_loader = ConfigLoader(self.repo_root, self.logger)
        self.config = self.config_loader.load_config()
        
        # Setup logging with debug mode from config
        debug_mode = self.config.get('debug_mode', False)
        setup_logging(debug_mode=debug_mode)
        
        # Shell executor with debug mode
        self.shell_executor = ShellExecutor(
            debug_mode=debug_mode,
            default_timeout=1800
        )
        
        # CRITICAL FIX: Run pacman -Sy BEFORE any operations
        self._sync_pacman_databases_initial()
        
        # Phase 2: State management and VPS communication
        self.build_state = BuildState(self.logger)
        
        # VPS clients
        self.ssh_client = SSHClient(self.config, self.shell_executor, self.logger)
        self.rsync_client = RsyncClient(self.config, self.shell_executor, self.logger)
        
        # Setup SSH configuration
        ssh_key = self.config.get('ssh_key', '')
        self.ssh_client.setup_ssh_config(ssh_key)
        
        # Version tracker with JSON state
        self.version_tracker = VersionTracker(
            repo_root=self.repo_root,
            ssh_client=self.ssh_client,
            logger=self.logger
        )
        
        # Phase 3: Build and repository logic
        self.version_manager = VersionManager(self.shell_executor, self.logger)
        
        # Builders (will be used only when server says we need to build)
        self.aur_builder = AURBuilder(
            config=self.config,
            shell_executor=self.shell_executor,
            version_manager=self.version_manager,
            version_tracker=self.version_tracker,
            build_state=self.build_state,
            logger=self.logger
        )
        
        self.local_builder = LocalBuilder(
            config=self.config,
            shell_executor=self.shell_executor,
            version_manager=self.version_manager,
            version_tracker=self.version_tracker,
            build_state=self.build_state,
            logger=self.logger
        )
        
        # Repository managers
        self.database_manager = DatabaseManager(
            config=self.config,
            ssh_client=self.ssh_client,
            rsync_client=self.rsync_client,
            logger=self.logger
        )
        
        self.cleanup_manager = CleanupManager(
            config=self.config,
            version_tracker=self.version_tracker,
            ssh_client=self.ssh_client,
            rsync_client=self.rsync_client,
            logger=self.logger
        )
        
        # GPG handler
        self.gpg_handler = GPGHandler()
        
        # Package lists
        self.local_packages: List[str] = []
        self.aur_packages: List[str] = []
        
        # Track sanitized artifacts
        self._sanitized_files: Dict[str, str] = {}
        
        # Local staging directory for database operations
        self._staging_dir: Optional[Path] = None
        
        self.logger.info("âœ… PackageBuilder initialized with SERVER-FIRST architecture")
    
    def _sync_pacman_databases_initial(self) -> bool:
        """
        CRITICAL FIX: Sync pacman databases BEFORE any operations
        
        Returns:
            True if sync successful
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("CRITICAL: Syncing pacman databases BEFORE operations")
        self.logger.info("=" * 60)
        
        cmd = "sudo LC_ALL=C pacman -Sy --noconfirm"
        result = self.shell_executor.run(
            cmd,
            log_cmd=True,
            timeout=300,
            check=False,
            shell=True
        )
        
        if result.returncode == 0:
            self.logger.info("âœ… Pacman databases synced successfully")
            return True
        else:
            self.logger.error("âŒ Initial pacman sync failed")
            if result.stderr:
                self.logger.error(f"Error: {result.stderr[:500]}")
            return False
    
    def _get_package_lists(self) -> Tuple[List[str], List[str]]:
        """Get package lists from configuration"""
        if not self.local_packages or not self.aur_packages:
            self.local_packages, self.aur_packages = self.config_loader.get_package_lists()
        return self.local_packages, self.aur_packages
    
    def _resolve_vcs_version_before_build(self, pkg_name: str, is_aur: bool) -> Optional[Tuple[str, str, str]]:
        """
        VCS PRIORITY FIX: Resolve git/VCS package version BEFORE building
        
        Args:
            pkg_name: Package name
            is_aur: Whether it's an AUR package
        
        Returns:
            Tuple of (pkgver, pkgrel, epoch) or None if failed
        """
        self.logger.info(f"ðŸ” Pre-resolving VCS version for {pkg_name}...")
        
        if is_aur:
            aur_dir = Path(self.config.get('aur_build_dir', 'build_aur'))
            temp_dir = aur_dir / f"temp_{pkg_name}"
            
            if temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)
            
            aur_urls = self.config.get('aur_urls', [
                "https://aur.archlinux.org/{pkg_name}.git",
                "git://aur.archlinux.org/{pkg_name}.git"
            ])
            
            clone_success = False
            for aur_url_template in aur_urls:
                aur_url = aur_url_template.format(pkg_name=pkg_name)
                result = self.shell_executor.run(
                    f"git clone --depth 1 {aur_url} {temp_dir}",
                    check=False,
                    log_cmd=False
                )
                if result and result.returncode == 0:
                    clone_success = True
                    break
            
            if not clone_success:
                self.logger.error(f"Failed to clone {pkg_name} for version resolution")
                return None
            
            pkg_dir = temp_dir
        else:
            pkg_dir = self.repo_root / pkg_name
            if not pkg_dir.exists():
                self.logger.error(f"Package directory not found: {pkg_name}")
                return None
        
        try:
            version_info = self.version_tracker.resolve_vcs_version(pkg_name, pkg_dir)
            
            if version_info:
                pkgver, pkgrel, epoch = version_info
                self.logger.info(f"âœ… VCS version resolved: {pkgver}-{pkgrel}")
            else:
                pkgver, pkgrel, epoch = self.version_manager.extract_version_from_srcinfo(pkg_dir)
                self.logger.info(f"âœ… Standard version extracted: {pkgver}-{pkgrel}")
            
            if is_aur and temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)
            
            return pkgver, pkgrel, epoch
        except Exception as e:
            self.logger.error(f"Failed to resolve version for {pkg_name}: {e}")
            if is_aur and temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)
            return None
    
    def _sanitize_artifacts(self, pkg_name: str) -> List[Path]:
        """
        INTERNAL SANITIZATION: Replace ':' with '_' in filenames BEFORE rsync
        
        Args:
            pkg_name: Package name
        
        Returns:
            List of sanitized file paths
        """
        self.logger.info(f"ðŸ”§ Sanitizing artifacts for {pkg_name}...")
        
        output_dir = Path(self.config.get('output_dir', 'built_packages'))
        sanitized_files = []
        
        patterns = [f"*{pkg_name}*.pkg.tar.*", f"{pkg_name}*.pkg.tar.*"]
        
        for pattern in patterns:
            for pkg_file in output_dir.glob(pattern):
                original_name = pkg_file.name
                
                if ':' in original_name:
                    sanitized_name = original_name.replace(':', '_')
                    sanitized_path = pkg_file.with_name(sanitized_name)
                    
                    try:
                        pkg_file.rename(sanitized_path)
                        self.logger.info(f"  ðŸ”„ Renamed: {original_name} -> {sanitized_name}")
                        
                        self._sanitized_files[str(pkg_file)] = str(sanitized_path)
                        
                        sig_file = pkg_file.with_suffix(pkg_file.suffix + '.sig')
                        if sig_file.exists():
                            sanitized_sig = sanitized_path.with_suffix(sanitized_path.suffix + '.sig')
                            sig_file.rename(sanitized_sig)
                            self.logger.info(f"  ðŸ”„ Renamed signature: {sig_file.name} -> {sanitized_sig.name}")
                        
                        sanitized_files.append(sanitized_path)
                    except Exception as e:
                        self.logger.error(f"Failed to rename {original_name}: {e}")
                        sanitized_files.append(pkg_file)
                else:
                    sanitized_files.append(pkg_file)
        
        build_dirs = [
            Path(self.config.get('aur_build_dir', 'build_aur')) / pkg_name,
            self.repo_root / pkg_name
        ]
        
        for build_dir in build_dirs:
            if build_dir.exists():
                for pkg_file in build_dir.glob("*.pkg.tar.*"):
                    original_name = pkg_file.name
                    if ':' in original_name:
                        sanitized_name = original_name.replace(':', '_')
                        sanitized_path = pkg_file.with_name(sanitized_name)
                        
                        try:
                            pkg_file.rename(sanitized_path)
                            self.logger.info(f"  ðŸ”„ Renamed in build dir: {original_name} -> {sanitized_name}")
                            self._sanitized_files[str(pkg_file)] = str(sanitized_path)
                        except Exception as e:
                            self.logger.warning(f"Failed to rename in build dir {original_name}: {e}")
        
        self.logger.info(f"âœ… Sanitized {len(sanitized_files)} files for {pkg_name}")
        return sanitized_files
    
    def _check_server_for_package(self, pkg_name: str, version_str: str, is_aur: bool) -> str:
        """
        SERVER-FIRST LOGIC: Check if package exists on server
        
        Args:
            pkg_name: Package name
            version_str: Full version string
            is_aur: Whether it's an AUR package
        
        Returns:
            "ADOPT" if package exists on server, "BUILD" if not
        """
        found, remote_version, remote_hash = self.version_tracker.is_package_on_remote(pkg_name, version_str)
        
        if found:
            self.logger.info(f"âœ… [ADOPT] {pkg_name} {version_str} found on server. Skipping build.")
            
            self.version_tracker.register_built_package(pkg_name, version_str, remote_hash)
            self.build_state.add_skipped(pkg_name, version_str, is_aur=is_aur, reason="already-on-server")
            
            return "ADOPT"
        else:
            self.logger.info(f"ðŸ”„ [BUILD] {pkg_name} {version_str} not on server or outdated. Starting build.")
            return "BUILD"
    
    def _build_aur_packages_server_first(self) -> None:
        """
        Build AUR packages using SERVER-FIRST logic with VCS PRIORITY FIX
        """
        if not self.aur_packages:
            self.logger.info("No AUR packages to build")
            return
        
        self.logger.info(f"\nðŸ”¨ Processing {len(self.aur_packages)} AUR packages (SERVER-FIRST)")
        
        for pkg_name in self.aur_packages:
            self.logger.info(f"\n--- Processing AUR: {pkg_name} ---")
            
            version_info = self._resolve_vcs_version_before_build(pkg_name, is_aur=True)
            if not version_info:
                self.build_state.add_failed(
                    pkg_name,
                    "unknown",
                    is_aur=True,
                    error_message="Failed to resolve version"
                )
                continue
            
            pkgver, pkgrel, epoch = version_info
            version_str = self.version_manager.get_full_version_string(pkgver, pkgrel, epoch)
            
            decision = self._check_server_for_package(pkg_name, version_str, is_aur=True)
            
            if decision == "ADOPT":
                continue
            
            self.logger.info(f"ðŸš€ Building {pkg_name} ({version_str})...")
            
            success = self.aur_builder.build(pkg_name, None)
            
            if success:
                self._sanitize_artifacts(pkg_name)
                
                self.version_tracker.register_built_package(pkg_name, version_str)
                self.build_state.add_built(pkg_name, version_str, is_aur=True)
                
                self._queue_old_version_cleanup(pkg_name, version_str)
            else:
                self.build_state.add_failed(
                    pkg_name,
                    version_str,
                    is_aur=True,
                    error_message="AUR build failed"
                )
    
    def _build_local_packages_server_first(self) -> None:
        """
        Build local packages using SERVER-FIRST logic with VCS PRIORITY FIX
        """
        if not self.local_packages:
            self.logger.info("No local packages to build")
            return
        
        self.logger.info(f"\nðŸ”¨ Processing {len(self.local_packages)} local packages (SERVER-FIRST)")
        
        for pkg_name in self.local_packages:
            self.logger.info(f"\n--- Processing Local: {pkg_name} ---")
            
            version_info = self._resolve_vcs_version_before_build(pkg_name, is_aur=False)
            if not version_info:
                self.build_state.add_failed(
                    pkg_name,
                    "unknown",
                    is_aur=False,
                    error_message="Failed to resolve version"
                )
                continue
            
            pkgver, pkgrel, epoch = version_info
            version_str = self.version_manager.get_full_version_string(pkgver, pkgrel, epoch)
            
            decision = self._check_server_for_package(pkg_name, version_str, is_aur=False)
            
            if decision == "ADOPT":
                continue
            
            self.logger.info(f"ðŸš€ Building {pkg_name} ({version_str})...")
            
            success = self.local_builder.build(pkg_name, None)
            
            if success:
                self._sanitize_artifacts(pkg_name)
                
                self.version_tracker.register_built_package(pkg_name, version_str)
                self.build_state.add_built(pkg_name, version_str, is_aur=False)
                
                self._queue_old_version_cleanup(pkg_name, version_str)
            else:
                self.build_state.add_failed(
                    pkg_name,
                    version_str,
                    is_aur=False,
                    error_message="Local build failed"
                )
    
    def _queue_old_version_cleanup(self, pkg_name: str, keep_version: str):
        """
        Queue old versions for cleanup (but don't execute yet)
        
        Args:
            pkg_name: Package name
            keep_version: Version to keep (newly built/adopted)
        """
        self.logger.info(f"ðŸ§¹ Queuing cleanup of old versions for {pkg_name}...")
        
        remote_files = self.ssh_client.get_cached_inventory()
        if not remote_files:
            return
        
        for remote_path in remote_files.values():
            filename = Path(remote_path).name
            
            parsed = self.version_tracker._parse_package_filename_with_arch(filename)
            if not parsed:
                continue
            
            remote_pkg_name, remote_version, architecture = parsed
            
            if remote_pkg_name.lower() == pkg_name.lower():
                if not self.version_tracker._versions_match(remote_version, keep_version):
                    self.version_tracker.queue_deletion(remote_path)
                    self.logger.debug(f"ðŸ—‘ï¸ Queued for deletion: {filename} (old version: {remote_version})")
                else:
                    self.logger.debug(f"âœ… Keeping: {filename} (current version: {remote_version})")
    
    def _create_local_staging(self) -> Path:
        """
        Create local staging directory for database operations
        
        Returns:
            Path to staging directory
        """
        if self._staging_dir and self._staging_dir.exists():
            shutil.rmtree(self._staging_dir, ignore_errors=True)
        
        self._staging_dir = Path(tempfile.mkdtemp(prefix="repo_staging_"))
        self.logger.info(f"ðŸ“ Created staging directory: {self._staging_dir}")
        return self._staging_dir
    
    def _download_existing_database_only(self) -> bool:
        """
        Download ONLY database files from VPS to staging (no packages)
        
        Returns:
            True if successful or no database exists (first run)
        """
        repo_name = self.config.get('repo_name', '')
        
        self.logger.info("ðŸ“¥ Downloading existing database files only from VPS...")
        
        patterns = [
            f"{repo_name}.db.tar.gz*",
            f"{repo_name}.files.tar.gz*"
        ]
        
        for pattern in patterns:
            self.logger.info(f"  Downloading pattern: {pattern}")
            
            success = self.rsync_client.mirror_remote(
                remote_pattern=pattern,
                local_dir=self._staging_dir,
                temp_dir=None
            )
            
            if not success:
                self.logger.warning(f"âš ï¸ Failed to download {pattern}")
        
        db_files = list(self._staging_dir.glob(f"{repo_name}.db.tar.gz*"))
        files_files = list(self._staging_dir.glob(f"{repo_name}.files.tar.gz*"))
        
        total_files = len(db_files) + len(files_files)
        
        if total_files > 0:
            self.logger.info(f"âœ… Downloaded {total_files} database files")
        else:
            self.logger.info("â„¹ï¸ No existing database files found (first run or clean state)")
        
        return True
    
    def _create_dummy_files_for_all_packages(self) -> None:
        """
        Create dummy files for ALL packages that should be in the repository
        
        Creates dummy files for:
        1. Newly built packages (already moved to staging)
        2. Adopted packages (already on server)
        3. Any other packages that exist on server
        
        This ensures repo-add has a complete view of the repository.
        """
        self.logger.info("ðŸ”„ Creating dummy files for all repository packages...")
        
        # Get all packages that should be in the repository
        all_packages = set()
        
        # 1. Get adopted packages (already on server, skipped in this build)
        adopted_packages = self.version_tracker.get_skipped_packages_dict()
        for pkg_name, version_str in adopted_packages.items():
            # Create filename pattern for this package
            dummy_name = f"{pkg_name}-{version_str}-x86_64.pkg.tar.zst"
            all_packages.add((dummy_name, True))  # True = is dummy (needs creation)
        
        # 2. Get built packages (already in staging with real files)
        staging_packages = list(self._staging_dir.glob("*.pkg.tar.zst"))
        for pkg_file in staging_packages:
            if pkg_file.stat().st_size > 0:  # Only count real files
                all_packages.add((pkg_file.name, False))  # False = real file (already exists)
        
        # 3. Get all packages from server inventory
        remote_files = self.ssh_client.get_cached_inventory()
        for filename in remote_files.keys():
            if filename.endswith('.pkg.tar.zst'):
                # Check if we already have this package (either dummy or real)
                if not any(filename == pkg_name for pkg_name, _ in all_packages):
                    all_packages.add((filename, True))  # Need dummy
        
        # Create dummy files for those that need them
        created_count = 0
        for pkg_filename, needs_dummy in all_packages:
            if needs_dummy:
                dummy_path = self._staging_dir / pkg_filename
                if not dummy_path.exists():
                    try:
                        dummy_path.touch()
                        self.logger.debug(f"  Created dummy: {pkg_filename}")
                        created_count += 1
                        
                        # Also create dummy signature
                        sig_path = dummy_path.with_suffix(dummy_path.suffix + '.sig')
                        if not sig_path.exists():
                            sig_path.touch()
                            self.logger.debug(f"  Created dummy signature: {sig_filename}")
                    except Exception as e:
                        self.logger.warning(f"Failed to create dummy for {pkg_filename}: {e}")
        
        self.logger.info(f"âœ… Created {created_count} dummy package files")
        self.logger.info(f"ðŸ“Š Total packages in staging: {len(all_packages)}")
    
    def _move_new_packages_to_staging(self) -> List[Path]:
        """
        Move newly built packages to staging directory
        
        Returns:
            List of paths to new packages moved to staging
        """
        output_dir = Path(self.config.get('output_dir', 'built_packages'))
        
        new_packages = list(output_dir.glob("*.pkg.tar.zst"))
        if not new_packages:
            self.logger.info("â„¹ï¸ No new packages to move to staging")
            return []
        
        self.logger.info(f"ðŸ“¦ Moving {len(new_packages)} new packages to staging...")
        
        moved_packages = []
        
        for new_pkg in new_packages:
            try:
                dest = self._staging_dir / new_pkg.name
                if dest.exists():
                    dest.unlink()
                shutil.move(str(new_pkg), str(dest))
                moved_packages.append(dest)
                
                sig_file = new_pkg.with_suffix(new_pkg.suffix + '.sig')
                if sig_file.exists():
                    sig_dest = dest.with_suffix(dest.suffix + '.sig')
                    if sig_dest.exists():
                        sig_dest.unlink()
                    shutil.move(str(sig_file), str(sig_dest))
                
                self.logger.debug(f"  Moved: {new_pkg.name}")
            except Exception as e:
                self.logger.error(f"Failed to move {new_pkg.name}: {e}")
        
        self.logger.info(f"âœ… Moved {len(moved_packages)} new packages to staging")
        return moved_packages
    
    def _clean_dummy_files_before_upload(self) -> None:
        """
        Remove all 0-byte dummy files from staging before upload
        
        This ensures dummy files never reach the VPS
        """
        self.logger.info("ðŸ§¹ Cleaning dummy files before upload...")
        
        removed_count = 0
        
        # Remove dummy package files (0-byte .pkg.tar.zst)
        for pkg_file in self._staging_dir.glob("*.pkg.tar.zst"):
            if pkg_file.stat().st_size == 0:
                try:
                    pkg_file.unlink()
                    removed_count += 1
                    
                    # Also remove corresponding signature if it's also 0-byte
                    sig_file = pkg_file.with_suffix(pkg_file.suffix + '.sig')
                    if sig_file.exists() and sig_file.stat().st_size == 0:
                        sig_file.unlink()
                        removed_count += 1
                except Exception as e:
                    self.logger.warning(f"Failed to remove dummy file {pkg_file.name}: {e}")
        
        self.logger.info(f"âœ… Removed {removed_count} dummy files")
    
    def _update_database_locally(self) -> bool:
        """
        Update repository database locally with GPG signing
        
        Returns:
            True if successful
        """
        repo_name = self.config.get('repo_name', '')
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("LOCAL DATABASE UPDATE WITH GPG SIGNING")
        self.logger.info("=" * 60)
        
        old_cwd = os.getcwd()
        os.chdir(self._staging_dir)
        
        try:
            db_file = f"{repo_name}.db.tar.gz"
            
            self.logger.info("Cleaning old database files...")
            for f in [f"{repo_name}.db", f"{repo_name}.db.tar.gz", 
                      f"{repo_name}.files", f"{repo_name}.files.tar.gz"]:
                if os.path.exists(f):
                    os.remove(f)
            
            package_files = list(glob.glob("*.pkg.tar.zst"))
            if not package_files:
                self.logger.error("âŒ No package files found for database update")
                return False
            
            self.logger.info(f"Found {len(package_files)} package files for database update")
            
            if self.gpg_handler.gpg_enabled:
                self.logger.info("ðŸ” Running repo-add with GPG signing...")
                
                # Set GNUPGHOME environment variable for non-interactive signing
                env = os.environ.copy()
                if hasattr(self.gpg_handler, 'gpg_home') and self.gpg_handler.gpg_home:
                    env['GNUPGHOME'] = self.gpg_handler.gpg_home
                
                cmd = f"repo-add --sign --remove {db_file} *.pkg.tar.zst"
                
                result = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    env=env,
                    check=False
                )
            else:
                self.logger.info("ðŸ”§ Running repo-add without signing...")
                cmd = f"repo-add --remove {db_file} *.pkg.tar.zst"
                
                result = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    check=False
                )
            
            if result.returncode == 0:
                self.logger.info("âœ… Database updated successfully")
                
                if not os.path.exists(db_file):
                    self.logger.error("âŒ Database file not created")
                    return False
                
                # Verify the database was created with signatures if GPG enabled
                if self.gpg_handler.gpg_enabled:
                    sig_file = f"{db_file}.sig"
                    if os.path.exists(sig_file):
                        sig_size = os.path.getsize(sig_file)
                        if sig_size > 0:
                            self.logger.info(f"âœ… Database signed successfully ({sig_size} bytes)")
                        else:
                            self.logger.warning("âš ï¸ Database signature file is empty")
                    else:
                        self.logger.warning("âš ï¸ Database signature file not found")
                
                # Verify database entries
                self._verify_database_entries(db_file)
                return True
            else:
                self.logger.error(f"âŒ repo-add failed with exit code {result.returncode}:")
                if result.stdout:
                    self.logger.error(f"STDOUT: {result.stdout[:500]}")
                if result.stderr:
                    self.logger.error(f"STDERR: {result.stderr[:500]}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Database update error: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            os.chdir(old_cwd)
    
    def _verify_database_entries(self, db_file: str) -> None:
        """Verify database entries after update"""
        try:
            list_cmd = ["tar", "-tzf", db_file]
            result = subprocess.run(list_cmd, capture_output=True, text=True, check=False)
            if result.returncode == 0:
                db_entries = [line for line in result.stdout.split('\n') if line.endswith('/desc')]
                self.logger.info(f"âœ… Database contains {len(db_entries)} package entries")
                if len(db_entries) == 0:
                    self.logger.error("âŒâŒâŒ DATABASE IS EMPTY!")
                else:
                    self.logger.info(f"Sample entries: {db_entries[:3]}")
            else:
                self.logger.warning(f"Could not list database contents: {result.stderr}")
        except Exception as e:
            self.logger.warning(f"Could not verify database: {e}")
    
    def _upload_only_real_files(self) -> bool:
        """
        Upload ONLY real files (not dummies) to VPS
        
        Returns:
            True if successful
        """
        self.logger.info("\nðŸ“¤ Uploading real files to VPS (excluding dummies)...")
        
        files_to_upload = []
        
        # Collect real package files (non-zero size)
        for pkg_file in self._staging_dir.glob("*.pkg.tar.zst"):
            if pkg_file.stat().st_size > 0:
                files_to_upload.append(pkg_file)
                
                # Include signature if it exists and has content
                sig_file = pkg_file.with_suffix(pkg_file.suffix + '.sig')
                if sig_file.exists() and sig_file.stat().st_size > 0:
                    files_to_upload.append(sig_file)
        
        self.logger.info(f"Found {len([f for f in files_to_upload if str(f).endswith('.pkg.tar.zst')])} real package files")
        
        # Collect database files
        repo_name = self.config.get('repo_name', '')
        for pattern in [f"{repo_name}.db*", f"{repo_name}.files*"]:
            for db_file in self._staging_dir.glob(pattern):
                if db_file.stat().st_size > 0:
                    files_to_upload.append(db_file)
        
        if not files_to_upload:
            self.logger.warning("âš ï¸ No files to upload (all files are dummies or empty)")
            return True
        
        self.logger.info(f"ðŸ“¦ Total real files to upload: {len(files_to_upload)}")
        
        files_list = [str(f) for f in files_to_upload]
        upload_success = self.rsync_client.upload(files_list, self._staging_dir)
        
        if upload_success:
            self.logger.info("âœ… All real files uploaded successfully")
            return True
        else:
            self.logger.error("âŒ File upload failed")
            return False
    
    def _cleanup_staging(self) -> None:
        """Clean up staging directory"""
        if self._staging_dir and os.path.exists(self._staging_dir):
            try:
                shutil.rmtree(self._staging_dir, ignore_errors=True)
                self.logger.debug(f"ðŸ§¹ Cleaned up staging directory: {self._staging_dir}")
                self._staging_dir = None
            except Exception as e:
                self.logger.warning(f"Could not clean staging directory: {e}")
    
    def _force_database_update(self) -> bool:
        """
        FORCE database update even if no new packages were built
        
        This ensures the remote DB is always healthy and signed
        
        Returns:
            True if successful
        """
        self.logger.info("\nðŸ”§ FORCING database update (ensuring remote DB is healthy)...")
        
        try:
            # Step 1: Create staging directory
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 1: Create local staging directory")
            self.logger.info("=" * 60)
            staging_dir = self._create_local_staging()
            
            # Step 2: Download existing database files
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 2: Download existing database files from VPS")
            self.logger.info("=" * 60)
            self._download_existing_database_only()
            
            # Step 3: Move any new packages to staging
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 3: Move new packages to staging")
            self.logger.info("=" * 60)
            self._move_new_packages_to_staging()
            
            # Step 4: Create dummy files for all packages
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 4: Create dummy files for all repository packages")
            self.logger.info("=" * 60)
            self._create_dummy_files_for_all_packages()
            
            # Step 5: Update database locally with GPG signing
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 5: Update database locally with GPG signing")
            self.logger.info("=" * 60)
            if not self._update_database_locally():
                self.logger.error("âŒ Local database update failed")
                return False
            
            # Step 6: Clean dummy files before upload
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 6: Clean dummy files before upload")
            self.logger.info("=" * 60)
            self._clean_dummy_files_before_upload()
            
            # Step 7: Upload only real files
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 7: Upload only real files to VPS")
            self.logger.info("=" * 60)
            if not self._upload_only_real_files():
                self.logger.error("âŒ File upload failed")
                return False
            
            # Step 8: Execute queued cleanup
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 8: Execute queued cleanup operations")
            self.logger.info("=" * 60)
            cleanup_success = self.version_tracker.commit_queued_deletions()
            
            if cleanup_success:
                self.logger.info("âœ… Cleanup operations completed")
            else:
                self.logger.warning("âš ï¸ Some cleanup operations failed")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Force database update failed: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 9: Cleanup staging directory")
            self.logger.info("=" * 60)
            self._cleanup_staging()
    
    def run(self) -> int:
        """
        Main execution workflow - SERVER-FIRST ARCHITECTURE
        
        Returns:
            Exit code (0 for success, 1 for failure)
        """
        try:
            self.logger.info("\n" + "=" * 60)
            self.logger.info("ðŸš€ MANJARO PACKAGE BUILDER - SERVER-FIRST ARCHITECTURE")
            self.logger.info("=" * 60)
            
            self.logger.info("\nðŸ”§ Initial setup...")
            self.logger.info(f"Repository root: {self.repo_root}")
            self.logger.info(f"Repository name: {self.config.get('repo_name')}")
            self.logger.info(f"Output directory: {self.config.get('output_dir')}")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 0: GPG INITIALIZATION")
            self.logger.info("=" * 60)
            
            if self.gpg_handler.gpg_enabled:
                if not self.gpg_handler.import_gpg_key():
                    self.logger.error("âŒ Failed to import GPG key, disabling signing")
                else:
                    self.logger.info("âœ… GPG initialized successfully")
            else:
                self.logger.info("â„¹ï¸ GPG signing disabled (no key provided)")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 1: SSH CONNECTION TEST")
            self.logger.info("=" * 60)
            
            if not self.ssh_client.test_connection():
                self.logger.error("âŒ SSH connection failed")
                return 1
            
            self.ssh_client.debug_remote_directory()
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 2: REMOTE DIRECTORY SETUP")
            self.logger.info("=" * 60)
            
            if not self.ssh_client.ensure_directory():
                self.logger.warning("âš ï¸ Could not ensure remote directory exists")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 3: PACKAGE DISCOVERY")
            self.logger.info("=" * 60)
            
            self.local_packages, self.aur_packages = self._get_package_lists()
            
            self.logger.info(f"ðŸ“¦ Package statistics:")
            self.logger.info(f"   Local packages: {len(self.local_packages)}")
            self.logger.info(f"   AUR packages: {len(self.aur_packages)}")
            self.logger.info(f"   Total packages: {len(self.local_packages) + len(self.aur_packages)}")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 4: SERVER-FIRST PACKAGE PROCESSING")
            self.logger.info("=" * 60)
            
            self._build_aur_packages_server_first()
            self._build_local_packages_server_first()
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 5: FORCED DATABASE UPDATE (ALWAYS RUN)")
            self.logger.info("=" * 60)
            
            # ALWAYS run database update, even if no new packages
            db_update_success = self._force_database_update()
            
            if not db_update_success:
                self.logger.error("\nâŒ Database update failed!")
                return 1
            
            self.gpg_handler.cleanup()
            self.logger.info("\nâœ… Repository maintenance completed successfully!")
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("STEP 6: FINAL STATISTICS")
            self.logger.info("=" * 60)
            
            self.build_state.mark_complete()
            summary = self.build_state.get_summary()
            
            self.logger.info(f"Duration: {summary['duration_seconds']:.1f}s")
            self.logger.info(f"AUR packages:    {summary['aur_success']} built, {summary['aur_skipped']} adopted, {summary['aur_failed']} failed")
            self.logger.info(f"Local packages:  {summary['local_success']} built, {summary['local_skipped']} adopted, {summary['local_failed']} failed")
            self.logger.info(f"Total built:     {summary['built']}")
            self.logger.info(f"Total adopted:   {summary['skipped']}")
            self.logger.info(f"GPG signing:     {'Enabled' if self.gpg_handler.gpg_enabled else 'Disabled'}")
            self.logger.info(f"VCS priority fix: âœ… Implemented")
            self.logger.info(f"Internal sanitization: âœ… {len(self._sanitized_files)} files renamed")
            self.logger.info(f"Forced DB update: âœ… Always executed")
            
            state_summary = self.version_tracker.get_state_summary()
            self.logger.info(f"Packages tracked: {state_summary['total_packages']}")
            self.logger.info("=" * 60)
            
            return 0
            
        except Exception as e:
            self.logger.error(f"\nâŒ Build failed: {e}")
            import traceback
            traceback.print_exc()
            
            if hasattr(self, 'gpg_handler'):
                self.gpg_handler.cleanup()
            
            if hasattr(self, 'version_tracker'):
                self.version_tracker.save_state()
            
            if hasattr(self, '_staging_dir') and self._staging_dir and os.path.exists(self._staging_dir):
                shutil.rmtree(self._staging_dir, ignore_errors=True)
            
            return 1
--- FILE: .github/scripts/modules/gpg/__init__.py ---
"""
GPG and signing operations
"""

# GPG handler already exists, will be moved in a later phase
--- FILE: .github/scripts/modules/gpg/gpg_handler.py ---
"""
GPG Handler Module - Handles GPG key import, signing, and pacman-key operations
"""

import os
import subprocess
import shutil
import tempfile
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


class GPGHandler:
    """Handles GPG key import, repository signing, and pacman-key operations"""
    
    def __init__(self):
        self.gpg_private_key = os.getenv('GPG_PRIVATE_KEY')
        self.gpg_key_id = os.getenv('GPG_KEY_ID')
        self.gpg_enabled = bool(self.gpg_private_key and self.gpg_key_id)
        self.gpg_home = None
        self.gpg_env = None
        
        # Safe logging - no sensitive information
        if self.gpg_key_id:
            logger.info(f"GPG Environment Check: Key ID found: YES, Key data found: {'YES' if self.gpg_private_key else 'NO'}")
        else:
            logger.info("GPG Environment Check: No GPG key ID configured")
    
    def import_gpg_key(self) -> bool:
        """Import GPG private key and set trust level WITHOUT interactive terminal (container-safe)"""
        if not self.gpg_enabled:
            logger.info("GPG Key not detected. Skipping repository signing.")
            return False
        
        logger.info("GPG Key detected. Importing private key...")
        
        # Handle both string and bytes for the private key
        key_data = self.gpg_private_key
        if isinstance(key_data, bytes):
            key_data_str = key_data.decode('utf-8')
        else:
            key_data_str = str(key_data)
        
        # Validate private key format before attempting import
        if not key_data_str or '-----BEGIN PGP PRIVATE KEY BLOCK-----' not in key_data_str:
            logger.error("âŒ CRITICAL: Invalid GPG private key format.")
            logger.error("Disabling GPG signing for this build.")
            self.gpg_enabled = False
            return False
        
        try:
            # Create a temporary GPG home directory
            temp_gpg_home = tempfile.mkdtemp(prefix="gpg_home_")
            
            # Set environment for GPG
            env = os.environ.copy()
            env['GNUPGHOME'] = temp_gpg_home
            
            # Import the private key
            if isinstance(self.gpg_private_key, bytes):
                key_input = self.gpg_private_key
            else:
                key_input = self.gpg_private_key.encode('utf-8')
            
            import_process = subprocess.run(
                ['gpg', '--batch', '--import'],
                input=key_input,
                capture_output=True,
                text=False,
                env=env,
                check=False
            )
            
            if import_process.returncode != 0:
                stderr = import_process.stderr.decode('utf-8') if isinstance(import_process.stderr, bytes) else import_process.stderr
                logger.error(f"Failed to import GPG key: {stderr}")
                shutil.rmtree(temp_gpg_home, ignore_errors=True)
                return False
            
            logger.info("âœ… GPG key imported successfully")
            
            # Get fingerprint and set ultimate trust
            list_process = subprocess.run(
                ['gpg', '--list-keys', '--with-colons', self.gpg_key_id],
                capture_output=True,
                text=True,
                env=env,
                check=False
            )
            
            fingerprint = None
            if list_process.returncode == 0:
                for line in list_process.stdout.split('\n'):
                    if line.startswith('fpr:'):
                        parts = line.split(':')
                        if len(parts) > 9:
                            fingerprint = parts[9]
                            # Set ultimate trust (6 = ultimate)
                            trust_process = subprocess.run(
                                ['gpg', '--import-ownertrust'],
                                input=f"{fingerprint}:6:\n".encode('utf-8'),
                                capture_output=True,
                                text=False,
                                env=env,
                                check=False
                            )
                            if trust_process.returncode == 0:
                                logger.info("âœ… Set ultimate trust for GPG key")
                            break
            
            # Export public key and add to pacman-key WITHOUT interactive terminal
            if fingerprint:
                try:
                    # Export public key to a temporary file
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.asc', delete=False) as pub_key_file:
                        export_process = subprocess.run(
                            ['gpg', '--armor', '--export', fingerprint],
                            capture_output=True,
                            text=True,
                            env=env,
                            check=True
                        )
                        pub_key_file.write(export_process.stdout)
                        pub_key_path = pub_key_file.name
                    
                    # Add to pacman-key WITH SUDO
                    logger.info("Adding GPG key to pacman-key...")
                    add_process = subprocess.run(
                        ['sudo', 'pacman-key', '--add', pub_key_path],
                        capture_output=True,
                        text=True,
                        check=False
                    )
                    
                    if add_process.returncode != 0:
                        logger.error(f"Failed to add key to pacman-key: {add_process.stderr}")
                    else:
                        logger.info("âœ… Key added to pacman-key")
                    
                    # Import ownertrust into pacman keyring
                    logger.info("Setting ultimate trust in pacman keyring...")
                    ownertrust_content = f"{fingerprint}:6:\n"
                    
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.trust', delete=False) as trust_file:
                        trust_file.write(ownertrust_content)
                        trust_file_path = trust_file.name
                    
                    trust_cmd = [
                        'sudo', 'gpg',
                        '--homedir', '/etc/pacman.d/gnupg',
                        '--batch',
                        '--import-ownertrust',
                        trust_file_path
                    ]
                    
                    try:
                        trust_process = subprocess.run(
                            trust_cmd,
                            capture_output=True,
                            text=True,
                            check=False
                        )
                        
                        if trust_process.returncode == 0:
                            logger.info("âœ… Set ultimate trust for key in pacman keyring")
                        else:
                            logger.warning(f"âš ï¸ Failed to set trust with gpg: {trust_process.stderr[:200]}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Error setting trust with gpg: {e}")
                    finally:
                        os.unlink(trust_file_path)
                        os.unlink(pub_key_path)
                    
                except Exception as e:
                    logger.error(f"Error during pacman-key setup: {e}")
            
            # Store the GPG home directory for later use
            self.gpg_home = temp_gpg_home
            self.gpg_env = env
            
            return True
            
        except Exception as e:
            logger.error(f"Error importing GPG key: {e}")
            if 'temp_gpg_home' in locals():
                shutil.rmtree(temp_gpg_home, ignore_errors=True)
            return False
    
    def sign_repository_files(self, repo_name: str, output_dir: str) -> bool:
        """Sign repository database files with GPG"""
        if not self.gpg_enabled:
            logger.info("GPG signing disabled - skipping repository signing")
            return False
        
        if not hasattr(self, 'gpg_home') or not hasattr(self, 'gpg_env'):
            logger.error("GPG key not imported. Cannot sign repository files.")
            return False
        
        try:
            output_path = Path(output_dir)
            files_to_sign = [
                output_path / f"{repo_name}.db",
                output_path / f"{repo_name}.files"
            ]
            
            signed_count = 0
            failed_count = 0
            
            for file_to_sign in files_to_sign:
                if not file_to_sign.exists():
                    logger.warning(f"Repository file not found for signing: {file_to_sign.name}")
                    continue
                
                logger.info(f"Signing repository database: {file_to_sign.name}")
                
                # Create detached signature
                sig_file = file_to_sign.with_suffix(file_to_sign.suffix + '.sig')
                
                sign_process = subprocess.run(
                    [
                        'gpg', '--detach-sign',
                        '--default-key', self.gpg_key_id,
                        '--output', str(sig_file),
                        str(file_to_sign)
                    ],
                    capture_output=True,
                    text=True,
                    env=self.gpg_env,
                    check=False
                )
                
                if sign_process.returncode == 0:
                    logger.info(f"âœ… Created signature: {sig_file.name}")
                    signed_count += 1
                else:
                    logger.warning(f"âš ï¸ Failed to sign {file_to_sign.name}: {sign_process.stderr[:200]}")
                    failed_count += 1
            
            if signed_count > 0:
                logger.info(f"âœ… Successfully signed {signed_count} repository file(s)")
                # CRITICAL FIX: Minor warnings should not block the build
                if failed_count > 0:
                    logger.warning(f"âš ï¸ {failed_count} file(s) failed to sign, but continuing anyway")
                return True
            else:
                logger.error("Failed to sign any repository files")
                # CRITICAL FIX: Don't fail the build if GPG signing has issues
                logger.warning("âš ï¸ Continuing build without GPG signatures")
                return False
                
        except Exception as e:
            logger.error(f"Error signing repository files: {e}")
            # CRITICAL FIX: Don't fail the build if GPG signing has issues
            logger.warning("âš ï¸ Continuing build without GPG signatures due to error")
            return False
    
    def cleanup(self):
        """Clean up temporary GPG home directory"""
        if hasattr(self, 'gpg_home'):
            try:
                shutil.rmtree(self.gpg_home, ignore_errors=True)
                logger.debug("Cleaned up temporary GPG home directory")
            except Exception as e:
                logger.warning(f"Could not clean up GPG directory: {e}")
--- FILE: .github/scripts/config.py ---
"""
Configuration file for Manjaro Package Builder
"""

import os

# Repository configuration
REPO_DB_NAME = "manjaro-awesome"  # Default repository name
OUTPUT_DIR = "built_packages"     # Local output directory
BUILD_TRACKING_DIR = ".buildtracking"  # Build tracking directory

# PACKAGER identity from environment variable (secure via GitHub Secrets)
PACKAGER_ID = os.getenv("PACKAGER_ENV", "Maintainer <no-reply@gshoots.hu>")

# SSH and Git configuration
SSH_REPO_URL = "git@github.com:megvadulthangya/manjaro-awesome.git"
SSH_OPTIONS = [
    "-o", "StrictHostKeyChecking=no",
    "-o", "ConnectTimeout=30",
    "-o", "BatchMode=yes"
]

# Build timeouts (seconds)
MAKEPKG_TIMEOUT = {
    "default": 3600,        # 1 hour for normal packages
    "large_packages": 7200, # 2 hours for large packages (gtk, qt, chromium)
    "simplescreenrecorder": 5400,  # 1.5 hours
}

# Special dependency mappings
SPECIAL_DEPENDENCIES = {
    "gtk2": ["gtk-doc", "docbook-xsl", "libxslt", "gobject-introspection"],
    "awesome-git": ["lua", "lgi", "imagemagick", "asciidoc"],
    "awesome-freedesktop-git": ["lua", "lgi", "imagemagick", "asciidoc"],
    "lain-git": ["lua", "lgi", "imagemagick", "asciidoc"],
    "simplescreenrecorder": ["jack2"],  # Convert jack to jack2
}

# Build tool checks (will be installed if missing)
REQUIRED_BUILD_TOOLS = [
    "make", "gcc", "pkg-config", "autoconf", "automake", 
    "libtool", "cmake", "meson", "ninja", "patch"
]

# Temporary directories (runtime-required, /tmp is POSIX invariant)
MIRROR_TEMP_DIR = "/tmp/repo_mirror"
SYNC_CLONE_DIR = "/tmp/manjaro-awesome-gitclone"

# AUR configuration
AUR_URLS = [
    "https://aur.archlinux.org/{pkg_name}.git",
    "git://aur.archlinux.org/{pkg_name}.git"
]

# Build directory names
AUR_BUILD_DIR = "build_aur"

# GitHub repository for synchronization
GITHUB_REPO = "megvadulthangya/manjaro-awesome.git"

# Debug mode configuration - when True, bypass logger for critical build output
DEBUG_MODE = True
--- FILE: .github/scripts/packages.py ---
"""
Package definitions for Manjaro Package Builder
"""

# LOCAL packages (from our repository)
LOCAL_PACKAGES = [
    "gghelper",
#    "gtk2",
#    "awesome-freedesktop-git",
#    "lain-git",
    "awesome-rofi",
#    "awesome-git",
    "awesome-welcome",
#    "tilix-git",
    "nordic-backgrounds",
    "awesome-copycats-manjaro",
    "i3lock-fancy-git",
    "ttf-font-awesome-5",
    "nvidia-driver-assistant"
#    "grayjay-bin"
]

# AUR packages (from Arch User Repository)
AUR_PACKAGES = [
    "awesome-git",
    "awesome-freedesktop-git",
    "lain-git",
    "grayjay-bin",
    "libinput-gestures",
    "gtk2",
    "gtkd",
    "qt5-styleplugins",
    "urxvt-resize-font-git",
    "i3lock-color",
    "raw-thumbnailer",
    "gsconnect",
    "tamzen-font",
    "betterlockscreen",
    "nordic-theme",
#    "nordic-darker-theme",
    "geany-nord-theme",
    "geany-plugin-preview",
    "nordzy-icon-theme",
    "oh-my-posh-bin",
    "fish-done",
    "tilix-git",
    "find-the-command",
    "p7zip-gui",
    "qownnotes",
    "xorg-font-utils",
    "xnviewmp",
    "simplescreenrecorder",
    "gtkhash-thunar",
    "a4tech-bloody-driver-git",
#    "nordic-bluish-accent-theme",
#    "nordic-bluish-accent-standard-buttons-theme",
#    "nordic-polar-standard-buttons-theme",
#    "nordic-standard-buttons-theme",
#    "nordic-darker-standard-buttons-theme"
]

# Optionally, you can also define package groups or categories
PACKAGE_CATEGORIES = {
    "desktop": ["awesome-freedesktop-git", "lain-git", "awesome-rofi"],
    "themes": ["nordic-theme", "nordic-darker-theme", "nordic-bluish-accent-theme"],
    "fonts": ["tamzen-font", "ttf-font-awesome-5"],
    "tools": ["libinput-gestures", "betterlockscreen", "simplescreenrecorder"],
    "drivers": ["nvidia-driver-assistant", "a4tech-bloody-driver-git"]
}

--- FILE: .github/scripts/builder.py ---
#!/usr/bin/env python3
"""
Manjaro Package Builder - Refactored Modular Architecture with Zero-Residue Policy
Main entry point for the refactored modular system
"""

import os
import sys
import traceback

# Add the modules directory to sys.path
script_dir = os.path.dirname(os.path.abspath(__file__))
modules_dir = os.path.join(script_dir, "modules")
sys.path.insert(0, modules_dir)

# Try to import from the new modular structure
try:
    from modules.orchestrator.package_builder import PackageBuilder
    print(">>> DEBUG: Successfully imported PackageBuilder from modular system")
    MODULES_LOADED = True
except ImportError as e:
    print(f"âŒ CRITICAL: Failed to import PackageBuilder: {e}")
    print(f"âŒ Please ensure modules are in: {modules_dir}/")
    print(f"âŒ Current sys.path: {sys.path}")
    MODULES_LOADED = False
    sys.exit(1)
except Exception as e:
    print(f"âŒ CRITICAL: Error importing PackageBuilder: {e}")
    MODULES_LOADED = False
    sys.exit(1)


def main() -> int:
    """
    Main entry point for the package builder
    
    Returns:
        Exit code (0 for success, 1 for failure)
    """
    try:
        if not MODULES_LOADED:
            print("âŒ Modules not loaded, cannot proceed")
            return 1
        
        print(">>> DEBUG: Starting PackageBuilder.run()")
        builder = PackageBuilder()
        return builder.run()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Build interrupted by user")
        return 130  # Standard exit code for Ctrl+C
    except SystemExit as e:
        # Re-raise system exit to preserve exit code
        raise e
    except Exception as e:
        print(f"\nâŒ UNEXPECTED ERROR in main(): {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
--- FILE: .github/scripts/rsync_upload.py ---
#!/usr/bin/env python3
"""
RSYNC Upload Test - Python Version
Ez a szkript teszteli a fÃ¡jlfeltÃ¶ltÃ©st RSYNC-vel egy tÃ¡voli szerverre.
"""

import os
import sys
import time
import subprocess
import tarfile
from pathlib import Path
from datetime import datetime
from typing import Tuple, Optional, List, Dict
import shutil
import stat

# === KONSTANSOK ===
OUTPUT_DIR = Path("/home/builder/built_packages")
TEST_PREFIX = f"github_test_{int(time.time())}"

# === KONFIGURÃCIÃ“ ===
class Config:
    """KonfigurÃ¡ciÃ³s osztÃ¡ly"""
    def __init__(self):
        self.remote_dir = os.environ.get("REMOTE_DIR", "/var/www/repo")
        self.vps_user = os.environ.get("VPS_USER", "root")
        self.vps_host = os.environ.get("VPS_HOST", "")
        self.test_size_mb = int(os.environ.get("TEST_SIZE_MB", "10"))
        
        # EllenÅ‘rizzÃ¼k a kÃ¶telezÅ‘ vÃ¡ltozÃ³kat
        if not self.vps_host:
            raise ValueError("VPS_HOST nincs beÃ¡llÃ­tva!")
        
        # SSH utasÃ­tÃ¡s
        self.ssh_cmd = ["ssh", "-o", "StrictHostKeyChecking=no", 
                       "-o", "ConnectTimeout=30", "-o", "BatchMode=yes"]

# === LOGOLÃS ===
class Logger:
    """LogolÃ³ osztÃ¡ly"""
    
    @staticmethod
    def log(level: str, message: str):
        timestamp = datetime.now().strftime("%H:%M:%S")
        level_icons = {
            "INFO": "â„¹ï¸",
            "SUCCESS": "âœ…",
            "ERROR": "âŒ",
            "WARNING": "âš ï¸"
        }
        icon = level_icons.get(level, "")
        print(f"[{timestamp}] {icon} {message}")
    
    @staticmethod
    def info(message: str):
        Logger.log("INFO", message)
    
    @staticmethod
    def success(message: str):
        Logger.log("SUCCESS", message)
    
    @staticmethod
    def error(message: str):
        Logger.log("ERROR", message)
    
    @staticmethod
    def warning(message: str):
        Logger.log("WARNING", message)

# === FÅ OSZTÃLY ===
class RsyncUploadTester:
    """RSYNC feltÃ¶ltÃ©s tesztelÅ‘"""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = Logger()
        self.test_files: List[Path] = []
        
        # Kimeneti kÃ¶nyvtÃ¡r lÃ©trehozÃ¡sa
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        # JogosultsÃ¡gok beÃ¡llÃ­tÃ¡sa
        os.chmod(OUTPUT_DIR, stat.S_IRWXU | stat.S_IRWXG | stat.S_IROTH | stat.S_IXOTH)
    
    def run_command(self, cmd: List[str], check: bool = True, 
                    capture: bool = False, shell: bool = False) -> Tuple[int, str, str]:
        """Parancs futtatÃ¡sa"""
        try:
            self.logger.info(f"FuttatÃ¡s: {' '.join(cmd) if not shell else cmd}")
            
            if shell and isinstance(cmd, list):
                cmd = " ".join(cmd)
            
            result = subprocess.run(
                cmd, 
                check=check, 
                capture_output=capture,
                text=True,
                shell=shell
            )
            return (
                result.returncode,
                result.stdout if capture else "",
                result.stderr if capture else ""
            )
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Parancs hibÃ¡san fejezÅ‘dÃ¶tt be: {e}")
            if capture:
                return (e.returncode, e.stdout, e.stderr)
            if check:
                raise
            return (e.returncode, "", str(e))
        except Exception as e:
            self.logger.error(f"Parancs futtatÃ¡si hiba: {e}")
            if check:
                raise
            return (1, "", str(e))
    
    def ssh_command(self, remote_cmd: str, check: bool = True) -> Tuple[int, str, str]:
        """SSH parancs futtatÃ¡sa"""
        full_cmd = self.config.ssh_cmd + [
            f"{self.config.vps_user}@{self.config.vps_host}",
            remote_cmd
        ]
        return self.run_command(full_cmd, check=check, capture=True)
    
    def test_ssh_connection(self) -> bool:
        """SSH kapcsolat tesztelÃ©se"""
        self.logger.info("1. SSH kapcsolat teszt...")
        
        try:
            returncode, stdout, stderr = self.ssh_command("echo 'SSH OK' && hostname")
            if returncode == 0:
                self.logger.success(f"SSH kapcsolat rendben - {stdout.strip()}")
                return True
            else:
                self.logger.error(f"SSH kapcsolat sikertelen: {stderr}")
                return False
        except Exception as e:
            self.logger.error(f"SSH kapcsolat hiba: {e}")
            return False
    
    def test_remote_directory(self) -> bool:
        """TÃ¡voli kÃ¶nyvtÃ¡r ellenÅ‘rzÃ©se"""
        self.logger.info("2. TÃ¡voli kÃ¶nyvtÃ¡r ellenÅ‘rzÃ©se...")
        
        remote_dir = self.config.remote_dir
        returncode, stdout, stderr = self.ssh_command(
            f"if [ -d '{remote_dir}' ]; then "
            f"echo 'KÃ¶nyvtÃ¡r lÃ©tezik' && ls -ld '{remote_dir}'; "
            f"else echo 'KÃ¶nyvtÃ¡r nem lÃ©tezik, lÃ©trehozom...' && "
            f"sudo mkdir -p '{remote_dir}' && sudo chmod 755 '{remote_dir}'; fi"
        )
        
        if returncode == 0:
            self.logger.success(f"KÃ¶nyvtÃ¡r rendben: {stdout.splitlines()[0] if stdout else 'OK'}")
            return True
        else:
            self.logger.error(f"KÃ¶nyvtÃ¡r problÃ©ma: {stderr}")
            return False
    
    def create_dummy_file(self, path: Path, size_mb: int) -> bool:
        """Dummy fÃ¡jl lÃ©trehozÃ¡sa"""
        try:
            # MB-ban megadott mÃ©ret byte-okra konvertÃ¡lÃ¡sa
            size_bytes = size_mb * 1024 * 1024
            
            # VÃ©letlenszerÅ± adatokkal feltÃ¶ltÃ©s
            with open(path, 'wb') as f:
                # 1MB-os blokkokban Ã­runk a hatÃ©konysÃ¡g Ã©rdekÃ©ben
                block_size = 1024 * 1024  # 1MB
                blocks = size_mb
                remaining = size_bytes % block_size
                
                for i in range(blocks):
                    f.write(os.urandom(block_size))
                
                if remaining > 0:
                    f.write(os.urandom(remaining))
            
            # JogosultsÃ¡gok beÃ¡llÃ­tÃ¡sa
            os.chmod(path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)
            return True
            
        except Exception as e:
            self.logger.error(f"Hiba a fÃ¡jl lÃ©trehozÃ¡sakor {path}: {e}")
            return False
    
    def create_test_files(self) -> bool:
        """TesztfÃ¡jlok lÃ©trehozÃ¡sa"""
        self.logger.info("3. TesztfÃ¡jlok lÃ©trehozÃ¡sa...")
        
        try:
            # TÃ¶rÃ¶ljÃ¼k a rÃ©gi fÃ¡jlokat
            for f in OUTPUT_DIR.glob("*"):
                try:
                    f.unlink()
                except:
                    pass
            
            # FÃ¡jlmÃ©retek - VALÃ“DI PKG NEVEKKEL
            file_specs = [
                ("awesome-git-4.0.r123.gabc123def-1-x86_64.pkg.tar.zst", 5),
                ("nvidia-driver-470.199.02-1-x86_64.pkg.tar.zst", 190),
                (f"custom-package-1.0.{self.config.test_size_mb}-1-x86_64.pkg.tar.zst", self.config.test_size_mb),
            ]
            
            # FÃ¡jlok lÃ©trehozÃ¡sa
            for filename, size_mb in file_specs:
                self.logger.info(f"  - {filename} ({size_mb}MB)...")
                filepath = OUTPUT_DIR / filename
                
                if self.create_dummy_file(filepath, size_mb):
                    self.test_files.append(filepath)
                else:
                    self.logger.error(f"Nem sikerÃ¼lt lÃ©trehozni: {filename}")
                    return False
            
            # AdatbÃ¡zis fÃ¡jl lÃ©trehozÃ¡sa (tar.gz)
            self.logger.info("  - AdatbÃ¡zis fÃ¡jl...")
            db_filename = OUTPUT_DIR / "test-repo.db.tar.gz"
            
            try:
                import gzip
                import io
                
                # EgyszerÅ± tar.gz fÃ¡jl lÃ©trehozÃ¡sa
                with tarfile.open(db_filename, "w:gz") as tar:
                    for test_file in self.test_files:
                        tar.add(test_file, arcname=test_file.name)
                
                self.test_files.append(db_filename)
                
            except Exception as e:
                self.logger.warning(f"AdatbÃ¡zis fÃ¡jl lÃ©trehozÃ¡sa nem sikerÃ¼lt: {e}")
                # LÃ©trehozunk egy Ã¼res adatbÃ¡zis fÃ¡jlt
                with open(db_filename, 'wb') as f:
                    f.write(b"dummy repo database")
                self.test_files.append(db_filename)
            
            # FÃ¡jlinformÃ¡ciÃ³k
            self.logger.info("FÃ¡jlok elkÃ©szÃ¼ltek:")
            total_size = 0
            for f in self.test_files:
                size = f.stat().st_size
                size_mb = size / (1024 * 1024)
                total_size += size_mb
                self.logger.info(f"    {f.name} - {size_mb:.1f}MB")
            
            self.logger.info(f"    Ã–sszesen: {total_size:.1f}MB")
            return True
            
        except Exception as e:
            self.logger.error(f"FÃ¡jl lÃ©trehozÃ¡si hiba: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_rsync_upload(self) -> bool:
        """RSYNC feltÃ¶ltÃ©s futtatÃ¡sa"""
        self.logger.info("4. RSYNC feltÃ¶ltÃ©s indÃ­tÃ¡sa...")
        self.logger.info(f"  ForrÃ¡s: {OUTPUT_DIR}/")
        self.logger.info(f"  CÃ©l: {self.config.vps_user}@{self.config.vps_host}:{self.config.remote_dir}/")
        
        # EllenÅ‘rizzÃ¼k, vannak-e fÃ¡jlok
        if not self.test_files:
            self.logger.error("Nincsenek feltÃ¶lthetÅ‘ fÃ¡jlok!")
            return False
        
        # GyÅ±jtsÃ¼k Ã¶ssze a fÃ¡jlokat
        file_patterns = [
            str(OUTPUT_DIR / "*.pkg.tar.zst"),
            str(OUTPUT_DIR / "*.db.tar.gz")
        ]
        
        # Shell glob hasznÃ¡lata a fÃ¡jlok keresÃ©sÃ©re
        import glob
        files_to_upload = []
        for pattern in file_patterns:
            files_to_upload.extend(glob.glob(pattern))
        
        if not files_to_upload:
            self.logger.error("Nem talÃ¡lhatÃ³k fÃ¡jlok a glob pattern alapjÃ¡n!")
            self.logger.info(f"Glob pattern: {file_patterns}")
            self.logger.info(f"OUTPUT_DIR tartalma: {list(OUTPUT_DIR.iterdir())}")
            return False
        
        self.logger.info(f"  FeltÃ¶ltendÅ‘ fÃ¡jlok ({len(files_to_upload)} db):")
        for f in files_to_upload:
            size_mb = os.path.getsize(f) / (1024 * 1024)
            self.logger.info(f"    - {os.path.basename(f)} ({size_mb:.1f}MB)")
        
        # RSYNC parancs Ã¶sszeÃ¡llÃ­tÃ¡sa - SHELL MODBAN!
        rsync_cmd = f"""
        rsync -avz \
          --progress \
          --stats \
          --chmod=0644 \
          -e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=30 -o BatchMode=yes" \
          {" ".join(f"'{f}'" for f in files_to_upload)} \
          '{self.config.vps_user}@{self.config.vps_host}:{self.config.remote_dir}/'
        """
        
        start_time = time.time()
        
        try:
            self.logger.info("RSYNC futtatÃ¡sa...")
            
            # RSYNC futtatÃ¡sa shell mÃ³dban
            returncode, stdout, stderr = self.run_command(
                rsync_cmd,
                check=False,
                capture=True,
                shell=True
            )
            
            # Kimenet kiÃ­rÃ¡sa
            if stdout:
                for line in stdout.splitlines():
                    if line.strip():
                        print(f"    {line}")
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            if returncode == 0:
                self.logger.success(f"RSYNC sikeres! ({duration} mÃ¡sodperc)")
                
                # StatisztikÃ¡k kinyerÃ©se
                if "sent" in stdout.lower():
                    for line in stdout.splitlines():
                        if "sent" in line.lower() and "received" in line.lower():
                            self.logger.info(f"    Ãtvitel: {line.strip()}")
                
                # FÃ¡jlok ellenÅ‘rzÃ©se
                self.verify_remote_files()
                return True
            else:
                self.logger.error(f"RSYNC sikertelen! (return code: {returncode})")
                if stderr:
                    self.logger.error(f"RSYNC hiba: {stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"RSYNC futtatÃ¡si hiba: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def verify_remote_files(self):
        """TÃ¡voli fÃ¡jlok ellenÅ‘rzÃ©se"""
        self.logger.info("5. FÃ¡jlok ellenÅ‘rzÃ©se a szerveren...")
        
        remote_cmd = f"""
        echo "=== SZERVER FÃJLOK ==="
        ls -la "{self.config.remote_dir}/" 2>/dev/null | head -20
        echo ""
        echo "=== PKG FÃJLOK ==="
        ls -lh "{self.config.remote_dir}/"*.pkg.tar.* 2>/dev/null || echo "Nincsenek .pkg.tar fÃ¡jlok"
        echo ""
        echo "=== DB FÃJL ==="
        ls -lh "{self.config.remote_dir}/"*.db.tar.gz 2>/dev/null || echo "Nincs .db.tar.gz fÃ¡jl"
        echo ""
        echo "=== HELY FOGYASZTÃS ==="
        du -sh "{self.config.remote_dir}/" 2>/dev/null || echo "Nem elÃ©rhetÅ‘"
        """
        
        returncode, stdout, stderr = self.ssh_command(remote_cmd, check=False)
        
        if returncode == 0 and stdout:
            for line in stdout.splitlines():
                if line.strip():
                    print(f"    {line}")
        elif stderr:
            self.logger.warning(f"EllenÅ‘rzÃ©s hibÃ¡ja: {stderr}")
    
    def cleanup(self):
        """TakarÃ­tÃ¡s"""
        self.logger.info("6. TakarÃ­tÃ¡s...")
        
        # LokÃ¡lis fÃ¡jlok tÃ¶rlÃ©se
        try:
            # TÃ¶rÃ¶ljÃ¼k a teljes OUTPUT_DIR tartalmÃ¡t
            for item in OUTPUT_DIR.iterdir():
                try:
                    if item.is_file():
                        item.unlink()
                    elif item.is_dir():
                        shutil.rmtree(item)
                except Exception as e:
                    self.logger.warning(f"Nem sikerÃ¼lt tÃ¶rÃ¶lni {item}: {e}")
            
            self.logger.success("LokÃ¡lis fÃ¡jlok tÃ¶rÃ¶lve")
        except Exception as e:
            self.logger.error(f"LokÃ¡lis tÃ¶rlÃ©s hiba: {e}")
        
        # TÃ¡voli tesztfÃ¡jlok tÃ¶rlÃ©se
        try:
            # Csak a mai tesztfÃ¡jlokat tÃ¶rÃ¶ljÃ¼k
            remote_cmd = f"""
            echo "TÃ¡voli tesztfÃ¡jlok tÃ¶rlÃ©se..."
            # TÃ¶rÃ¶ljÃ¼k az Ã¶sszes .pkg.tar.zst fÃ¡jlt
            rm -f "{self.config.remote_dir}/"*.pkg.tar.zst 2>/dev/null
            # TÃ¶rÃ¶ljÃ¼k az Ã¶sszes .db.tar.gz fÃ¡jlt
            rm -f "{self.config.remote_dir}/"*.db.tar.gz 2>/dev/null
            echo "âœ… TÃ¡voli tesztfÃ¡jlok tÃ¶rÃ¶lve"
            """
            
            returncode, stdout, stderr = self.ssh_command(remote_cmd, check=False)
            if returncode == 0:
                if stdout:
                    self.logger.success(stdout.splitlines()[-1] if stdout else "TÃ¶rÃ¶lve")
            else:
                self.logger.warning(f"TÃ¡voli tÃ¶rlÃ©s figyelmeztetÃ©s: {stderr}")
        except Exception as e:
            self.logger.warning(f"TÃ¡voli tÃ¶rlÃ©s hiba: {e}")
    
    def run(self) -> bool:
        """FÅ‘ teszt futtatÃ¡sa"""
        self.logger.info("=== RSYNC FELTÃ–LTÃ‰S TESZT (Python) ===")
        self.logger.info(f"Host: {self.config.vps_host}")
        self.logger.info(f"User: {self.config.vps_user}")
        self.logger.info(f"Remote: {self.config.remote_dir}")
        self.logger.info(f"File size: {self.config.test_size_mb}MB")
        print()
        
        # LÃ©pÃ©sek
        steps = [
            ("SSH kapcsolat", self.test_ssh_connection),
            ("KÃ¶nyvtÃ¡r ellenÅ‘rzÃ©s", self.test_remote_directory),
            ("FÃ¡jlok lÃ©trehozÃ¡sa", self.create_test_files),
        ]
        
        success = True
        for step_name, step_func in steps:
            if not step_func():
                self.logger.error(f"{step_name} sikertelen!")
                success = False
                break
        
        # RSYNC feltÃ¶ltÃ©s csak ha minden elÅ‘zÅ‘ lÃ©pÃ©s sikeres
        rsync_success = False
        if success:
            rsync_success = self.run_rsync_upload()
        
        # TakarÃ­tÃ¡s mindig
        self.cleanup()
        
        # Ã–sszefoglalÃ³
        self.print_summary(success and rsync_success)
        
        return success and rsync_success
    
    def print_summary(self, overall_success: bool):
        """Ã–sszefoglalÃ³ kiÃ­rÃ¡sa"""
        print()
        print("=" * 50)
        self.logger.info("=== TESZT VÃ‰GE ===")
        print()
        
        if overall_success:
            self.logger.success("ðŸŽ‰ RSYNC MÅ°KÃ–DIK!")
            print()
            print("âœ… Az eredeti CI script RSYNC-re Ã¡tÃ­rhatÃ³.")
            print()
            print("ðŸ“‹ Javasolt RSYNC konfigurÃ¡ciÃ³ az eredeti CI-hez:")
            print()
            print('''
            # Az eredeti scriptben cserÃ©ld le az scp rÃ©szt:
            
            # RÃ‰GI (SCP):
            # scp $SSH_OPTS $OUTPUT_DIR/* $VPS_USER@$VPS_HOST:$REMOTE_DIR/
            
            # ÃšJ (RSYNC):
            log_info "FÃ¡jlok feltÃ¶ltÃ©se RSYNC-kel..."
            rsync -avz \\
                --progress \\
                --stats \\
                --chmod=0644 \\
                -e "ssh $SSH_OPTS" \\
                "$OUTPUT_DIR/"*.pkg.tar.* \\
                "$VPS_USER@$VPS_HOST:$REMOTE_DIR/"
            ''')
        else:
            self.logger.error("RSYNC SIKERTELEN")
            print()
            print("ðŸ”§ HibaelhÃ¡rÃ­tÃ¡s:")
            print("1. EllenÅ‘rizd az SSH kulcs jogosultsÃ¡gokat")
            print("2. EllenÅ‘rizd a tÃ¡voli kÃ¶nyvtÃ¡r Ã­rÃ¡si jogosultsÃ¡gait")
            print("3. EllenÅ‘rizd a tÅ±zfal beÃ¡llÃ­tÃ¡sokat (port 22)")
            print("4. EllenÅ‘rizd, hogy a szerver elÃ©rhetÅ‘-e a kontÃ©nerbÅ‘l")
            print("5. SSH kapcsolat tesztelÃ©se kÃ©zzel:")
            print(f"   ssh -i /home/builder/.ssh/id_ed25519 {self.config.vps_user}@{self.config.vps_host}")
        
        print()
        print(f"ðŸ•’ Teszt idÅ‘pont: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 50)

# === FÅ PROGRAM ===
def main():
    """FÅ‘ program"""
    try:
        # KonfigurÃ¡ciÃ³ betÃ¶ltÃ©se
        config = Config()
        
        # TesztelÅ‘ lÃ©trehozÃ¡sa Ã©s futtatÃ¡sa
        tester = RsyncUploadTester(config)
        success = tester.run()
        
        # KilÃ©pÃ©si kÃ³d
        sys.exit(0 if success else 1)
        
    except ValueError as e:
        Logger.error(f"KonfigurÃ¡ciÃ³s hiba: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        Logger.info("Teszt megszakÃ­tva")
        sys.exit(130)
    except Exception as e:
        Logger.error(f"VÃ¡ratlan hiba: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()