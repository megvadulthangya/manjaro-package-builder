
--- FILE: .github/scripts/modules/build/__init__.py ---
"""
Build module for package building operations
"""
--- FILE: .github/scripts/modules/build/build_tracker.py ---
"""
Build Tracker Module - Tracks build progress and statistics
"""

import time
from typing import Dict, List


class BuildTracker:
    """Tracks build progress, statistics, and package information"""
    
    def __init__(self):
        # State
        self.hokibot_data = []
        self.rebuilt_local_packages = []
        self.skipped_packages = []
        self.built_packages = []
        
        # Statistics
        self.stats = {
            "aur_success": 0,
            "local_success": 0,
            "aur_failed": 0,
            "local_failed": 0,
        }
        
        # Start time
        self.start_time = time.time()
    
    def add_hokibot_data(self, pkg_name: str, pkgver: str, pkgrel: str, epoch: str = None):
        """Add package metadata for hokibot tracking"""
        self.hokibot_data.append({
            'name': pkg_name,
            'built_version': f"{epoch or '0'}:{pkgver}-{pkgrel}" if epoch and epoch != '0' else f"{pkgver}-{pkgrel}",
            'pkgver': pkgver,
            'pkgrel': pkgrel,
            'epoch': epoch
        })
    
    def record_built_package(self, pkg_name: str, version: str, is_aur: bool = False):
        """Record a successfully built package"""
        self.built_packages.append(f"{pkg_name} ({version})")
        if is_aur:
            self.stats["aur_success"] += 1
        else:
            self.stats["local_success"] += 1
    
    def record_failed_package(self, is_aur: bool = False):
        """Record a failed package build"""
        if is_aur:
            self.stats["aur_failed"] += 1
        else:
            self.stats["local_failed"] += 1
    
    def record_skipped_package(self, pkg_name: str, version: str):
        """Record a skipped package (already up-to-date)"""
        self.skipped_packages.append(f"{pkg_name} ({version})")
    
    def get_elapsed_time(self) -> float:
        """Get elapsed time since tracking started"""
        return time.time() - self.start_time
    
    def get_summary(self) -> Dict:
        """Get build summary statistics"""
        return {
            "elapsed": self.get_elapsed_time(),
            **self.stats,
            "total_built": self.stats["aur_success"] + self.stats["local_success"],
            "skipped": len(self.skipped_packages),
            "hokibot_entries": len(self.hokibot_data)
        }
--- FILE: .github/scripts/modules/build/local_builder.py ---
"""
Local Builder Module - Handles local package building logic
"""

import subprocess
import logging

logger = logging.getLogger(__name__)


class LocalBuilder:
    """Handles local package building operations"""
    
    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
    
    def run_makepkg(self, pkg_dir: str, packager_id: str, flags: str = "-si --noconfirm --clean", timeout: int = 3600) -> subprocess.CompletedProcess:
        """Run makepkg command with specified flags"""
        cmd = f"makepkg {flags}"
        
        import os
        extra_env = {"PACKAGER": packager_id}
        
        if self.debug_mode:
            print(f"üîß [DEBUG] Running makepkg in {pkg_dir}: {cmd}", flush=True)
        
        try:
            result = subprocess.run(
                cmd,
                cwd=pkg_dir,
                shell=True,
                capture_output=True,
                text=True,
                check=False,
                timeout=timeout,
                env={**os.environ, **extra_env}
            )
            
            if self.debug_mode:
                if result.stdout:
                    print(f"üîß [DEBUG] MAKEPKG STDOUT:\n{result.stdout}", flush=True)
                if result.stderr:
                    print(f"üîß [DEBUG] MAKEPKG STDERR:\n{result.stderr}", flush=True)
                print(f"üîß [DEBUG] MAKEPKG EXIT CODE: {result.returncode}", flush=True)
            
            return result
        except subprocess.TimeoutExpired as e:
            logger.error(f"makepkg timed out after {timeout} seconds")
            raise
        except Exception as e:
            logger.error(f"Error running makepkg: {e}")
            raise
--- FILE: .github/scripts/modules/build/version_manager.py ---
"""
Version Manager Module - Handles version extraction, comparison, and management
"""

import os
import subprocess
import logging
from pathlib import Path
from typing import Tuple, Optional

logger = logging.getLogger(__name__)


class VersionManager:
    """Handles package version extraction, comparison, and management"""
    
    def extract_version_from_srcinfo(self, pkg_dir: Path) -> Tuple[str, str, Optional[str]]:
        """Extract pkgver, pkgrel, and epoch from .SRCINFO or makepkg --printsrcinfo output"""
        srcinfo_path = pkg_dir / ".SRCINFO"
        
        # First try to read existing .SRCINFO
        if srcinfo_path.exists():
            try:
                with open(srcinfo_path, 'r') as f:
                    srcinfo_content = f.read()
                return self._parse_srcinfo_content(srcinfo_content)
            except Exception as e:
                logger.warning(f"Failed to parse existing .SRCINFO: {e}")
        
        # Generate .SRCINFO using makepkg --printsrcinfo
        try:
            result = subprocess.run(
                ['makepkg', '--printsrcinfo'],
                cwd=pkg_dir,
                capture_output=True,
                text=True,
                check=False
            )
            
            if result.returncode == 0 and result.stdout:
                # Also write to .SRCINFO for future use
                with open(srcinfo_path, 'w') as f:
                    f.write(result.stdout)
                return self._parse_srcinfo_content(result.stdout)
            else:
                logger.warning(f"makepkg --printsrcinfo failed: {result.stderr}")
                raise RuntimeError(f"Failed to generate .SRCINFO: {result.stderr}")
                
        except Exception as e:
            logger.error(f"Error running makepkg --printsrcinfo: {e}")
            raise
    
    def _parse_srcinfo_content(self, srcinfo_content: str) -> Tuple[str, str, Optional[str]]:
        """Parse SRCINFO content to extract version information"""
        pkgver = None
        pkgrel = None
        epoch = None
        
        lines = srcinfo_content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Handle key-value pairs
            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip()
                
                if key == 'pkgver':
                    pkgver = value
                elif key == 'pkgrel':
                    pkgrel = value
                elif key == 'epoch':
                    epoch = value
        
        if not pkgver or not pkgrel:
            raise ValueError("Could not extract pkgver and pkgrel from .SRCINFO")
        
        return pkgver, pkgrel, epoch
    
    def get_full_version_string(self, pkgver: str, pkgrel: str, epoch: Optional[str]) -> str:
        """Construct full version string from components"""
        if epoch and epoch != '0':
            return f"{epoch}:{pkgver}-{pkgrel}"
        return f"{pkgver}-{pkgrel}"
    
    def compare_versions(self, remote_version: Optional[str], pkgver: str, pkgrel: str, epoch: Optional[str]) -> bool:
        """
        Compare versions using vercmp-style logic
        
        Returns:
            True if AUR_VERSION > REMOTE_VERSION (should build), False otherwise
        """
        # If no remote version exists, we should build
        if not remote_version:
            logger.info(f"[DEBUG] Comparing Package: Remote(NONE) vs New({pkgver}-{pkgrel}) -> BUILD TRIGGERED (no remote)")
            return True
        
        # Parse remote version
        remote_epoch = None
        remote_pkgver = None
        remote_pkgrel = None
        
        # Check if remote has epoch
        if ':' in remote_version:
            remote_epoch_str, rest = remote_version.split(':', 1)
            remote_epoch = remote_epoch_str
            if '-' in rest:
                remote_pkgver, remote_pkgrel = rest.split('-', 1)
            else:
                remote_pkgver = rest
                remote_pkgrel = "1"
        else:
            if '-' in remote_version:
                remote_pkgver, remote_pkgrel = remote_version.split('-', 1)
            else:
                remote_pkgver = remote_version
                remote_pkgrel = "1"
        
        # Build version strings for comparison
        new_version_str = f"{epoch or '0'}:{pkgver}-{pkgrel}"
        remote_version_str = f"{remote_epoch or '0'}:{remote_pkgver}-{remote_pkgrel}"
        
        # Use vercmp for proper version comparison
        try:
            result = subprocess.run(['vercmp', new_version_str, remote_version_str], 
                                  capture_output=True, text=True, check=False)
            if result.returncode == 0:
                cmp_result = int(result.stdout.strip())
                
                if cmp_result > 0:
                    logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({pkgver}-{pkgrel}) -> BUILD TRIGGERED (new version is newer)")
                    return True
                elif cmp_result == 0:
                    logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({pkgver}-{pkgrel}) -> SKIP (versions identical)")
                    return False
                else:
                    logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({pkgver}-{pkgrel}) -> SKIP (remote version is newer)")
                    return False
            else:
                # Fallback to simple comparison if vercmp fails
                logger.warning("vercmp failed, using fallback comparison")
                return self._fallback_version_comparison(remote_version, pkgver, pkgrel, epoch)
                
        except Exception as e:
            logger.warning(f"vercmp comparison failed: {e}, using fallback")
            return self._fallback_version_comparison(remote_version, pkgver, pkgrel, epoch)
    
    def _fallback_version_comparison(self, remote_version: str, pkgver: str, pkgrel: str, epoch: Optional[str]) -> bool:
        """Fallback version comparison when vercmp is not available"""
        # Parse remote version
        remote_epoch = None
        remote_pkgver = None
        remote_pkgrel = None
        
        if ':' in remote_version:
            remote_epoch_str, rest = remote_version.split(':', 1)
            remote_epoch = remote_epoch_str
            if '-' in rest:
                remote_pkgver, remote_pkgrel = rest.split('-', 1)
            else:
                remote_pkgver = rest
                remote_pkgrel = "1"
        else:
            if '-' in remote_version:
                remote_pkgver, remote_pkgrel = remote_version.split('-', 1)
            else:
                remote_pkgver = remote_version
                remote_pkgrel = "1"
        
        # Compare epochs first
        if epoch != remote_epoch:
            try:
                epoch_int = int(epoch or 0)
                remote_epoch_int = int(remote_epoch or 0)
                if epoch_int > remote_epoch_int:
                    logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> BUILD TRIGGERED (epoch {epoch_int} > {remote_epoch_int})")
                    return True
                else:
                    logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (epoch {epoch_int} <= {remote_epoch_int})")
                    return False
            except ValueError:
                if epoch != remote_epoch:
                    logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (epoch string mismatch)")
                    return False
        
        # Compare pkgver
        if pkgver != remote_pkgver:
            logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> BUILD TRIGGERED (pkgver different)")
            return True
        
        # Compare pkgrel
        try:
            remote_pkgrel_int = int(remote_pkgrel)
            pkgrel_int = int(pkgrel)
            if pkgrel_int > remote_pkgrel_int:
                logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> BUILD TRIGGERED (pkgrel {pkgrel_int} > {remote_pkgrel_int})")
                return True
            else:
                logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (pkgrel {pkgrel_int} <= {remote_pkgrel_int})")
                return False
        except ValueError:
            if pkgrel != remote_pkgrel:
                logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (pkgrel string mismatch)")
                return False
        
        # Versions are identical
        logger.info(f"[DEBUG] Comparing Package: Remote({remote_version}) vs New({epoch or ''}{pkgver}-{pkgrel}) -> SKIP (versions identical)")
        return False
--- FILE: .github/scripts/modules/build/artifact_manager.py ---
"""
Artifact Manager Module - Handles package file management and cleanup
"""

import shutil
import tarfile
import logging
from pathlib import Path
from datetime import datetime

logger = logging.getLogger(__name__)


class ArtifactManager:
    """Handles package file management and workspace cleanup"""
    
    def clean_workspace(self, pkg_dir: Path):
        """Clean workspace before building to avoid contamination"""
        logger.info(f"üßπ Cleaning workspace for {pkg_dir.name}...")
        
        # Clean src/ directory if exists
        src_dir = pkg_dir / "src"
        if src_dir.exists():
            try:
                shutil.rmtree(src_dir, ignore_errors=True)
                logger.info(f"  Cleaned src/ directory")
            except Exception as e:
                logger.warning(f"  Could not clean src/: {e}")
        
        # Clean pkg/ directory if exists
        pkg_build_dir = pkg_dir / "pkg"
        if pkg_build_dir.exists():
            try:
                shutil.rmtree(pkg_build_dir, ignore_errors=True)
                logger.info(f"  Cleaned pkg/ directory")
            except Exception as e:
                logger.warning(f"  Could not clean pkg/: {e}")
        
        # Clean any leftover .tar.* files
        for leftover in pkg_dir.glob("*.pkg.tar.*"):
            try:
                leftover.unlink()
                logger.info(f"  Removed leftover package: {leftover.name}")
            except Exception as e:
                logger.warning(f"  Could not remove {leftover}: {e}")

    def create_artifact_archive(self, built_packages_path: Path, log_path: Path) -> Path:
        """
        Create a .tar.gz archive of built packages and logs to avoid colon (:) characters
        in filenames during GitHub upload.
        
        Args:
            built_packages_path: Path to directory containing built packages
            log_path: Path to log file
            
        Returns:
            Path to created archive file
        """
        logger.info("üì¶ Creating artifact archive to avoid colon character issues...")
        
        # Generate timestamp for archive name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"artifacts_{timestamp}.tar.gz"
        archive_path = built_packages_path.parent / archive_name
        
        try:
            with tarfile.open(archive_path, "w:gz") as tar:
                # Add all built package files
                for pkg_file in built_packages_path.glob("*.pkg.tar.*"):
                    # Sanitize filename for tar (remove colon characters)
                    sanitized_name = pkg_file.name.replace(":", "_")
                    arcname = f"packages/{sanitized_name}"
                    tar.add(pkg_file, arcname=arcname)
                    logger.debug(f"Added to archive: {pkg_file.name} as {sanitized_name}")
                
                # Add log file if it exists
                if log_path.exists():
                    arcname = f"logs/{log_path.name}"
                    tar.add(log_path, arcname=arcname)
                    logger.debug(f"Added to archive: {log_path.name}")
                
                # Add repository database files if they exist
                for db_file in built_packages_path.glob("*.db*"):
                    arcname = f"databases/{db_file.name}"
                    tar.add(db_file, arcname=arcname)
                    logger.debug(f"Added to archive: {db_file.name}")
                
                for files_db in built_packages_path.glob("*.files*"):
                    arcname = f"databases/{files_db.name}"
                    tar.add(files_db, arcname=arcname)
                    logger.debug(f"Added to archive: {files_db.name}")
                
                # Add GPG signatures if they exist
                for sig_file in built_packages_path.glob("*.sig"):
                    arcname = f"signatures/{sig_file.name}"
                    tar.add(sig_file, arcname=arcname)
                    logger.debug(f"Added to archive: {sig_file.name}")
            
            # Verify archive was created
            if archive_path.exists():
                size_mb = archive_path.stat().st_size / (1024 * 1024)
                logger.info(f"‚úÖ Created artifact archive: {archive_path.name} ({size_mb:.2f} MB)")
                
                # List contents for verification
                with tarfile.open(archive_path, "r:gz") as tar:
                    members = tar.getmembers()
                    logger.info(f"Archive contains {len(members)} files")
                    
                    # Group files by type for summary
                    packages = [m for m in members if m.name.startswith("packages/")]
                    logs = [m for m in members if m.name.startswith("logs/")]
                    databases = [m for m in members if m.name.startswith("databases/")]
                    signatures = [m for m in members if m.name.startswith("signatures/")]
                    
                    logger.info(f"  Packages: {len(packages)} files")
                    logger.info(f"  Logs: {len(logs)} files")
                    logger.info(f"  Databases: {len(databases)} files")
                    logger.info(f"  Signatures: {len(signatures)} files")
                
                # Clean up original files with colons after archiving
                self._cleanup_colon_files(built_packages_path)
                
                return archive_path
            else:
                logger.error("‚ùå Failed to create artifact archive")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Error creating artifact archive: {e}")
            # Clean up partial archive if it exists
            if archive_path.exists():
                archive_path.unlink(missing_ok=True)
            return None
    
    def _cleanup_colon_files(self, directory: Path):
        """Remove files with colon characters after they've been archived"""
        logger.info("üßπ Cleaning up files with colon characters...")
        
        removed_count = 0
        for file_path in directory.glob("*.pkg.tar.*"):
            if ":" in file_path.name:
                try:
                    file_path.unlink(missing_ok=True)
                    removed_count += 1
                    logger.debug(f"Removed file with colon: {file_path.name}")
                except Exception as e:
                    logger.warning(f"Could not remove {file_path}: {e}")
        
        if removed_count > 0:
            logger.info(f"‚úÖ Removed {removed_count} files with colon characters")
--- FILE: .github/scripts/modules/build/aur_builder.py ---
"""
AUR Builder Module - Handles AUR package building logic
"""

import re
import subprocess
import logging
from pathlib import Path
from typing import List

logger = logging.getLogger(__name__)


class AURBuilder:
    """Handles AUR package building and dependency resolution"""
    
    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
    
    def install_dependencies_strict(self, deps: List[str]) -> bool:
        """STRICT dependency resolution: pacman first, then yay"""
        if not deps:
            return True
        
        print(f"\nInstalling {len(deps)} dependencies...")
        logger.info(f"Dependencies to install: {deps}")
        
        # CRITICAL FIX: Update pacman-key database first
        print("üîÑ Updating pacman-key database...")
        cmd = "sudo pacman-key --updatedb"
        result = self._run_cmd(cmd, log_cmd=True, check=False, timeout=300)
        if result.returncode != 0:
            logger.warning(f"‚ö†Ô∏è pacman-key --updatedb warning: {result.stderr[:200]}")
        
        # Clean dependency names
        clean_deps = []
        phantom_packages = set()
        
        for dep in deps:
            dep_clean = re.sub(r'[<=>].*', '', dep).strip()
            if dep_clean and dep_clean.strip() and not any(x in dep_clean for x in ['$', '{', '}', '(', ')', '[', ']']):
                if re.search(r'[a-zA-Z0-9]', dep_clean):
                    # FIX: Hard-filter out phantom package 'lgi'
                    if dep_clean == 'lgi':
                        phantom_packages.add('lgi')
                        logger.warning(f"‚ö†Ô∏è Found phantom package 'lgi' - will be replaced with 'lua-lgi'")
                        continue
                    clean_deps.append(dep_clean)
        
        # Remove any duplicate entries
        clean_deps = list(dict.fromkeys(clean_deps))
        
        # FIX: If we removed 'lgi', ensure 'lua-lgi' is present
        if 'lgi' in phantom_packages and 'lua-lgi' not in clean_deps:
            logger.info("Adding 'lua-lgi' to replace phantom package 'lgi'")
            clean_deps.append('lua-lgi')
        
        if not clean_deps:
            logger.info("No valid dependencies to install after cleaning")
            return True
        
        logger.info(f"Valid dependencies to install: {clean_deps}")
        if phantom_packages:
            logger.info(f"Phantom packages removed: {', '.join(phantom_packages)}")
        
        # CRITICAL FIX: Use Syy instead of Sy to force refresh
        deps_str = ' '.join(clean_deps)
        cmd = f"sudo LC_ALL=C pacman -Syy --needed --noconfirm {deps_str}"
        result = self._run_cmd(cmd, log_cmd=True, check=False, timeout=1200)
        
        if result.returncode == 0:
            logger.info("‚úÖ All dependencies installed via pacman")
            return True
        
        logger.warning(f"‚ö†Ô∏è pacman failed for some dependencies (exit code: {result.returncode})")
        
        # Fallback to AUR (yay) WITHOUT sudo - but first sync pacman
        cmd = f"sudo LC_ALL=C pacman -Syy && LC_ALL=C yay -S --needed --noconfirm {deps_str}"
        result = self._run_cmd(cmd, log_cmd=True, check=False, user="builder", timeout=1800)
        
        if result.returncode == 0:
            logger.info("‚úÖ Dependencies installed via yay")
            return True
        
        logger.error(f"‚ùå Both pacman and yay failed for dependencies")
        return False
    
    def _run_cmd(self, cmd, cwd=None, capture=True, check=True, shell=True, user=None, log_cmd=False, timeout=1800):
        """
        Run command with comprehensive logging and timeout.
        
        CRITICAL FIX: When DEBUG_MODE is True, bypass logger and print directly to stdout
        to ensure output appears in CI/CD console.
        """
        if log_cmd:
            if self.debug_mode:
                print(f"üîß [DEBUG] RUNNING COMMAND: {cmd}", flush=True)
            else:
                logger.info(f"RUNNING COMMAND: {cmd}")
        
        if cwd is None:
            cwd = Path.cwd()
        
        if user:
            import os
            env = os.environ.copy()
            env['HOME'] = f'/home/{user}'
            env['USER'] = user
            env['LC_ALL'] = 'C'
            
            try:
                sudo_cmd = ['sudo', '-u', user]
                if shell:
                    sudo_cmd.extend(['bash', '-c', f'cd "{cwd}" && {cmd}'])
                else:
                    sudo_cmd.extend(cmd)
                
                result = subprocess.run(
                    sudo_cmd,
                    capture_output=capture,
                    text=True,
                    check=check,
                    env=env,
                    timeout=timeout
                )
                
                # CRITICAL: When in debug mode, bypass logger and print directly
                if log_cmd or self.debug_mode:
                    if self.debug_mode:
                        if result.stdout:
                            print(f"üîß [DEBUG] STDOUT:\n{result.stdout}", flush=True)
                        if result.stderr:
                            print(f"üîß [DEBUG] STDERR:\n{result.stderr}", flush=True)
                        print(f"üîß [DEBUG] EXIT CODE: {result.returncode}", flush=True)
                    else:
                        if result.stdout:
                            logger.info(f"STDOUT: {result.stdout[:500]}")
                        if result.stderr:
                            logger.info(f"STDERR: {result.stderr[:500]}")
                        logger.info(f"EXIT CODE: {result.returncode}")
                
                # CRITICAL: If command failed and we're in debug mode, print full output
                if result.returncode != 0 and self.debug_mode:
                    print(f"‚ùå [DEBUG] COMMAND FAILED: {cmd}", flush=True)
                    if result.stdout and len(result.stdout) > 500:
                        print(f"‚ùå [DEBUG] FULL STDOUT (truncated):\n{result.stdout[:2000]}", flush=True)
                    if result.stderr and len(result.stderr) > 500:
                        print(f"‚ùå [DEBUG] FULL STDERR (truncated):\n{result.stderr[:2000]}", flush=True)
                
                return result
            except subprocess.TimeoutExpired as e:
                error_msg = f"‚ö†Ô∏è Command timed out after {timeout} seconds: {cmd}"
                if self.debug_mode:
                    print(f"‚ùå [DEBUG] {error_msg}", flush=True)
                logger.error(error_msg)
                raise
            except subprocess.CalledProcessError as e:
                if log_cmd or self.debug_mode:
                    error_msg = f"Command failed: {cmd}"
                    if self.debug_mode:
                        print(f"‚ùå [DEBUG] {error_msg}", flush=True)
                        if hasattr(e, 'stdout') and e.stdout:
                            print(f"‚ùå [DEBUG] EXCEPTION STDOUT:\n{e.stdout}", flush=True)
                        if hasattr(e, 'stderr') and e.stderr:
                            print(f"‚ùå [DEBUG] EXCEPTION STDERR:\n{e.stderr}", flush=True)
                    else:
                        logger.error(error_msg)
                if check:
                    raise
                return e
        else:
            try:
                import os
                env = os.environ.copy()
                env['LC_ALL'] = 'C'
                
                result = subprocess.run(
                    cmd,
                    cwd=cwd,
                    shell=shell,
                    capture_output=capture,
                    text=True,
                    check=check,
                    env=env,
                    timeout=timeout
                )
                
                # CRITICAL: When in debug mode, bypass logger and print directly
                if log_cmd or self.debug_mode:
                    if self.debug_mode:
                        if result.stdout:
                            print(f"üîß [DEBUG] STDOUT:\n{result.stdout}", flush=True)
                        if result.stderr:
                            print(f"üîß [DEBUG] STDERR:\n{result.stderr}", flush=True)
                        print(f"üîß [DEBUG] EXIT CODE: {result.returncode}", flush=True)
                    else:
                        if result.stdout:
                            logger.info(f"STDOUT: {result.stdout[:500]}")
                        if result.stderr:
                            logger.info(f"STDERR: {result.stderr[:500]}")
                        logger.info(f"EXIT CODE: {result.returncode}")
                
                # CRITICAL: If command failed and we're in debug mode, print full output
                if result.returncode != 0 and self.debug_mode:
                    print(f"‚ùå [DEBUG] COMMAND FAILED: {cmd}", flush=True)
                    if result.stdout and len(result.stdout) > 500:
                        print(f"‚ùå [DEBUG] FULL STDOUT (truncated):\n{result.stdout[:2000]}", flush=True)
                    if result.stderr and len(result.stderr) > 500:
                        print(f"‚ùå [DEBUG] FULL STDERR (truncated):\n{result.stderr[:2000]}", flush=True)
                
                return result
            except subprocess.TimeoutExpired as e:
                error_msg = f"‚ö†Ô∏è Command timed out after {timeout} seconds: {cmd}"
                if self.debug_mode:
                    print(f"‚ùå [DEBUG] {error_msg}", flush=True)
                logger.error(error_msg)
                raise
            except subprocess.CalledProcessError as e:
                if log_cmd or self.debug_mode:
                    error_msg = f"Command failed: {cmd}"
                    if self.debug_mode:
                        print(f"‚ùå [DEBUG] {error_msg}", flush=True)
                        if hasattr(e, 'stdout') and e.stdout:
                            print(f"‚ùå [DEBUG] EXCEPTION STDOUT:\n{e.stdout}", flush=True)
                        if hasattr(e, 'stderr') and e.stderr:
                            print(f"‚ùå [DEBUG] EXCEPTION STDERR:\n{e.stderr}", flush=True)
                    else:
                        logger.error(error_msg)
                if check:
                    raise
                return e
--- FILE: .github/scripts/modules/common/__init__.py ---
"""
Common utilities module
"""
--- FILE: .github/scripts/modules/common/config_loader.py ---
"""
Config Loader Module - Handles configuration loading and validation
"""

import os
import sys
from pathlib import Path


class ConfigLoader:
    """Handles configuration loading and validation"""
    
    @staticmethod
    def get_repo_root():
        """Get the repository root directory reliably"""
        github_workspace = os.getenv('GITHUB_WORKSPACE')
        if github_workspace:
            workspace_path = Path(github_workspace)
            if workspace_path.exists():
                return workspace_path
        
        container_workspace = Path('/__w/manjaro-awesome/manjaro-awesome')
        if container_workspace.exists():
            return container_workspace
        
        # Get script directory and go up to repo root
        script_path = Path(__file__).resolve()
        repo_root = script_path.parent.parent.parent.parent
        if repo_root.exists():
            return repo_root
        
        return Path.cwd()
    
    @staticmethod
    def load_environment_config():
        """Load configuration from environment variables"""
        return {
            'vps_user': os.getenv('VPS_USER'),
            'vps_host': os.getenv('VPS_HOST'),
            'ssh_key': os.getenv('VPS_SSH_KEY'),
            'repo_server_url': os.getenv('REPO_SERVER_URL', ''),
            'remote_dir': os.getenv('REMOTE_DIR'),
            'repo_name': os.getenv('REPO_NAME'),
        }
    
    @staticmethod
    def load_from_python_config():
        """Load configuration from config.py if available"""
        try:
            import scripts.config as config_module
            return {
                'output_dir': getattr(config_module, 'OUTPUT_DIR', 'built_packages'),
                'build_tracking_dir': getattr(config_module, 'BUILD_TRACKING_DIR', '.build_tracking'),
                'mirror_temp_dir': getattr(config_module, 'MIRROR_TEMP_DIR', '/tmp/repo_mirror'),
                'sync_clone_dir': getattr(config_module, 'SYNC_CLONE_DIR', '/tmp/manjaro-awesome-gitclone'),
                'aur_urls': getattr(config_module, 'AUR_URLS', ["https://aur.archlinux.org/{pkg_name}.git", "git://aur.archlinux.org/{pkg_name}.git"]),
                'aur_build_dir': getattr(config_module, 'AUR_BUILD_DIR', 'build_aur'),
                'ssh_options': getattr(config_module, 'SSH_OPTIONS', ["-o", "StrictHostKeyChecking=no", "-o", "ConnectTimeout=30", "-o", "BatchMode=yes"]),
                'github_repo': os.getenv('GITHUB_REPO', getattr(config_module, 'GITHUB_REPO', 'megvadulthangya/manjaro-awesome.git')),
                'packager_id': getattr(config_module, 'PACKAGER_ID', 'Maintainer <no-reply@gshoots.hu>'),
                'debug_mode': getattr(config_module, 'DEBUG_MODE', False),
            }
        except ImportError:
            return {
                'output_dir': 'built_packages',
                'build_tracking_dir': '.build_tracking',
                'mirror_temp_dir': '/tmp/repo_mirror',
                'sync_clone_dir': '/tmp/manjaro-awesome-gitclone',
                'aur_urls': ["https://aur.archlinux.org/{pkg_name}.git", "git://aur.archlinux.org/{pkg_name}.git"],
                'aur_build_dir': 'build_aur',
                'ssh_options': ["-o", "StrictHostKeyChecking=no", "-o", "ConnectTimeout=30", "-o", "BatchMode=yes"],
                'github_repo': 'megvadulthangya/manjaro-awesome.git',
                'packager_id': 'Maintainer <no-reply@gshoots.hu>',
                'debug_mode': False,
            }
--- FILE: .github/scripts/modules/common/environment.py ---
"""
Environment Module - Handles environment validation and setup
"""

import os
import sys
import re
import logging

logger = logging.getLogger(__name__)


class EnvironmentValidator:
    """Handles environment validation and setup"""
    
    @staticmethod
    def validate_env() -> None:
        """Comprehensive pre-flight environment validation - check for all required variables"""
        print("\n" + "=" * 60)
        print("PRE-FLIGHT ENVIRONMENT VALIDATION")
        print("=" * 60)
        
        required_vars = [
            'REPO_NAME',
            'VPS_HOST',
            'VPS_USER',
            'VPS_SSH_KEY',
            'REMOTE_DIR',
        ]
        
        optional_but_recommended = [
            'REPO_SERVER_URL',
            'GPG_KEY_ID',
            'GPG_PRIVATE_KEY',
            'PACKAGER_ENV',
        ]
        
        # Check required variables
        missing_vars = []
        for var in required_vars:
            value = os.getenv(var)
            if not value or value.strip() == '':
                missing_vars.append(var)
                logger.error(f"[ERROR] Variable {var} is empty! Ensure it is set in GitHub Secrets.")
        
        if missing_vars:
            sys.exit(1)
        
        # Check optional variables and warn if missing
        for var in optional_but_recommended:
            value = os.getenv(var)
            if not value or value.strip() == '':
                logger.warning(f"‚ö†Ô∏è Optional variable {var} is empty")
        
        # ‚úÖ BIZTONS√ÅGI JAV√çT√ÅS: NE jelen√≠ts√ºnk meg titkos inform√°ci√≥kat!
        logger.info("‚úÖ Environment validation passed:")
        for var in required_vars + optional_but_recommended:
            value = os.getenv(var)
            if value and value.strip() != '':
                logger.info(f"   {var}: [LOADED]")
            else:
                logger.info(f"   {var}: [MISSING]")
        
        # Validate REPO_NAME for pacman.conf
        repo_name = os.getenv('REPO_NAME')
        if repo_name:
            if not re.match(r'^[a-zA-Z0-9_-]+$', repo_name):
                logger.error(f"[ERROR] Invalid REPO_NAME '{repo_name}'. Must contain only letters, numbers, hyphens, and underscores.")
                sys.exit(1)
            if len(repo_name) > 50:
                logger.error(f"[ERROR] REPO_NAME '{repo_name}' is too long (max 50 characters).")
                sys.exit(1)
--- FILE: .github/scripts/modules/common/logging_utils.py ---
"""
Logging Utilities Module - Configures and manages logging
"""

import logging


def setup_logging():
    """Configure logging for the application"""
    logging.basicConfig(
        level=logging.INFO,
        format='[%(asctime)s] %(levelname)s: %(message)s',
        datefmt='%H:%M:%S',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('builder.log')
        ]
    )
    
    return logging.getLogger(__name__)
--- FILE: .github/scripts/modules/common/shell_executor.py ---
"""
Shell Executor Module - Handles shell command execution with comprehensive logging
"""

import os
import subprocess
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


class ShellExecutor:
    """Handles shell command execution with comprehensive logging and timeout"""
    
    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
    
    def run_command(self, cmd, cwd=None, capture=True, check=True, shell=True, user=None, 
                   log_cmd=False, timeout=1800, extra_env=None):
        """Run command with comprehensive logging, timeout, and optional extra environment variables"""
        if log_cmd or self.debug_mode:
            if self.debug_mode:
                print(f"üîß [SHELL DEBUG] RUNNING COMMAND: {cmd}", flush=True)
            else:
                logger.info(f"RUNNING COMMAND: {cmd}")
        
        if cwd is None:
            cwd = Path.cwd()
        
        # Prepare environment
        env = os.environ.copy()
        if extra_env:
            env.update(extra_env)
        
        if user:
            env['HOME'] = f'/home/{user}'
            env['USER'] = user
            env['LC_ALL'] = 'C'
            
            try:
                sudo_cmd = ['sudo', '-u', user]
                if shell:
                    sudo_cmd.extend(['bash', '-c', f'cd "{cwd}" && {cmd}'])
                else:
                    sudo_cmd.extend(cmd)
                
                result = subprocess.run(
                    sudo_cmd,
                    capture_output=capture,
                    text=True,
                    check=check,
                    env=env,
                    timeout=timeout
                )
                
                # CRITICAL FIX: When in debug mode, bypass logger for critical output
                if log_cmd or self.debug_mode:
                    if self.debug_mode:
                        if result.stdout:
                            print(f"üîß [SHELL DEBUG] STDOUT:\n{result.stdout}", flush=True)
                        if result.stderr:
                            print(f"üîß [SHELL DEBUG] STDERR:\n{result.stderr}", flush=True)
                        print(f"üîß [SHELL DEBUG] EXIT CODE: {result.returncode}", flush=True)
                    else:
                        if result.stdout:
                            logger.info(f"STDOUT: {result.stdout[:500]}")
                        if result.stderr:
                            logger.info(f"STDERR: {result.stderr[:500]}")
                        logger.info(f"EXIT CODE: {result.returncode}")
                
                # CRITICAL: If command failed and we're in debug mode, print full output
                if result.returncode != 0 and self.debug_mode:
                    print(f"‚ùå [SHELL DEBUG] COMMAND FAILED: {cmd}", flush=True)
                    if result.stdout and len(result.stdout) > 500:
                        print(f"‚ùå [SHELL DEBUG] FULL STDOUT (truncated):\n{result.stdout[:2000]}", flush=True)
                    if result.stderr and len(result.stderr) > 500:
                        print(f"‚ùå [SHELL DEBUG] FULL STDERR (truncated):\n{result.stderr[:2000]}", flush=True)
                
                return result
            except subprocess.TimeoutExpired as e:
                error_msg = f"‚ö†Ô∏è Command timed out after {timeout} seconds: {cmd}"
                if self.debug_mode:
                    print(f"‚ùå [SHELL DEBUG] {error_msg}", flush=True)
                logger.error(error_msg)
                raise
            except subprocess.CalledProcessError as e:
                if log_cmd or self.debug_mode:
                    error_msg = f"Command failed: {cmd}"
                    if self.debug_mode:
                        print(f"‚ùå [SHELL DEBUG] {error_msg}", flush=True)
                        if hasattr(e, 'stdout') and e.stdout:
                            print(f"‚ùå [SHELL DEBUG] EXCEPTION STDOUT:\n{e.stdout}", flush=True)
                        if hasattr(e, 'stderr') and e.stderr:
                            print(f"‚ùå [SHELL DEBUG] EXCEPTION STDERR:\n{e.stderr}", flush=True)
                    else:
                        logger.error(error_msg)
                if check:
                    raise
                return e
        else:
            try:
                env['LC_ALL'] = 'C'
                
                result = subprocess.run(
                    cmd,
                    cwd=cwd,
                    shell=shell,
                    capture_output=capture,
                    text=True,
                    check=check,
                    env=env,
                    timeout=timeout
                )
                
                # CRITICAL FIX: When in debug mode, bypass logger for critical output
                if log_cmd or self.debug_mode:
                    if self.debug_mode:
                        if result.stdout:
                            print(f"üîß [SHELL DEBUG] STDOUT:\n{result.stdout}", flush=True)
                        if result.stderr:
                            print(f"üîß [SHELL DEBUG] STDERR:\n{result.stderr}", flush=True)
                        print(f"üîß [SHELL DEBUG] EXIT CODE: {result.returncode}", flush=True)
                    else:
                        if result.stdout:
                            logger.info(f"STDOUT: {result.stdout[:500]}")
                        if result.stderr:
                            logger.info(f"STDERR: {result.stderr[:500]}")
                        logger.info(f"EXIT CODE: {result.returncode}")
                
                # CRITICAL: If command failed and we're in debug mode, print full output
                if result.returncode != 0 and self.debug_mode:
                    print(f"‚ùå [SHELL DEBUG] COMMAND FAILED: {cmd}", flush=True)
                    if result.stdout and len(result.stdout) > 500:
                        print(f"‚ùå [SHELL DEBUG] FULL STDOUT (truncated):\n{result.stdout[:2000]}", flush=True)
                    if result.stderr and len(result.stderr) > 500:
                        print(f"‚ùå [SHELL DEBUG] FULL STDERR (truncated):\n{result.stderr[:2000]}", flush=True)
                
                return result
            except subprocess.TimeoutExpired as e:
                error_msg = f"‚ö†Ô∏è Command timed out after {timeout} seconds: {cmd}"
                if self.debug_mode:
                    print(f"‚ùå [SHELL DEBUG] {error_msg}", flush=True)
                logger.error(error_msg)
                raise
            except subprocess.CalledProcessError as e:
                if log_cmd or self.debug_mode:
                    error_msg = f"Command failed: {cmd}"
                    if self.debug_mode:
                        print(f"‚ùå [SHELL DEBUG] {error_msg}", flush=True)
                        if hasattr(e, 'stdout') and e.stdout:
                            print(f"‚ùå [SHELL DEBUG] EXCEPTION STDOUT:\n{e.stdout}", flush=True)
                        if hasattr(e, 'stderr') and e.stderr:
                            print(f"‚ùå [SHELL DEBUG] EXCEPTION STDERR:\n{e.stderr}", flush=True)
                    else:
                        logger.error(error_msg)
                if check:
                    raise
                return e
--- FILE: .github/scripts/modules/gpg/__init__.py ---
"""
GPG module for key handling and signing
"""
--- FILE: .github/scripts/modules/gpg/gpg_handler.py ---
"""
GPG Handler Module - Handles GPG key import, signing, and pacman-key operations
"""

import os
import subprocess
import shutil
import tempfile
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


class GPGHandler:
    """Handles GPG key import, repository signing, and pacman-key operations"""
    
    def __init__(self):
        self.gpg_private_key = os.getenv('GPG_PRIVATE_KEY')
        self.gpg_key_id = os.getenv('GPG_KEY_ID')
        self.gpg_enabled = bool(self.gpg_private_key and self.gpg_key_id)
        self.gpg_home = None
        self.gpg_env = None
        
        # Safe logging - no sensitive information
        if self.gpg_key_id:
            logger.info(f"GPG Environment Check: Key ID found: YES, Key data found: {'YES' if self.gpg_private_key else 'NO'}")
        else:
            logger.info("GPG Environment Check: No GPG key ID configured")
    
    def import_gpg_key(self) -> bool:
        """Import GPG private key and set trust level WITHOUT interactive terminal (container-safe)"""
        if not self.gpg_enabled:
            logger.info("GPG Key not detected. Skipping repository signing.")
            return False
        
        logger.info("GPG Key detected. Importing private key...")
        
        # Handle both string and bytes for the private key
        key_data = self.gpg_private_key
        if isinstance(key_data, bytes):
            key_data_str = key_data.decode('utf-8')
        else:
            key_data_str = str(key_data)
        
        # Validate private key format before attempting import
        if not key_data_str or '-----BEGIN PGP PRIVATE KEY BLOCK-----' not in key_data_str:
            logger.error("‚ùå CRITICAL: Invalid GPG private key format.")
            logger.error("Disabling GPG signing for this build.")
            self.gpg_enabled = False
            return False
        
        try:
            # Create a temporary GPG home directory
            temp_gpg_home = tempfile.mkdtemp(prefix="gpg_home_")
            
            # Set environment for GPG
            env = os.environ.copy()
            env['GNUPGHOME'] = temp_gpg_home
            
            # Import the private key
            if isinstance(self.gpg_private_key, bytes):
                key_input = self.gpg_private_key
            else:
                key_input = self.gpg_private_key.encode('utf-8')
            
            import_process = subprocess.run(
                ['gpg', '--batch', '--import'],
                input=key_input,
                capture_output=True,
                text=False,
                env=env,
                check=False
            )
            
            if import_process.returncode != 0:
                stderr = import_process.stderr.decode('utf-8') if isinstance(import_process.stderr, bytes) else import_process.stderr
                logger.error(f"Failed to import GPG key: {stderr}")
                shutil.rmtree(temp_gpg_home, ignore_errors=True)
                return False
            
            logger.info("‚úÖ GPG key imported successfully")
            
            # Get fingerprint and set ultimate trust
            list_process = subprocess.run(
                ['gpg', '--list-keys', '--with-colons', self.gpg_key_id],
                capture_output=True,
                text=True,
                env=env,
                check=False
            )
            
            fingerprint = None
            if list_process.returncode == 0:
                for line in list_process.stdout.split('\n'):
                    if line.startswith('fpr:'):
                        parts = line.split(':')
                        if len(parts) > 9:
                            fingerprint = parts[9]
                            # Set ultimate trust (6 = ultimate)
                            trust_process = subprocess.run(
                                ['gpg', '--import-ownertrust'],
                                input=f"{fingerprint}:6:\n".encode('utf-8'),
                                capture_output=True,
                                text=False,
                                env=env,
                                check=False
                            )
                            if trust_process.returncode == 0:
                                logger.info("‚úÖ Set ultimate trust for GPG key")
                            break
            
            # CRITICAL FIX: Initialize pacman-key if not already initialized
            if not os.path.exists('/etc/pacman.d/gnupg'):
                logger.info("Initializing pacman keyring...")
                init_process = subprocess.run(
                    ['sudo', 'pacman-key', '--init'],
                    capture_output=True,
                    text=True,
                    check=False
                )
                if init_process.returncode == 0:
                    logger.info("‚úÖ Pacman keyring initialized")
                else:
                    logger.warning(f"‚ö†Ô∏è Pacman-key init warning: {init_process.stderr[:200]}")
            
            # Export public key and add to pacman-key WITHOUT interactive terminal
            if fingerprint:
                try:
                    # Export public key to a temporary file
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.asc', delete=False) as pub_key_file:
                        export_process = subprocess.run(
                            ['gpg', '--armor', '--export', fingerprint],
                            capture_output=True,
                            text=True,
                            env=env,
                            check=True
                        )
                        pub_key_file.write(export_process.stdout)
                        pub_key_path = pub_key_file.name
                    
                    # Add to pacman-key WITH SUDO
                    logger.info("Adding GPG key to pacman-key...")
                    add_process = subprocess.run(
                        ['sudo', 'pacman-key', '--add', pub_key_path],
                        capture_output=True,
                        text=True,
                        check=False
                    )
                    
                    if add_process.returncode != 0:
                        logger.error(f"Failed to add key to pacman-key: {add_process.stderr}")
                    else:
                        logger.info("‚úÖ Key added to pacman-key")
                    
                    # CRITICAL FIX: Update pacman-key database and populate keyring
                    logger.info("Updating pacman-key database...")
                    update_process = subprocess.run(
                        ['sudo', 'pacman-key', '--updatedb'],
                        capture_output=True,
                        text=True,
                        check=False
                    )
                    if update_process.returncode == 0:
                        logger.info("‚úÖ Pacman-key database updated")
                    else:
                        logger.warning(f"‚ö†Ô∏è Pacman-key update warning: {update_process.stderr[:200]}")
                    
                    logger.info("Populating pacman keyring...")
                    populate_process = subprocess.run(
                        ['sudo', 'pacman-key', '--populate'],
                        capture_output=True,
                        text=True,
                        check=False
                    )
                    if populate_process.returncode == 0:
                        logger.info("‚úÖ Pacman keyring populated")
                    else:
                        logger.warning(f"‚ö†Ô∏è Pacman-key populate warning: {populate_process.stderr[:200]}")
                    
                    # Import ownertrust into pacman keyring
                    logger.info("Setting ultimate trust in pacman keyring...")
                    ownertrust_content = f"{fingerprint}:6:\n"
                    
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.trust', delete=False) as trust_file:
                        trust_file.write(ownertrust_content)
                        trust_file_path = trust_file.name
                    
                    trust_cmd = [
                        'sudo', 'gpg',
                        '--homedir', '/etc/pacman.d/gnupg',
                        '--batch',
                        '--import-ownertrust',
                        trust_file_path
                    ]
                    
                    try:
                        trust_process = subprocess.run(
                            trust_cmd,
                            capture_output=True,
                            text=True,
                            check=False
                        )
                        
                        if trust_process.returncode == 0:
                            logger.info("‚úÖ Set ultimate trust for key in pacman keyring")
                        else:
                            logger.warning(f"‚ö†Ô∏è Failed to set trust with gpg: {trust_process.stderr[:200]}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error setting trust with gpg: {e}")
                    finally:
                        os.unlink(trust_file_path)
                        os.unlink(pub_key_path)
                    
                except Exception as e:
                    logger.error(f"Error during pacman-key setup: {e}")
            
            # Store the GPG home directory for later use
            self.gpg_home = temp_gpg_home
            self.gpg_env = env
            
            return True
            
        except Exception as e:
            logger.error(f"Error importing GPG key: {e}")
            if 'temp_gpg_home' in locals():
                shutil.rmtree(temp_gpg_home, ignore_errors=True)
            return False
    
    def sign_repository_files(self, repo_name: str, output_dir: str) -> bool:
        """Sign repository database files with GPG"""
        if not self.gpg_enabled:
            logger.info("GPG signing disabled - skipping repository signing")
            return False
        
        if not hasattr(self, 'gpg_home') or not hasattr(self, 'gpg_env'):
            logger.error("GPG key not imported. Cannot sign repository files.")
            return False
        
        try:
            output_path = Path(output_dir)
            files_to_sign = [
                output_path / f"{repo_name}.db",
                output_path / f"{repo_name}.db.tar.gz",
                output_path / f"{repo_name}.files",
                output_path / f"{repo_name}.files.tar.gz"
            ]
            
            signed_count = 0
            failed_count = 0
            
            for file_to_sign in files_to_sign:
                if not file_to_sign.exists():
                    logger.warning(f"Repository file not found for signing: {file_to_sign.name}")
                    continue
                
                logger.info(f"Signing repository database: {file_to_sign.name}")
                
                # Delete existing .sig file before signing
                sig_file = file_to_sign.with_suffix(file_to_sign.suffix + '.sig')
                if sig_file.exists():
                    try:
                        sig_file.unlink()
                        logger.info(f"üóëÔ∏è Removed existing signature: {sig_file.name}")
                    except Exception as e:
                        logger.warning(f"Could not remove existing signature {sig_file.name}: {e}")
                
                # Create detached signature
                sign_process = subprocess.run(
                    [
                        'gpg', '--detach-sign',
                        '--default-key', self.gpg_key_id,
                        '--output', str(sig_file),
                        str(file_to_sign)
                    ],
                    capture_output=True,
                    text=True,
                    env=self.gpg_env,
                    check=False
                )
                
                if sign_process.returncode == 0:
                    logger.info(f"‚úÖ Created signature: {sig_file.name}")
                    signed_count += 1
                else:
                    logger.warning(f"‚ö†Ô∏è Failed to sign {file_to_sign.name}: {sign_process.stderr[:200]}")
                    failed_count += 1
            
            if signed_count > 0:
                logger.info(f"‚úÖ Successfully signed {signed_count} repository file(s)")
                # CRITICAL FIX: Minor warnings should not block the build
                if failed_count > 0:
                    logger.warning(f"‚ö†Ô∏è {failed_count} file(s) failed to sign, but continuing anyway")
                return True
            else:
                logger.error("Failed to sign any repository files")
                # CRITICAL FIX: Don't fail the build if GPG signing has issues
                logger.warning("‚ö†Ô∏è Continuing build without GPG signatures")
                return False
                
        except Exception as e:
            logger.error(f"Error signing repository files: {e}")
            # CRITICAL FIX: Don't fail the build if GPG signing has issues
            logger.warning("‚ö†Ô∏è Continuing build without GPG signatures due to error")
            return False
    
    def cleanup(self):
        """Clean up temporary GPG home directory"""
        if hasattr(self, 'gpg_home'):
            try:
                shutil.rmtree(self.gpg_home, ignore_errors=True)
                logger.debug("Cleaned up temporary GPG home directory")
            except Exception as e:
                logger.warning(f"Could not clean up GPG directory: {e}")

--- FILE: .github/scripts/modules/orchestrator/__init__.py ---
"""
Orchestrator module for package building coordination
"""
--- FILE: .github/scripts/modules/orchestrator/state.py ---
"""
State Module - Manages application state and configuration
"""

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional


@dataclass
class BuildState:
    """Represents the current build state"""
    repo_root: Path
    repo_name: str
    output_dir: Path
    build_tracking_dir: Path
    mirror_temp_dir: Path
    sync_clone_dir: Path
    aur_build_dir: Path
    packager_id: str
    debug_mode: bool
    
    # Remote state
    remote_files: List[str] = None
    repo_exists: bool = False
    has_packages: bool = False
    
    # Build results
    built_packages: List[str] = None
    skipped_packages: List[str] = None
    rebuilt_local_packages: List[str] = None
    
    def __post_init__(self):
        if self.remote_files is None:
            self.remote_files = []
        if self.built_packages is None:
            self.built_packages = []
        if self.skipped_packages is None:
            self.skipped_packages = []
        if self.rebuilt_local_packages is None:
            self.rebuilt_local_packages = []
    
    def add_remote_file(self, filename: str):
        """Add a remote file to the state"""
        self.remote_files.append(filename)
    
    def add_built_package(self, pkg_name: str, version: str):
        """Add a built package to the state"""
        self.built_packages.append(f"{pkg_name} ({version})")
    
    def add_skipped_package(self, pkg_name: str, version: str):
        """Add a skipped package to the state"""
        self.skipped_packages.append(f"{pkg_name} ({version})")
    
    def add_rebuilt_local_package(self, pkg_name: str):
        """Add a rebuilt local package to the state"""
        self.rebuilt_local_packages.append(pkg_name)
--- FILE: .github/scripts/modules/orchestrator/package_builder.py ---
"""
Package Builder Module - Main orchestrator for package building coordination
WITH CACHE-AWARE BUILDING
"""

import os
import sys
import re
import subprocess
import shutil
import tempfile
import time
import glob
from pathlib import Path
from typing import List, Optional, Tuple

# Add parent directory to path for imports
script_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if script_dir not in sys.path:
    sys.path.insert(0, script_dir)

# Import our modules - adjust imports to work from the modules directory
try:
    from modules.common.logging_utils import setup_logger
    logger = setup_logger(__name__)
except ImportError:
    import logging
    logger = logging.getLogger(__name__)
    logging.basicConfig(level=logging.INFO)

from modules.common.config_loader import ConfigLoader
from modules.common.environment import EnvironmentValidator
from modules.common.shell_executor import ShellExecutor
from modules.build.artifact_manager import ArtifactManager
from modules.build.aur_builder import AURBuilder
from modules.build.local_builder import LocalBuilder
from modules.build.version_manager import VersionManager
from modules.build.build_tracker import BuildTracker
from modules.gpg.gpg_handler import GPGHandler
from modules.vps.ssh_client import SSHClient
from modules.vps.rsync_client import RsyncClient
from modules.repo.cleanup_manager import CleanupManager
from modules.repo.database_manager import DatabaseManager
from modules.repo.version_tracker import VersionTracker


class PackageBuilder:
    """Main orchestrator that coordinates between modules for package building WITH CACHE SUPPORT"""
    
    def __init__(self):
        # Run pre-flight environment validation
        EnvironmentValidator.validate_env()
        
        # Load configuration
        self.config_loader = ConfigLoader()
        self.repo_root = self.config_loader.get_repo_root()
        env_config = self.config_loader.load_environment_config()
        python_config = self.config_loader.load_from_python_config()
        
        # Store configuration
        self.vps_user = env_config['vps_user']
        self.vps_host = env_config['vps_host']
        self.ssh_key = env_config['ssh_key']
        self.repo_server_url = env_config['repo_server_url']
        self.remote_dir = env_config['remote_dir']
        self.repo_name = env_config['repo_name']
        
        self.output_dir = self.repo_root / python_config['output_dir']
        self.build_tracking_dir = self.repo_root / python_config['build_tracking_dir']
        self.mirror_temp_dir = Path(python_config['mirror_temp_dir'])
        self.sync_clone_dir = Path(python_config['sync_clone_dir'])
        self.aur_urls = python_config['aur_urls']
        self.aur_build_dir = self.repo_root / python_config['aur_build_dir']
        self.ssh_options = python_config['ssh_options']
        self.github_repo = python_config['github_repo']
        self.packager_id = python_config['packager_id']
        self.debug_mode = python_config['debug_mode']
        
        # Cache configuration
        self.use_cache = os.getenv('USE_CACHE', 'false').lower() == 'true'
        
        # Create directories
        self.output_dir.mkdir(exist_ok=True)
        self.build_tracking_dir.mkdir(exist_ok=True)
        
        # Initialize modules
        self._init_modules()
        
        # State
        self.remote_files = []
        self.built_packages = []
        self.skipped_packages = []
        self.rebuilt_local_packages = []
        
        # Statistics
        self.stats = {
            "start_time": time.time(),
            "aur_success": 0,
            "local_success": 0,
            "aur_failed": 0,
            "local_failed": 0,
            "cache_hits": 0,
            "cache_misses": 0,
        }
    
    def _init_modules(self):
        """Initialize all modules with configuration"""
        try:
            # VPS modules
            vps_config = {
                'vps_user': self.vps_user,
                'vps_host': self.vps_host,
                'remote_dir': self.remote_dir,
                'ssh_options': self.ssh_options,
                'repo_name': self.repo_name,
            }
            self.ssh_client = SSHClient(vps_config)
            self.ssh_client.setup_ssh_config(self.ssh_key)
            
            self.rsync_client = RsyncClient(vps_config)
            
            # Repository modules
            repo_config = {
                'repo_name': self.repo_name,
                'output_dir': self.output_dir,
                'remote_dir': self.remote_dir,
                'mirror_temp_dir': self.mirror_temp_dir,
                'vps_user': self.vps_user,
                'vps_host': self.vps_host,
            }
            self.cleanup_manager = CleanupManager(repo_config)
            self.database_manager = DatabaseManager(repo_config)
            self.version_tracker = VersionTracker(repo_config)
            
            # Build modules
            self.artifact_manager = ArtifactManager()
            self.aur_builder = AURBuilder(self.debug_mode)
            self.local_builder = LocalBuilder(self.debug_mode)
            self.version_manager = VersionManager()
            self.build_tracker = BuildTracker()
            
            # GPG Handler
            self.gpg_handler = GPGHandler()
            
            # Shell executor
            self.shell_executor = ShellExecutor(self.debug_mode)
            
            logger.info("‚úÖ All modules initialized successfully")
            
            # Log cache status
            if self.use_cache:
                logger.info("üîß CACHE: Cache-aware building ENABLED")
                # Check cache status
                built_count = len(list(self.output_dir.glob("*.pkg.tar.*")))
                logger.info(f"üîß CACHE: Found {built_count} cached package files in output_dir")
            else:
                logger.info("üîß CACHE: Cache-aware building DISABLED")
            
        except NameError as e:
            logger.error(f"‚ùå NameError during module initialization: {e}")
            logger.error("This indicates missing imports in module files")
            sys.exit(1)
        except Exception as e:
            logger.error(f"‚ùå Error initializing modules: {e}")
            sys.exit(1)
    
    def get_package_lists(self):
        """Get package lists from packages.py or exit if not available"""
        try:
            # First try to import from the current directory
            import packages
            print("üì¶ Using package lists from packages.py")
            local_packages_list, aur_packages_list = packages.LOCAL_PACKAGES, packages.AUR_PACKAGES
            print(f">>> DEBUG: Found {len(local_packages_list + aur_packages_list)} packages to check")
            return local_packages_list, aur_packages_list
        except ImportError:
            try:
                # Try to import from parent directory
                import sys
                sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                import scripts.packages as packages
                print("üì¶ Using package lists from packages.py")
                local_packages_list, aur_packages_list = packages.LOCAL_PACKAGES, packages.AUR_PACKAGES
                print(f">>> DEBUG: Found {len(local_packages_list + aur_packages_list)} packages to check")
                return local_packages_list, aur_packages_list
            except ImportError:
                logger.error("Cannot load package lists from packages.py. Exiting.")
                sys.exit(1)
    
    def _check_cache_for_package(self, pkg_name: str, is_aur: bool) -> Tuple[bool, Optional[str]]:
        """
        Check if package is available in cache and up-to-date.
        
        Args:
            pkg_name: Package name
            is_aur: Whether it's an AUR package
        
        Returns:
            Tuple of (cached: bool, version: Optional[str])
        """
        if not self.use_cache:
            return False, None
        
        # Check built packages cache first
        cache_patterns = [
            f"{self.output_dir}/{pkg_name}-*.pkg.tar.*",
            f"{self.output_dir}/*{pkg_name}*.pkg.tar.*"
        ]
        
        cached_files = []
        for pattern in cache_patterns:
            cached_files.extend(glob.glob(pattern))
        
        if cached_files:
            # Extract version from cached file
            for cached_file in cached_files:
                try:
                    # Parse version from filename
                    filename = os.path.basename(cached_file)
                    base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
                    parts = base.split('-')
                    
                    # Try to extract version
                    for i in range(len(parts) - 2, 0, -1):
                        possible_name = '-'.join(parts[:i])
                        if possible_name == pkg_name or possible_name.startswith(pkg_name + '-'):
                            if len(parts) >= i + 3:
                                version_part = parts[i]
                                release_part = parts[i+1]
                                if i + 1 < len(parts) and parts[i].isdigit() and i + 2 < len(parts):
                                    epoch_part = parts[i]
                                    version_part = parts[i+1]
                                    release_part = parts[i+2]
                                    cached_version = f"{epoch_part}:{version_part}-{release_part}"
                                    
                                    # Extract components for comparison
                                    epoch = epoch_part
                                    pkgver = version_part
                                    pkgrel = release_part
                                else:
                                    cached_version = f"{version_part}-{release_part}"
                                    
                                    # Extract components for comparison
                                    epoch = None
                                    pkgver = version_part
                                    pkgrel = release_part
                                
                                # Check if this cached version is newer than remote
                                remote_version = self.get_remote_version(pkg_name)
                                if remote_version:
                                    # Compare versions
                                    should_build = self.version_manager.compare_versions(remote_version, pkgver, pkgrel, epoch)
                                    if not should_build:
                                        logger.info(f"üì¶ CACHE HIT: {pkg_name} (cached: {cached_version}, remote: {remote_version}) - SKIP BUILD")
                                        self.stats["cache_hits"] += 1
                                        return True, cached_version
                                    else:
                                        logger.info(f"üì¶ CACHE STALE: {pkg_name} (cached: {cached_version}, remote: {remote_version}) - NEEDS REBUILD")
                                        self.stats["cache_misses"] += 1
                                        return False, None
                                else:
                                    # No remote version, cache is valid
                                    logger.info(f"üì¶ CACHE HIT: {pkg_name} (cached: {cached_version}, no remote) - SKIP BUILD")
                                    self.stats["cache_hits"] += 1
                                    return True, cached_version
                except Exception as e:
                    logger.debug(f"Could not parse version from cached file {cached_file}: {e}")
        
        self.stats["cache_misses"] += 1
        return False, None
    
    def _apply_repository_state(self, exists: bool, has_packages: bool):
        """Apply repository state with proper SigLevel based on discovery - CRITICAL FIX: Run pacman -Sy after enabling repository"""
        pacman_conf = Path("/etc/pacman.conf")
        
        if not pacman_conf.exists():
            logger.warning("pacman.conf not found")
            return
        
        try:
            with open(pacman_conf, 'r') as f:
                content = f.read()
            
            repo_section = f"[{self.repo_name}]"
            lines = content.split('\n')
            new_lines = []
            
            # Remove old section if it exists
            in_section = False
            for line in lines:
                # Check if we're entering our section
                if line.strip() == repo_section or line.strip() == f"#{repo_section}":
                    in_section = True
                    continue
                elif in_section and (line.strip().startswith('[') or line.strip() == ''):
                    # We're leaving our section
                    in_section = False
                
                if not in_section:
                    new_lines.append(line)
            
            # Add new section if repository exists on VPS
            if exists:
                new_lines.append('')
                new_lines.append(f"# Custom repository: {self.repo_name}")
                new_lines.append(f"# Automatically enabled - found on VPS")
                new_lines.append(repo_section)
                if has_packages:
                    new_lines.append("SigLevel = Optional TrustAll")
                    logger.info("‚úÖ Enabling repository with SigLevel = Optional TrustAll (build mode)")
                else:
                    new_lines.append("# SigLevel = Optional TrustAll")
                    new_lines.append("# Repository exists but has no packages yet")
                    logger.info("‚ö†Ô∏è Repository section added but commented (no packages yet)")
                
                if self.repo_server_url:
                    new_lines.append(f"Server = {self.repo_server_url}")
                else:
                    new_lines.append("# Server = [URL not configured in secrets]")
                new_lines.append('')
            else:
                # Repository doesn't exist on VPS, add commented section
                new_lines.append('')
                new_lines.append(f"# Custom repository: {self.repo_name}")
                new_lines.append(f"# Disabled - not found on VPS (first run?)")
                new_lines.append(f"#{repo_section}")
                new_lines.append("#SigLevel = Optional TrustAll")
                if self.repo_server_url:
                    new_lines.append(f"#Server = {self.repo_server_url}")
                else:
                    new_lines.append("# Server = [URL not configured in secrets]")
                new_lines.append('')
                logger.info("‚ÑπÔ∏è Repository not found on VPS - keeping disabled")
            
            # Write back to pacman.conf
            with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_file:
                temp_file.write('\n'.join(new_lines))
                temp_path = temp_file.name
            
            # Copy to pacman.conf
            subprocess.run(['sudo', 'cp', temp_path, str(pacman_conf)], check=False)
            subprocess.run(['sudo', 'chmod', '644', str(pacman_conf)], check=False)
            os.unlink(temp_path)
            
            logger.info(f"‚úÖ Updated pacman.conf for repository '{self.repo_name}'")
            
            # CRITICAL FIX: Run pacman -Syy after enabling repository to force refresh
            if exists and has_packages:
                logger.info("üîÑ Synchronizing pacman databases after enabling repository...")
                
                # CRITICAL FIX: Update pacman-key database first
                cmd = "sudo pacman-key --updatedb"
                result = self.shell_executor.run_command(cmd, log_cmd=True, timeout=300, check=False)
                if result.returncode != 0:
                    logger.warning(f"‚ö†Ô∏è pacman-key --updatedb warning: {result.stderr[:200]}")
                
                cmd = "sudo LC_ALL=C pacman -Syy --noconfirm"
                result = self.shell_executor.run_command(cmd, log_cmd=True, timeout=300, check=False)
                if result.returncode == 0:
                    logger.info("‚úÖ Pacman databases synchronized successfully")
                else:
                    logger.warning(f"‚ö†Ô∏è Pacman sync warning: {result.stderr[:200]}")
            
        except Exception as e:
            logger.error(f"Failed to apply repository state: {e}")
    
    def _sync_pacman_databases(self):
        """Simplified pacman database sync with proper SigLevel handling"""
        print("\n" + "=" * 60)
        print("FINAL STEP: Syncing pacman databases")
        print("=" * 60)
        
        # First, ensure repository is enabled with proper SigLevel
        exists, has_packages = self.ssh_client.check_repository_exists_on_vps()
        self._apply_repository_state(exists, has_packages)
        
        if not exists:
            logger.info("‚ÑπÔ∏è Repository doesn't exist on VPS, skipping pacman sync")
            return False
        
        # CRITICAL FIX: Update pacman-key database first
        cmd = "sudo pacman-key --updatedb"
        result = self.shell_executor.run_command(cmd, log_cmd=True, timeout=300, check=False)
        if result.returncode != 0:
            logger.warning(f"‚ö†Ô∏è pacman-key --updatedb warning: {result.stderr[:200]}")
        
        # CRITICAL FIX: Use Syy to force refresh instead of Sy
        cmd = "sudo LC_ALL=C pacman -Syy --noconfirm"
        result = self.shell_executor.run_command(cmd, log_cmd=True, timeout=300, check=False)
        
        if result.returncode == 0:
            logger.info("‚úÖ Pacman databases synced successfully")
            
            # Debug: List packages in our custom repo
            debug_cmd = f"sudo pacman -Sl {self.repo_name}"
            logger.info(f"üîç DEBUG: Running command to see what packages pacman sees in our repo:")
            logger.info(f"Command: {debug_cmd}")
            
            debug_result = self.shell_executor.run_command(debug_cmd, log_cmd=True, timeout=30, check=False)
            
            if debug_result.returncode == 0:
                if debug_result.stdout.strip():
                    logger.info(f"Packages in {self.repo_name} according to pacman:")
                    for line in debug_result.stdout.splitlines():
                        logger.info(f"  {line}")
                else:
                    logger.warning(f"‚ö†Ô∏è pacman -Sl {self.repo_name} returned no output (repo might be empty)")
            else:
                logger.warning(f"‚ö†Ô∏è pacman -Sl failed: {debug_result.stderr[:200]}")
            
            return True
        else:
            logger.error("‚ùå Pacman sync failed")
            if result.stderr:
                logger.error(f"Error: {result.stderr[:500]}")
            return False
    
    def package_exists(self, pkg_name: str, version=None) -> bool:
        """Check if package exists on server"""
        return self.version_tracker.package_exists(pkg_name, self.remote_files)
    
    def get_remote_version(self, pkg_name: str) -> Optional[str]:
        """Get the version of a package from remote server using SRCINFO-based extraction"""
        return self.version_tracker.get_remote_version(pkg_name, self.remote_files)
    
    def _build_aur_package(self, pkg_name: str) -> bool:
        """Build AUR package with SRCINFO-based version comparison and PACKAGER injection - FIXED: AUR dependency fallback"""
        aur_dir = self.aur_build_dir
        aur_dir.mkdir(exist_ok=True)
        
        pkg_dir = aur_dir / pkg_name
        if pkg_dir.exists():
            shutil.rmtree(pkg_dir, ignore_errors=True)
        
        print(f"Cloning {pkg_name} from AUR...")
        
        # Try different AUR URLs from config (ALWAYS FRESH CLONE)
        clone_success = False
        for aur_url_template in self.aur_urls:
            aur_url = aur_url_template.format(pkg_name=pkg_name)
            logger.info(f"Trying AUR URL: {aur_url}")
            result = self.shell_executor.run_command(
                f"git clone --depth 1 {aur_url} {pkg_dir}",
                check=False
            )
            if result and result.returncode == 0:
                clone_success = True
                logger.info(f"Successfully cloned {pkg_name} from {aur_url}")
                break
            else:
                if pkg_dir.exists():
                    shutil.rmtree(pkg_dir, ignore_errors=True)
                logger.warning(f"Failed to clone from {aur_url}")
        
        if not clone_success:
            logger.error(f"Failed to clone {pkg_name} from any AUR URL")
            return False
        
        # Set correct permissions
        self.shell_executor.run_command(f"chown -R builder:builder {pkg_dir}", check=False)
        
        pkgbuild = pkg_dir / "PKGBUILD"
        if not pkgbuild.exists():
            logger.error(f"No PKGBUILD found for {pkg_name}")
            shutil.rmtree(pkg_dir, ignore_errors=True)
            return False
        
        # Extract version info from SRCINFO (not regex)
        try:
            pkgver, pkgrel, epoch = self.version_manager.extract_version_from_srcinfo(pkg_dir)
            version = self.version_manager.get_full_version_string(pkgver, pkgrel, epoch)
            
            # Get remote version for comparison
            remote_version = self.get_remote_version(pkg_name)
            
            # DECISION LOGIC: Only build if AUR_VERSION > REMOTE_VERSION
            if remote_version and not self.version_manager.compare_versions(remote_version, pkgver, pkgrel, epoch):
                logger.info(f"‚úÖ {pkg_name} already up to date on server ({remote_version}) - skipping")
                self.skipped_packages.append(f"{pkg_name} ({version})")
                
                # ZERO-RESIDUE FIX: Register the target version for this skipped package
                self.version_tracker.register_skipped_package(pkg_name, remote_version)
                
                shutil.rmtree(pkg_dir, ignore_errors=True)
                return False
            
            if remote_version:
                logger.info(f"‚ÑπÔ∏è  {pkg_name}: remote has {remote_version}, building {version}")
                
                # PRE-BUILD PURGE: Remove old version files BEFORE building new version
                self.cleanup_manager.pre_build_purge_old_versions(pkg_name, remote_version)
            else:
                logger.info(f"‚ÑπÔ∏è  {pkg_name}: not on server, building {version}")
                
        except Exception as e:
            logger.error(f"Failed to extract version for {pkg_name}: {e}")
            version = "unknown"
        
        try:
            logger.info(f"Building {pkg_name} ({version})...")
            
            # Clean workspace before building
            self.artifact_manager.clean_workspace(pkg_dir)
            
            print("Downloading sources...")
            source_result = self.shell_executor.run_command(f"makepkg -od --noconfirm", 
                                        cwd=pkg_dir, check=False, capture=True, timeout=600,
                                        extra_env={"PACKAGER": self.packager_id})
            if source_result.returncode != 0:
                logger.error(f"Failed to download sources for {pkg_name}")
                shutil.rmtree(pkg_dir, ignore_errors=True)
                return False
            
            # CRITICAL FIX: Install dependencies with AUR fallback
            print("Installing dependencies with AUR fallback...")
            
            # First attempt: Standard makepkg -si
            print("Building package (first attempt)...")
            build_result = self.shell_executor.run_command(
                f"makepkg -si --noconfirm --clean --nocheck",
                cwd=pkg_dir,
                capture=True,
                check=False,
                timeout=3600,
                extra_env={"PACKAGER": self.packager_id}
            )
            
            # If first attempt fails, try yay fallback for missing dependencies
            if build_result.returncode != 0:
                logger.warning(f"First build attempt failed for {pkg_name}, trying AUR dependency fallback...")
                
                # Extract missing dependencies from error output
                error_output = build_result.stderr if build_result.stderr else build_result.stdout
                missing_deps = []
                
                # Look for patterns like "error: target not found: <package>"
                import re
                missing_patterns = [
                    r"error: target not found: (\S+)",
                    r"Could not find all required packages:",
                    r":: Unable to find (\S+)",
                ]
                
                for pattern in missing_patterns:
                    matches = re.findall(pattern, error_output)
                    if matches:
                        missing_deps.extend(matches)
                
                # Also look for specific makepkg dependency errors
                if "makepkg: cannot find the" in error_output:
                    lines = error_output.split('\n')
                    for line in lines:
                        if "makepkg: cannot find the" in line:
                            # Extract package name from line like "makepkg: cannot find the 'gcc14' package"
                            dep_match = re.search(r"cannot find the '([^']+)'", line)
                            if dep_match:
                                missing_deps.append(dep_match.group(1))
                
                # Remove duplicates
                missing_deps = list(set(missing_deps))
                
                if missing_deps:
                    logger.info(f"Found missing dependencies: {missing_deps}")
                    
                    # CRITICAL FIX: Use Syy for dependency resolution
                    deps_str = ' '.join(missing_deps)
                    yay_cmd = f"LC_ALL=C yay -Syy --needed --noconfirm {deps_str}"
                    yay_result = self.shell_executor.run_command(yay_cmd, log_cmd=True, check=False, user="builder", timeout=1800)
                    
                    if yay_result.returncode == 0:
                        logger.info("‚úÖ Missing dependencies installed via yay, retrying build...")
                        
                        # Retry the build
                        build_result = self.shell_executor.run_command(
                            f"makepkg -si --noconfirm --clean --nocheck",
                            cwd=pkg_dir,
                            capture=True,
                            check=False,
                            timeout=3600,
                            extra_env={"PACKAGER": self.packager_id}
                        )
                    else:
                        logger.error(f"‚ùå Failed to install missing dependencies with yay")
                        shutil.rmtree(pkg_dir, ignore_errors=True)
                        return False
            
            if build_result.returncode == 0:
                moved = False
                for pkg_file in pkg_dir.glob("*.pkg.tar.*"):
                    dest = self.output_dir / pkg_file.name
                    shutil.move(str(pkg_file), str(dest))
                    logger.info(f"‚úÖ Built: {pkg_file.name}")
                    moved = True
                
                shutil.rmtree(pkg_dir, ignore_errors=True)
                
                if moved:
                    self.built_packages.append(f"{pkg_name} ({version})")
                    # ZERO-RESIDUE FIX: Register the target version for this built package
                    self.version_tracker.register_package_target_version(pkg_name, version)
                    return True
                else:
                    logger.error(f"No package files created for {pkg_name}")
                    return False
            else:
                logger.error(f"Failed to build {pkg_name}")
                shutil.rmtree(pkg_dir, ignore_errors=True)
                return False
                
        except Exception as e:
            logger.error(f"Error building {pkg_name}: {e}")
            shutil.rmtree(pkg_dir, ignore_errors=True)
            return False
    
    def _build_local_package(self, pkg_name: str) -> bool:
        """Build local package with SRCINFO-based version comparison and PACKAGER injection - FIXED: AUR dependency fallback"""
        pkg_dir = self.repo_root / pkg_name
        if not pkg_dir.exists():
            logger.error(f"Package directory not found: {pkg_name}")
            return False
        
        pkgbuild = pkg_dir / "PKGBUILD"
        if not pkgbuild.exists():
            logger.error(f"No PKGBUILD found for {pkg_name}")
            return False
        
        # Extract version info from SRCINFO (not regex)
        try:
            pkgver, pkgrel, epoch = self.version_manager.extract_version_from_srcinfo(pkg_dir)
            version = self.version_manager.get_full_version_string(pkgver, pkgrel, epoch)
            
            # Get remote version for comparison
            remote_version = self.get_remote_version(pkg_name)
            
            # DECISION LOGIC: Only build if AUR_VERSION > REMOTE_VERSION
            if remote_version and not self.version_manager.compare_versions(remote_version, pkgver, pkgrel, epoch):
                logger.info(f"‚úÖ {pkg_name} already up to date on server ({remote_version}) - skipping")
                self.skipped_packages.append(f"{pkg_name} ({version})")
                
                # ZERO-RESIDUE FIX: Register the target version for this skipped package
                self.version_tracker.register_skipped_package(pkg_name, remote_version)
                
                return False
            
            if remote_version:
                logger.info(f"‚ÑπÔ∏è  {pkg_name}: remote has {remote_version}, building {version}")
                
                # PRE-BUILD PURGE: Remove old version files BEFORE building new version
                self.cleanup_manager.pre_build_purge_old_versions(pkg_name, remote_version)
            else:
                logger.info(f"‚ÑπÔ∏è  {pkg_name}: not on server, building {version}")
                
        except Exception as e:
            logger.error(f"Failed to extract version for {pkg_name}: {e}")
            version = "unknown"
        
        try:
            logger.info(f"Building {pkg_name} ({version})...")
            
            # Clean workspace before building
            self.artifact_manager.clean_workspace(pkg_dir)
            
            print("Downloading sources...")
            source_result = self.shell_executor.run_command(f"makepkg -od --noconfirm", 
                                        cwd=pkg_dir, check=False, capture=True, timeout=600,
                                        extra_env={"PACKAGER": self.packager_id})
            if source_result.returncode != 0:
                logger.error(f"Failed to download sources for {pkg_name}")
                return False
            
            # CRITICAL FIX: Install dependencies with AUR fallback
            print("Installing dependencies with AUR fallback...")
            
            # First attempt: Standard makepkg with appropriate flags
            makepkg_flags = "-si --noconfirm --clean"
            if pkg_name == "gtk2":
                makepkg_flags += " --nocheck"
                logger.info("GTK2: Skipping check step (long)")
            
            print("Building package (first attempt)...")
            build_result = self.shell_executor.run_command(
                f"makepkg {makepkg_flags}",
                cwd=pkg_dir,
                capture=True,
                check=False,
                timeout=3600,
                extra_env={"PACKAGER": self.packager_id}
            )
            
            # If first attempt fails, try yay fallback for missing dependencies
            if build_result.returncode != 0:
                logger.warning(f"First build attempt failed for {pkg_name}, trying AUR dependency fallback...")
                
                # Extract missing dependencies from error output
                error_output = build_result.stderr if build_result.stderr else build_result.stdout
                missing_deps = []
                
                # Look for patterns like "error: target not found: <package>"
                import re
                missing_patterns = [
                    r"error: target not found: (\S+)",
                    r"Could not find all required packages:",
                    r":: Unable to find (\S+)",
                ]
                
                for pattern in missing_patterns:
                    matches = re.findall(pattern, error_output)
                    if matches:
                        missing_deps.extend(matches)
                
                # Also look for specific makepkg dependency errors
                if "makepkg: cannot find the" in error_output:
                    lines = error_output.split('\n')
                    for line in lines:
                        if "makepkg: cannot find the" in line:
                            # Extract package name from line like "makepkg: cannot find the 'gcc14' package"
                            dep_match = re.search(r"cannot find the '([^']+)'", line)
                            if dep_match:
                                missing_deps.append(dep_match.group(1))
                
                # Remove duplicates
                missing_deps = list(set(missing_deps))
                
                if missing_deps:
                    logger.info(f"Found missing dependencies: {missing_deps}")
                    
                    # CRITICAL FIX: Use Syy for dependency resolution
                    deps_str = ' '.join(missing_deps)
                    yay_cmd = f"LC_ALL=C yay -Syy --needed --noconfirm {deps_str}"
                    yay_result = self.shell_executor.run_command(yay_cmd, log_cmd=True, check=False, user="builder", timeout=1800)
                    
                    if yay_result.returncode == 0:
                        logger.info("‚úÖ Missing dependencies installed via yay, retrying build...")
                        
                        # Retry the build
                        build_result = self.shell_executor.run_command(
                            f"makepkg {makepkg_flags}",
                            cwd=pkg_dir,
                            capture=True,
                            check=False,
                            timeout=3600,
                            extra_env={"PACKAGER": self.packager_id}
                        )
                    else:
                        logger.error(f"‚ùå Failed to install missing dependencies with yay")
                        return False
            
            if build_result.returncode == 0:
                moved = False
                built_files = []
                for pkg_file in pkg_dir.glob("*.pkg.tar.*"):
                    dest = self.output_dir / pkg_file.name
                    shutil.move(str(pkg_file), str(dest))
                    logger.info(f"‚úÖ Built: {pkg_file.name}")
                    moved = True
                    built_files.append(str(dest))
                
                if moved:
                    self.built_packages.append(f"{pkg_name} ({version})")
                    self.rebuilt_local_packages.append(pkg_name)
                    
                    # ZERO-RESIDUE FIX: Register the target version for this built package
                    self.version_tracker.register_package_target_version(pkg_name, version)
                    
                    # Collect metadata for hokibot
                    if built_files:
                        # Simplified metadata extraction
                        filename = os.path.basename(built_files[0])
                        self.build_tracker.add_hokibot_data(pkg_name, pkgver, pkgrel, epoch)
                        logger.info(f"üìù HOKIBOT observed: {pkg_name} -> {version}")
                    
                    return True
                else:
                    logger.error(f"No package files created for {pkg_name}")
                    return False
            else:
                logger.error(f"Failed to build {pkg_name}")
                return False
                
        except Exception as e:
            logger.error(f"Error building {pkg_name}: {e}")
            return False
    
    def _build_single_package(self, pkg_name: str, is_aur: bool) -> bool:
        """Build a single package WITH CACHE CHECK"""
        print(f"\n--- Processing: {pkg_name} ({'AUR' if is_aur else 'Local'}) ---")
        
        # Check cache first
        cached, cached_version = self._check_cache_for_package(pkg_name, is_aur)
        if cached:
            logger.info(f"‚úÖ Using cached package: {pkg_name} ({cached_version})")
            self.built_packages.append(f"{pkg_name} ({cached_version}) [CACHED]")
            
            # Register target version for cleanup
            self.version_tracker.register_package_target_version(pkg_name, cached_version)
            
            # Record statistics
            if is_aur:
                self.stats["aur_success"] += 1
                self.build_tracker.record_built_package(pkg_name, cached_version, is_aur=True)
            else:
                self.stats["local_success"] += 1
                self.build_tracker.record_built_package(pkg_name, cached_version, is_aur=False)
            
            return True
        
        # If not cached, proceed with normal build
        if is_aur:
            return self._build_aur_package(pkg_name)
        else:
            return self._build_local_package(pkg_name)
    
    def build_packages(self) -> int:
        """Build packages with cache-aware optimization"""
        print("\n" + "=" * 60)
        print("Building packages (Cache-aware)")
        print("=" * 60)
        
        local_packages, aur_packages = self.get_package_lists()
        
        print(f"üì¶ Package statistics:")
        print(f"   Local packages: {len(local_packages)}")
        print(f"   AUR packages: {len(aur_packages)}")
        print(f"   Total packages: {len(local_packages) + len(aur_packages)}")
        print(f"   Cache enabled: {self.use_cache}")
        
        print(f"\nüî® Building {len(aur_packages)} AUR packages")
        for pkg in aur_packages:
            if self._build_single_package(pkg, is_aur=True):
                # Success already recorded in _build_single_package
                pass
            else:
                self.stats["aur_failed"] += 1
                self.build_tracker.record_failed_package(is_aur=True)
        
        print(f"\nüî® Building {len(local_packages)} local packages")
        for pkg in local_packages:
            if self._build_single_package(pkg, is_aur=False):
                # Success already recorded in _build_single_package
                pass
            else:
                self.stats["local_failed"] += 1
                self.build_tracker.record_failed_package(is_aur=False)
        
        return self.stats["aur_success"] + self.stats["local_success"]
    
    def upload_packages(self) -> bool:
        """Upload packages to server using RSYNC WITHOUT --delete flag"""
        # Get all package files and database files
        pkg_files = list(self.output_dir.glob("*.pkg.tar.*"))
        db_files = list(self.output_dir.glob(f"{self.repo_name}.*"))
        
        all_files = pkg_files + db_files
        
        if not all_files:
            logger.warning("No files to upload")
            self.version_tracker.set_upload_successful(False)
            return False
        
        # Ensure remote directory exists
        self.ssh_client.ensure_remote_directory()
        
        # Collect files using glob patterns
        file_patterns = [
            str(self.output_dir / "*.pkg.tar.*"),
            str(self.output_dir / f"{self.repo_name}.*")
        ]
        
        files_to_upload = []
        for pattern in file_patterns:
            files_to_upload.extend(glob.glob(pattern))
        
        if not files_to_upload:
            logger.error("No files found to upload!")
            self.version_tracker.set_upload_successful(False)
            return False
        
        # Upload files using Rsync client
        upload_success = self.rsync_client.upload_files(files_to_upload, self.output_dir)
        
        # Set upload success flag for cleanup
        self.version_tracker.set_upload_successful(upload_success)
        
        return upload_success
    
    def create_artifact_archive_for_github(self) -> Optional[Path]:
        """
        Create a tar.gz archive of all built artifacts for GitHub upload.
        This avoids issues with colon (:) characters in package filenames.
        """
        log_path = Path("builder.log")
        return self.artifact_manager.create_artifact_archive(self.output_dir, log_path)
    
    def run(self):
        """Main execution with cache-aware building"""
        print("\n" + "=" * 60)
        print("üöÄ MANJARO PACKAGE BUILDER (MODULAR ARCHITECTURE WITH CACHE)")
        print("=" * 60)
        
        try:
            print("\nüîß Initial setup...")
            print(f"Repository root: {self.repo_root}")
            print(f"Repository name: {self.repo_name}")
            print(f"Output directory: {self.output_dir}")
            print(f"PACKAGER identity: {self.packager_id}")
            print(f"Cache optimization: {'ENABLED' if self.use_cache else 'DISABLED'}")
            
            # Display initial cache status
            if self.use_cache:
                built_count = len(list(self.output_dir.glob("*.pkg.tar.*")))
                print(f"üì¶ Initial cache contains {built_count} package files")
            
            # STEP 0: Initialize GPG FIRST if enabled
            print("\n" + "=" * 60)
            print("STEP 0: GPG INITIALIZATION")
            print("=" * 60)
            if self.gpg_handler.gpg_enabled:
                if not self.gpg_handler.import_gpg_key():
                    logger.error("‚ùå Failed to import GPG key, disabling signing")
                else:
                    logger.info("‚úÖ GPG initialized successfully")
            else:
                logger.info("‚ÑπÔ∏è GPG signing disabled (no key provided)")
            
            # STEP 1: SIMPLIFIED REPOSITORY DISCOVERY
            print("\n" + "=" * 60)
            print("STEP 1: SIMPLIFIED REPOSITORY STATE DISCOVERY")
            print("=" * 60)
            
            # Check if repository exists on VPS
            repo_exists, has_packages = self.ssh_client.check_repository_exists_on_vps()
            
            # Apply repository state based on discovery
            self._apply_repository_state(repo_exists, has_packages)
            
            # Ensure remote directory exists
            self.ssh_client.ensure_remote_directory()
            
            # STEP 2: List remote packages for version comparison
            remote_packages = self.ssh_client.list_remote_packages()
            self.remote_files = [os.path.basename(f) for f in remote_packages] if remote_packages else []
            
            # MANDATORY STEP: Mirror ALL remote packages locally before any database operations
            # But check cache first
            if remote_packages:
                print("\n" + "=" * 60)
                print("MANDATORY PRECONDITION: Mirroring remote packages locally")
                print("=" * 60)
                
                # Check if we already have cached mirror
                cached_mirror_files = list(self.mirror_temp_dir.glob("*.pkg.tar.*"))
                if cached_mirror_files and self.use_cache:
                    print(f"üì¶ Using cached VPS mirror with {len(cached_mirror_files)} files")
                    # Copy cached files to output directory
                    for cached_file in cached_mirror_files:
                        dest = self.output_dir / cached_file.name
                        if not dest.exists():
                            shutil.copy2(cached_file, dest)
                else:
                    # No cache, perform fresh mirror
                    if not self.rsync_client.mirror_remote_packages(self.mirror_temp_dir, self.output_dir):
                        logger.error("‚ùå FAILED to mirror remote packages locally")
                        logger.error("Cannot proceed without local package mirror")
                        return 1
            else:
                logger.info("‚ÑπÔ∏è No remote packages to mirror (repository appears empty)")
            
            # STEP 3: Check existing database files
            existing_db_files, missing_db_files = self.database_manager.check_database_files()
            
            # Fetch existing database if available
            if existing_db_files:
                self.database_manager.fetch_existing_database(existing_db_files)
            
            # Build packages with cache optimization
            print("\n" + "=" * 60)
            print("STEP 5: PACKAGE BUILDING (CACHE-AWARE SRCINFO VERSIONING)")
            print("=" * 60)
            
            total_built = self.build_packages()
            
            # Check if we have any packages locally (mirrored + newly built)
            local_packages = self.database_manager._get_all_local_packages()
            
            if local_packages or remote_packages:
                print("\n" + "=" * 60)
                print("STEP 6: REPOSITORY DATABASE HANDLING (WITH LOCAL MIRROR)")
                print("=" * 60)
                
                # ZERO-RESIDUE FIX: Perform server cleanup BEFORE database generation
                print("\n" + "=" * 60)
                print("üö® PRE-DATABASE CLEANUP: Removing zombie packages from server")
                print("=" * 60)
                self.cleanup_manager.server_cleanup(self.version_tracker)
                
                # Generate database with ALL locally available packages
                if self.database_manager.generate_full_database(self.repo_name, self.output_dir, self.cleanup_manager):
                    # Sign repository database files if GPG is enabled
                    if self.gpg_handler.gpg_enabled:
                        if not self.gpg_handler.sign_repository_files(self.repo_name, str(self.output_dir)):
                            logger.warning("‚ö†Ô∏è Failed to sign repository files, continuing anyway")
                    
                    # Upload regenerated database and packages
                    if not self.ssh_client.test_ssh_connection():
                        logger.warning("SSH test failed, but trying upload anyway...")
                    
                    # Upload everything (packages + database + signatures)
                    upload_success = self.upload_packages()
                    
                    # ZERO-RESIDUE FIX: Perform final server cleanup AFTER upload
                    if upload_success:
                        print("\n" + "=" * 60)
                        print("üö® POST-UPLOAD CLEANUP: Final zombie package removal")
                        print("=" * 60)
                        self.cleanup_manager.server_cleanup(self.version_tracker)
                    
                    # Clean up GPG temporary directory
                    self.gpg_handler.cleanup()
                    
                    if upload_success:
                        # STEP 7: Update repository state and sync pacman
                        print("\n" + "=" * 60)
                        print("STEP 7: FINAL REPOSITORY STATE UPDATE")
                        print("=" * 60)
                        
                        # Re-check repository state (it should exist now)
                        repo_exists, has_packages = self.ssh_client.check_repository_exists_on_vps()
                        self._apply_repository_state(repo_exists, has_packages)
                        
                        # Sync pacman databases
                        self._sync_pacman_databases()
                        
                        print("\n‚úÖ Build completed successfully!")
                    else:
                        print("\n‚ùå Upload failed!")
                else:
                    print("\n‚ùå Database generation failed!")
            else:
                print("\nüìä Build summary:")
                print(f"   AUR packages built: {self.stats['aur_success']}")
                print(f"   AUR packages failed: {self.stats['aur_failed']}")
                print(f"   Local packages built: {self.stats['local_success']}")
                print(f"   Local packages failed: {self.stats['local_failed']}")
                print(f"   Total skipped: {len(self.skipped_packages)}")
                print(f"   Cache hits: {self.stats['cache_hits']}")
                print(f"   Cache misses: {self.stats['cache_misses']}")
                
                if self.stats['aur_failed'] > 0 or self.stats['local_failed'] > 0:
                    print("‚ö†Ô∏è Some packages failed to build")
                else:
                    print("‚úÖ All packages are up to date or built successfully!")
                
                # Clean up GPG even if no packages built
                self.gpg_handler.cleanup()
            
            # STEP 8: Create artifact archive for GitHub upload
            print("\n" + "=" * 60)
            print("STEP 8: CREATING ARTIFACT ARCHIVE FOR GITHUB")
            print("=" * 60)
            
            artifact_archive = self.create_artifact_archive_for_github()
            if artifact_archive:
                logger.info(f"‚úÖ Artifact archive created: {artifact_archive.name}")
                logger.info("üì¶ Upload this archive to GitHub to avoid colon character issues")
            else:
                logger.warning("‚ö†Ô∏è Failed to create artifact archive")
            
            elapsed = time.time() - self.stats["start_time"]
            summary = self.build_tracker.get_summary()
            
            print("\n" + "=" * 60)
            print("üìä BUILD SUMMARY WITH CACHE STATISTICS")
            print("=" * 60)
            print(f"Duration: {elapsed:.1f}s")
            print(f"AUR packages:    {summary['aur_success']} (failed: {summary['aur_failed']})")
            print(f"Local packages:  {summary['local_success']} (failed: {summary['local_failed']})")
            print(f"Total built:     {summary['total_built']}")
            print(f"Skipped:         {summary['skipped']}")
            print(f"Cache hits:      {self.stats['cache_hits']}")
            print(f"Cache misses:    {self.stats['cache_misses']}")
            print(f"Cache efficiency: {self.stats['cache_hits']/(self.stats['cache_hits']+self.stats['cache_misses'])*100:.1f}%")
            print(f"GPG signing:     {'Enabled' if self.gpg_handler.gpg_enabled else 'Disabled'}")
            print(f"PACKAGER:        {self.packager_id}")
            print(f"Zero-Residue:    ‚úÖ Exact-filename-match cleanup active")
            print(f"Target Version:  ‚úÖ Package target versions registered: {len(self.version_tracker._package_target_versions)}")
            print(f"Skipped Registry:‚úÖ Skipped packages tracked: {len(self.version_tracker._skipped_packages)}")
            print("=" * 60)
            
            if self.built_packages:
                print("\nüì¶ Built packages:")
                for pkg in self.built_packages:
                    print(f"  - {pkg}")
            
            return 0
            
        except Exception as e:
            print(f"\n‚ùå Build failed: {e}")
            import traceback
            traceback.print_exc()
            # Ensure GPG cleanup even on failure
            if hasattr(self, 'gpg_handler'):
                self.gpg_handler.cleanup()
            return 1

--- FILE: .github/scripts/modules/repo/__init__.py ---
"""
Repository management module
"""
--- FILE: .github/scripts/modules/repo/cleanup_manager.py ---
"""
Cleanup Manager Module - Handles Zero-Residue policy and package cleanup
"""

import os
import subprocess
import shutil
import re
import logging
from pathlib import Path
from typing import List, Set, Tuple, Optional, Dict

logger = logging.getLogger(__name__)


class CleanupManager:
    """Manages package cleanup and Zero-Residue policy implementation"""
    
    def __init__(self, config: dict):
        """
        Initialize CleanupManager with configuration
        
        Args:
            config: Dictionary containing:
                - repo_name: Repository name
                - output_dir: Local output directory (SOURCE OF TRUTH)
                - remote_dir: Remote directory on VPS
                - mirror_temp_dir: Temporary mirror directory
                - vps_user: VPS username
                - vps_host: VPS hostname
        """
        self.repo_name = config['repo_name']
        self.output_dir = Path(config['output_dir'])
        self.remote_dir = config['remote_dir']
        self.mirror_temp_dir = Path(config.get('mirror_temp_dir', '/tmp/repo_mirror'))
        self.vps_user = config['vps_user']
        self.vps_host = config['vps_host']
    
    def pre_build_purge_old_versions(self, pkg_name: str, old_version: str, target_version: Optional[str] = None):
        """
        üö® ZERO-RESIDUE POLICY: Surgical old version removal BEFORE building
        
        Removes old versions from local output directory before new build.
        
        Args:
            pkg_name: Package name
            old_version: Version to potentially delete
            target_version: Version we want to keep (None if building new)
        """
        # If we have a registered target version, use it
        # Note: This method needs access to version_tracker, will be called from PackageBuilder
        # with version_tracker passed as parameter
        pass
    
    def _delete_specific_version_local(self, pkg_name: str, version_to_delete: str):
        """Delete a specific version of a package from local output_dir"""
        patterns = self._version_to_patterns(pkg_name, version_to_delete)
        deleted_count = 0
        
        for pattern in patterns:
            for old_file in self.output_dir.glob(pattern):
                try:
                    # Verify this is actually the version we want to delete
                    extracted_version = self._extract_version_from_filename(old_file.name, pkg_name)
                    if extracted_version == version_to_delete:
                        old_file.unlink()
                        logger.info(f"üóëÔ∏è Surgically removed local {old_file.name}")
                        deleted_count += 1
                        
                        # Also remove signature
                        sig_file = old_file.with_suffix(old_file.suffix + '.sig')
                        if sig_file.exists():
                            sig_file.unlink()
                            logger.info(f"üóëÔ∏è Removed local signature {sig_file.name}")
                except Exception as e:
                    logger.warning(f"Could not delete local {old_file}: {e}")
        
        if deleted_count > 0:
            logger.info(f"‚úÖ Removed {deleted_count} local files for {pkg_name} version {version_to_delete}")
    
    def revalidate_output_dir_before_database(self):
        """
        üî• ZOMBIE PROTECTION: Final validation before database generation
        
        Enhanced to recognize skipped packages as legitimate (not zombies)
        
        Scans output_dir and ensures:
        1. Only one version per package exists
        2. If multiple versions exist, keep only the target version
        3. Delete any "zombie" files (old versions that shouldn't be there)
        
        This is the LAST CHANCE to clean up before repo-add runs.
        """
        print("\n" + "=" * 60)
        print("üö® FINAL VALIDATION: Removing zombie packages from output_dir")
        print("=" * 60)
        
        # Get all package files in output_dir
        package_files = list(self.output_dir.glob("*.pkg.tar.*"))
        
        if not package_files:
            logger.info("‚ÑπÔ∏è No package files in output_dir to validate")
            return
        
        logger.info(f"üîç Validating {len(package_files)} package files in output_dir...")
        
        # Group files by package name
        packages_dict: Dict[str, List[Tuple[str, Path]]] = {}
        
        for pkg_file in package_files:
            # Extract package name and version from filename
            extracted = self._parse_package_filename(pkg_file.name)
            if extracted:
                pkg_name, version_str = extracted
                if pkg_name not in packages_dict:
                    packages_dict[pkg_name] = []
                packages_dict[pkg_name].append((version_str, pkg_file))
        
        # Process each package
        total_deleted = 0
        
        for pkg_name, files in packages_dict.items():
            if len(files) > 1:
                logger.warning(f"‚ö†Ô∏è Multiple versions found for {pkg_name}: {[v[0] for v in files]}")
                
                # Check if we have a registered target version
                # Note: This method needs access to version_tracker, will be called from PackageBuilder
                # with version_tracker passed as parameter
                target_version = None  # Will be set by caller
        
        if total_deleted > 0:
            logger.info(f"üéØ Final validation: Removed {total_deleted} zombie package files")
        else:
            logger.info("‚úÖ Output_dir validation passed - no zombie packages found")
    
    def _parse_package_filename(self, filename: str) -> Optional[Tuple[str, str]]:
        """Parse package filename to extract name and version"""
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            # The package name is everything before the last 3 parts (version-release-arch)
            # or last 4 parts (epoch-version-release-arch)
            if len(parts) >= 4:
                # Try to find where package name ends
                for i in range(len(parts) - 3, 0, -1):
                    potential_name = '-'.join(parts[:i])
                    
                    # Check if remaining parts look like version-release-arch
                    remaining = parts[i:]
                    if len(remaining) >= 3:
                        # Check for epoch format (e.g., "2-26.1.9-1-x86_64")
                        if remaining[0].isdigit() and '-' in '-'.join(remaining[1:]):
                            epoch = remaining[0]
                            version_part = remaining[1]
                            release_part = remaining[2]
                            version_str = f"{epoch}:{version_part}-{release_part}"
                            return potential_name, version_str
                        # Standard format (e.g., "26.1.9-1-x86_64")
                        elif any(c.isdigit() for c in remaining[0]) and remaining[1].isdigit():
                            version_part = remaining[0]
                            release_part = remaining[1]
                            version_str = f"{version_part}-{release_part}"
                            return potential_name, version_str
        except Exception as e:
            logger.debug(f"Could not parse filename {filename}: {e}")
        
        return None
    
    def _version_to_patterns(self, pkg_name: str, version: str) -> List[str]:
        """Convert version string to filename patterns"""
        patterns = []
        
        if ':' in version:
            # Version with epoch: "2:26.1.9-1" -> "2-26.1.9-1-*.pkg.tar.*"
            epoch, rest = version.split(':', 1)
            patterns.append(f"{pkg_name}-{epoch}-{rest}-*.pkg.tar.*")
        else:
            # Standard version: "26.1.9-1" -> "*26.1.9-1-*.pkg.tar.*"
            patterns.append(f"{pkg_name}-{version}-*.pkg.tar.*")
        
        return patterns
    
    def _extract_version_from_filename(self, filename: str, pkg_name: str) -> Optional[str]:
        """
        Extract version from package filename
        
        Args:
            filename: Package filename (e.g., 'qownnotes-26.1.9-1-x86_64.pkg.tar.zst')
            pkg_name: Package name (e.g., 'qownnotes')
        
        Returns:
            Version string (e.g., '26.1.9-1') or None if cannot parse
        """
        try:
            # Remove extensions
            base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
            parts = base.split('-')
            
            # Find where package name ends
            for i in range(len(parts) - 2, 0, -1):
                possible_name = '-'.join(parts[:i])
                if possible_name == pkg_name or possible_name.startswith(pkg_name + '-'):
                    # Remaining parts: version-release-architecture
                    if len(parts) >= i + 3:
                        version_part = parts[i]
                        release_part = parts[i+1]
                        
                        # Check for epoch (e.g., "2-26.1.9-1" -> "2:26.1.9-1")
                        if i + 2 < len(parts) and parts[i].isdigit():
                            epoch_part = parts[i]
                            version_part = parts[i+1]
                            release_part = parts[i+2]
                            return f"{epoch_part}:{version_part}-{release_part}"
                        else:
                            return f"{version_part}-{release_part}"
        except Exception as e:
            logger.debug(f"Could not extract version from {filename}: {e}")
        
        return None
    
    def _find_latest_version(self, versions: List[str]) -> str:
        """
        Find the latest version from a list using vercmp
        
        Args:
            versions: List of version strings
        
        Returns:
            The latest version string
        """
        if not versions:
            return ""
        
        if len(versions) == 1:
            return versions[0]
        
        # Try to use vercmp for accurate comparison
        try:
            latest = versions[0]
            for i in range(1, len(versions)):
                result = subprocess.run(
                    ['vercmp', versions[i], latest],
                    capture_output=True,
                    text=True,
                    check=False
                )
                if result.returncode == 0:
                    cmp_result = int(result.stdout.strip())
                    if cmp_result > 0:
                        latest = versions[i]
            
            return latest
        except Exception as e:
            # Fallback: use string comparison (less accurate but works for simple cases)
            logger.warning(f"vercmp failed, using fallback version comparison: {e}")
            return max(versions)
    
    def server_cleanup(self, version_tracker):
        """
        üö® ZERO-RESIDUE SERVER CLEANUP: Remove zombie packages from VPS 
        using TARGET VERSIONS as SOURCE OF TRUTH.
        
        Only keeps packages that match registered target versions.
        """
        print("\n" + "=" * 60)
        print("üîí ZERO-RESIDUE SERVER CLEANUP: Target Versions are Source of Truth")
        print("=" * 60)
        
        # VALVE: Check if we have any target versions registered
        if not version_tracker._package_target_versions:
            logger.warning("‚ö†Ô∏è No target versions registered - skipping server cleanup")
            return
        
        logger.info(f"üîÑ Zero-Residue cleanup initiated with {len(version_tracker._package_target_versions)} target versions")
        
        # STEP 1: Get ALL files from VPS
        vps_files = self._get_vps_file_inventory()
        if vps_files is None:
            logger.error("‚ùå Failed to get VPS file inventory")
            return
        
        if not vps_files:
            logger.info("‚ÑπÔ∏è No files found on VPS - nothing to clean up")
            return
        
        # STEP 2: Identify files to keep based on target versions
        files_to_keep = set()
        files_to_delete = []
        
        for vps_file in vps_files:
            filename = Path(vps_file).name
            
            # Skip database and signature files from deletion logic
            is_db_or_sig = any(filename.endswith(ext) for ext in ['.db', '.db.tar.gz', '.sig', '.files', '.files.tar.gz'])
            if is_db_or_sig:
                files_to_keep.add(filename)
                continue
            
            # Parse package filename
            parsed = self._parse_package_filename(filename)
            if not parsed:
                # Can't parse, keep it to be safe
                files_to_keep.add(filename)
                continue
            
            pkg_name, version_str = parsed
            
            # Check if this package has a target version
            if pkg_name in version_tracker._package_target_versions:
                target_version = version_tracker._package_target_versions[pkg_name]
                if version_str == target_version:
                    # This is the version we want to keep
                    files_to_keep.add(filename)
                    logger.debug(f"‚úÖ Keeping {filename} (matches target version {target_version})")
                else:
                    # This is an old version - mark for deletion
                    files_to_delete.append(vps_file)
                    logger.info(f"üóëÔ∏è Marking for deletion: {filename} (target is {target_version})")
            else:
                # No target version registered for this package
                # Check if it's in our skipped packages
                if pkg_name in version_tracker._skipped_packages:
                    skipped_version = version_tracker._skipped_packages[pkg_name]
                    if version_str == skipped_version:
                        files_to_keep.add(filename)
                        logger.debug(f"‚úÖ Keeping {filename} (matches skipped version {skipped_version})")
                    else:
                        files_to_delete.append(vps_file)
                        logger.info(f"üóëÔ∏è Marking for deletion: {filename} (not in target versions)")
                else:
                    # Not in target versions or skipped packages - keep to be safe
                    files_to_keep.add(filename)
                    logger.warning(f"‚ö†Ô∏è Keeping unknown package: {filename} (not in target versions)")
        
        # STEP 3: Execute deletion
        if not files_to_delete:
            logger.info("‚úÖ No zombie packages found on VPS")
            return
        
        logger.warning(f"üö® Identified {len(files_to_delete)} zombie packages for deletion")
        
        # Delete files in batches to avoid command line length limits
        batch_size = 50
        deleted_count = 0
        
        for i in range(0, len(files_to_delete), batch_size):
            batch = files_to_delete[i:i + batch_size]
            if self._delete_files_remote(batch):
                deleted_count += len(batch)
        
        logger.info(f"üìä Server cleanup complete: Deleted {deleted_count} zombie packages, kept {len(files_to_keep)} files")
    
    def _get_vps_file_inventory(self) -> Optional[List[str]]:
        """Get complete inventory of all files on VPS"""
        logger.info("üìã Getting complete VPS file inventory...")
        remote_cmd = rf"""
        # Get all package files, signatures, and database files
        find "{self.remote_dir}" -maxdepth 1 -type f \( -name "*.pkg.tar.zst" -o -name "*.pkg.tar.xz" -o -name "*.sig" -o -name "*.db" -o -name "*.db.tar.gz" -o -name "*.files" -o -name "*.files.tar.gz" -o -name "*.abs.tar.gz" \) 2>/dev/null
        """
        
        ssh_cmd = [
            "ssh",
            f"{self.vps_user}@{self.vps_host}",
            remote_cmd
        ]
        
        try:
            result = subprocess.run(
                ssh_cmd,
                capture_output=True,
                text=True,
                check=False,
                timeout=30
            )
            
            if result.returncode != 0:
                logger.warning(f"Could not list VPS files: {result.stderr[:200]}")
                return None
            
            vps_files_raw = result.stdout.strip()
            if not vps_files_raw:
                logger.info("No files found on VPS - nothing to clean up")
                return []
            
            vps_files = [f.strip() for f in vps_files_raw.split('\n') if f.strip()]
            logger.info(f"Found {len(vps_files)} files on VPS")
            return vps_files
            
        except subprocess.TimeoutExpired:
            logger.error("‚ùå SSH timeout getting VPS file inventory")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error getting VPS file inventory: {e}")
            return None
    
    def _delete_files_remote(self, files_to_delete: List[str]) -> bool:
        """Delete files from remote server"""
        if not files_to_delete:
            return True
        
        # Quote each filename for safety
        quoted_files = [f"'{f}'" for f in files_to_delete]
        files_to_delete_str = ' '.join(quoted_files)
        
        delete_cmd = f"rm -fv {files_to_delete_str}"
        
        logger.info(f"üöÄ Executing deletion command for {len(files_to_delete)} files")
        
        # Execute the deletion command
        ssh_delete = [
            "ssh",
            f"{self.vps_user}@{self.vps_host}",
            delete_cmd
        ]
        
        try:
            result = subprocess.run(
                ssh_delete,
                capture_output=True,
                text=True,
                check=False,
                timeout=60
            )
            
            if result.returncode == 0:
                logger.info(f"‚úÖ Deletion successful for batch of {len(files_to_delete)} files")
                if result.stdout:
                    for line in result.stdout.splitlines():
                        if "removed" in line.lower():
                            logger.info(f"   {line}")
                return True
            else:
                logger.error(f"‚ùå Deletion failed: {result.stderr[:500]}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("‚ùå SSH command timed out - aborting cleanup for safety")
            return False
        except Exception as e:
            logger.error(f"‚ùå Error during deletion: {e}")
            return False
--- FILE: .github/scripts/modules/repo/database_manager.py ---
"""
Database Manager Module - Handles repository database operations
"""

import os
import subprocess
import shutil
import logging
from pathlib import Path
from typing import List, Tuple

logger = logging.getLogger(__name__)


class DatabaseManager:
    """Manages repository database operations"""
    
    def __init__(self, config: dict):
        """
        Initialize DatabaseManager with configuration
        
        Args:
            config: Dictionary containing:
                - repo_name: Repository name
                - output_dir: Local output directory
                - remote_dir: Remote directory on VPS
                - vps_user: VPS username
                - vps_host: VPS hostname
        """
        self.repo_name = config['repo_name']
        self.output_dir = Path(config['output_dir'])
        self.remote_dir = config['remote_dir']
        self.vps_user = config['vps_user']
        self.vps_host = config['vps_host']
    
    def generate_full_database(self, repo_name: str, output_dir: Path, cleanup_manager) -> bool:
        """
        Generate repository database from ALL locally available packages
        
        üö® KRITIKUS: Run final validation BEFORE repo-add
        """
        print("\n" + "=" * 60)
        print("PHASE: Repository Database Generation")
        print("=" * 60)
        
        # üö® KRITIKUS: Final validation to remove zombie packages
        cleanup_manager.revalidate_output_dir_before_database()
        
        # Get all package files from local output directory
        all_packages = self._get_all_local_packages()
        
        if not all_packages:
            logger.info("No packages available for database generation")
            return False
        
        logger.info(f"Generating database with {len(all_packages)} packages...")
        logger.info(f"Packages: {', '.join(all_packages[:10])}{'...' if len(all_packages) > 10 else ''}")
        
        old_cwd = os.getcwd()
        os.chdir(self.output_dir)
        
        try:
            db_file = f"{repo_name}.db.tar.gz"
            
            # Clean old database files
            for f in [f"{repo_name}.db", f"{repo_name}.db.tar.gz", 
                      f"{repo_name}.files", f"{repo_name}.files.tar.gz"]:
                if os.path.exists(f):
                    os.remove(f)
            
            # Verify each package file exists locally before database generation
            missing_packages = []
            valid_packages = []
            
            for pkg_filename in all_packages:
                if Path(pkg_filename).exists():
                    valid_packages.append(pkg_filename)
                else:
                    missing_packages.append(pkg_filename)
            
            if missing_packages:
                logger.error(f"‚ùå CRITICAL: {len(missing_packages)} packages missing locally:")
                for pkg in missing_packages[:5]:
                    logger.error(f"   - {pkg}")
                if len(missing_packages) > 5:
                    logger.error(f"   ... and {len(missing_packages) - 5} more")
                return False
            
            if not valid_packages:
                logger.error("No valid package files found for database generation")
                return False
            
            logger.info(f"‚úÖ All {len(valid_packages)} package files verified locally")
            
            # Generate database with repo-add using shell=True for wildcard expansion
            cmd = f"repo-add {db_file} *.pkg.tar.zst"
            
            logger.info(f"Running repo-add with shell=True to include ALL packages...")
            logger.info(f"Command: {cmd}")
            logger.info(f"Current directory: {os.getcwd()}")
            
            result = subprocess.run(
                cmd,
                shell=True,  # CRITICAL: Use shell=True for wildcard expansion
                capture_output=True,
                text=True,
                check=False
            )
            
            if result.returncode == 0:
                logger.info("‚úÖ Database created successfully")
                
                # Verify the database was created
                db_path = Path(db_file)
                if db_path.exists():
                    size_mb = db_path.stat().st_size / (1024 * 1024)
                    logger.info(f"Database size: {size_mb:.2f} MB")
                    
                    # CRITICAL: Verify database entries BEFORE upload
                    logger.info("üîç Verifying database entries before upload...")
                    list_cmd = ["tar", "-tzf", db_file]
                    list_result = subprocess.run(list_cmd, capture_output=True, text=True, check=False)
                    if list_result.returncode == 0:
                        db_entries = [line for line in list_result.stdout.split('\n') if line.endswith('/desc')]
                        logger.info(f"‚úÖ Database contains {len(db_entries)} package entries")
                        if len(db_entries) == 0:
                            logger.error("‚ùå‚ùå‚ùå DATABASE IS EMPTY! This is the root cause of the issue.")
                            return False
                        else:
                            logger.info(f"Sample database entries: {db_entries[:5]}")
                    else:
                        logger.warning(f"Could not list database contents: {list_result.stderr}")
                
                return True
            else:
                logger.error(f"repo-add failed with exit code {result.returncode}:")
                if result.stdout:
                    logger.error(f"STDOUT: {result.stdout[:500]}")
                if result.stderr:
                    logger.error(f"STDERR: {result.stderr[:500]}")
                return False
                
        finally:
            os.chdir(old_cwd)
    
    def _get_all_local_packages(self) -> List[str]:
        """Get ALL package files from local output directory (mirrored + newly built)"""
        print("\nüîç Getting complete package list from local directory...")
        
        local_files = list(self.output_dir.glob("*.pkg.tar.*"))
        
        if not local_files:
            logger.info("‚ÑπÔ∏è No package files found locally")
            return []
        
        local_filenames = [f.name for f in local_files]
        
        logger.info(f"üìä Local package count: {len(local_filenames)}")
        logger.info(f"Sample packages: {local_filenames[:10]}")
        
        return local_filenames
    
    def check_database_files(self) -> Tuple[List[str], List[str]]:
        """Check if repository database files exist on server"""
        print("\n" + "=" * 60)
        print("STEP 2: Checking existing database files on server")
        print("=" * 60)
        
        db_files = [
            f"{self.repo_name}.db",
            f"{self.repo_name}.db.tar.gz",
            f"{self.repo_name}.files",
            f"{self.repo_name}.files.tar.gz"
        ]
        
        existing_files = []
        missing_files = []
        
        for db_file in db_files:
            remote_cmd = f"test -f {self.remote_dir}/{db_file} && echo 'EXISTS' || echo 'MISSING'"
            
            ssh_cmd = [
                "ssh",
                f"{self.vps_user}@{self.vps_host}",
                remote_cmd
            ]
            
            try:
                result = subprocess.run(
                    ssh_cmd,
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                if result.returncode == 0 and "EXISTS" in result.stdout:
                    existing_files.append(db_file)
                    logger.info(f"‚úÖ Database file exists: {db_file}")
                else:
                    missing_files.append(db_file)
                    logger.info(f"‚ÑπÔ∏è Database file missing: {db_file}")
                    
            except Exception as e:
                logger.warning(f"Could not check {db_file}: {e}")
                missing_files.append(db_file)
        
        if existing_files:
            logger.info(f"Found {len(existing_files)} database files on server")
        else:
            logger.info("No database files found on server")
        
        return existing_files, missing_files
    
    def fetch_existing_database(self, existing_files: List[str]):
        """Fetch existing database files from server"""
        if not existing_files:
            return
        
        print("\nüì• Fetching existing database files from server...")
        
        for db_file in existing_files:
            remote_path = f"{self.remote_dir}/{db_file}"
            local_path = self.output_dir / db_file
            
            # Remove local copy if exists
            if local_path.exists():
                local_path.unlink()
            
            ssh_cmd = [
                "scp",
                "-o", "StrictHostKeyChecking=no",
                "-o", "ConnectTimeout=30",
                f"{self.vps_user}@{self.vps_host}:{remote_path}",
                str(local_path)
            ]
            
            try:
                result = subprocess.run(
                    ssh_cmd,
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                if result.returncode == 0 and local_path.exists():
                    size_mb = local_path.stat().st_size / (1024 * 1024)
                    logger.info(f"‚úÖ Fetched: {db_file} ({size_mb:.2f} MB)")
                else:
                    logger.warning(f"‚ö†Ô∏è Could not fetch {db_file}")
            except Exception as e:
                logger.warning(f"Could not fetch {db_file}: {e}")
--- FILE: .github/scripts/modules/repo/recovery_manager.py ---
"""
Recovery Manager Module - Handles repository recovery operations
"""

import logging

logger = logging.getLogger(__name__)


class RecoveryManager:
    """Handles repository recovery operations"""
    
    def __init__(self, config: dict):
        """
        Initialize RecoveryManager with configuration
        
        Args:
            config: Dictionary containing repository configuration
        """
        self.repo_name = config.get('repo_name', '')
        self.output_dir = config.get('output_dir', '')
    
    def recover_from_backup(self, backup_path: str) -> bool:
        """Recover repository from backup"""
        logger.info(f"Attempting recovery from backup: {backup_path}")
        # Implementation would go here
        return False
    
    def validate_repository_integrity(self) -> bool:
        """Validate repository integrity"""
        logger.info("Validating repository integrity")
        # Implementation would go here
        return True
--- FILE: .github/scripts/modules/repo/version_tracker.py ---
"""
Version Tracker Module - Handles package version tracking and comparison
"""

import re
import logging
from typing import Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


class VersionTracker:
    """Handles package version tracking, comparison, and Zero-Residue policy"""
    
    def __init__(self, config: dict):
        """
        Initialize VersionTracker with configuration
        
        Args:
            config: Dictionary containing:
                - repo_name: Repository name
                - output_dir: Local output directory (SOURCE OF TRUTH)
                - remote_dir: Remote directory on VPS
                - mirror_temp_dir: Temporary mirror directory
                - vps_user: VPS username
                - vps_host: VPS hostname
        """
        self.repo_name = config['repo_name']
        self.output_dir = config['output_dir']
        self.remote_dir = config['remote_dir']
        self.mirror_temp_dir = config.get('mirror_temp_dir', '/tmp/repo_mirror')
        self.vps_user = config['vps_user']
        self.vps_host = config['vps_host']
        
        # üö® ZERO-RESIDUE POLICY: Explicit version tracking
        self._skipped_packages: Dict[str, str] = {}  # {pkg_name: remote_version} - packages skipped as up-to-date
        self._package_target_versions: Dict[str, str] = {}  # {pkg_name: target_version} - versions we want to keep
        self._built_packages: Dict[str, str] = {}  # {pkg_name: built_version} - packages we just built
        self._upload_successful = False
    
    def set_upload_successful(self, successful: bool):
        """Set the upload success flag for safety valve"""
        self._upload_successful = successful
    
    def register_package_target_version(self, pkg_name: str, target_version: str):
        """
        Register the target version for a package.
        
        Args:
            pkg_name: Package name
            target_version: The version we want to keep (either built or latest from server)
        """
        self._package_target_versions[pkg_name] = target_version
        logger.info(f"üìù Registered target version for {pkg_name}: {target_version}")
    
    def register_skipped_package(self, pkg_name: str, remote_version: str):
        """
        Register a package that was skipped because it's up-to-date.
        
        Args:
            pkg_name: Package name
            remote_version: The remote version that should be kept (not deleted)
        """
        # Store in skipped registry
        self._skipped_packages[pkg_name] = remote_version
        
        # üö® CRITICAL: Explicitly set target version to remote version
        self._package_target_versions[pkg_name] = remote_version
        
        logger.info(f"üìù Registered SKIPPED package: {pkg_name} (remote: {remote_version}, target: {remote_version})")
    
    def package_exists(self, pkg_name: str, remote_files: List[str]) -> bool:
        """Check if package exists on server"""
        if not remote_files:
            return False
        
        pattern = f"^{re.escape(pkg_name)}-"
        matches = [f for f in remote_files if re.match(pattern, f)]
        
        if matches:
            logger.debug(f"Package {pkg_name} exists: {matches[0]}")
            return True
        
        return False
    
    def get_remote_version(self, pkg_name: str, remote_files: List[str]) -> Optional[str]:
        """Get the version of a package from remote server using SRCINFO-based extraction"""
        if not remote_files:
            return None
        
        # Look for any file with this package name
        for filename in remote_files:
            if filename.startswith(f"{pkg_name}-"):
                # Extract version from filename
                base = filename.replace('.pkg.tar.zst', '').replace('.pkg.tar.xz', '')
                parts = base.split('-')
                
                # Find where the package name ends
                for i in range(len(parts) - 2, 0, -1):
                    possible_name = '-'.join(parts[:i])
                    if possible_name == pkg_name or possible_name.startswith(pkg_name + '-'):
                        if len(parts) >= i + 3:
                            version_part = parts[i]
                            release_part = parts[i+1]
                            if i + 1 < len(parts) and parts[i].isdigit() and i + 2 < len(parts):
                                epoch_part = parts[i]
                                version_part = parts[i+1]
                                release_part = parts[i+2]
                                return f"{epoch_part}:{version_part}-{release_part}"
                            else:
                                return f"{version_part}-{release_part}"
        
        return None
--- FILE: .github/scripts/modules/scm/__init__.py ---
"""
Source Code Management module
"""
--- FILE: .github/scripts/modules/scm/git_client.py ---
"""
Git Client Module - Handles Git operations
"""

import subprocess
import logging

logger = logging.getLogger(__name__)


class GitClient:
    """Handles Git operations for repository management"""
    
    def __init__(self, repo_url: str, ssh_options: list = None):
        self.repo_url = repo_url
        self.ssh_options = ssh_options or []
    
    def clone_repository(self, target_dir: str, depth: int = 1) -> bool:
        """Clone a Git repository"""
        cmd = ["git", "clone", "--depth", str(depth)]
        
        # Add SSH options if provided
        if self.ssh_options:
            ssh_cmd = " ".join(self.ssh_options)
            cmd.extend(["-c", f"core.sshCommand=ssh {ssh_cmd}"])
        
        cmd.extend([self.repo_url, target_dir])
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=False)
            if result.returncode == 0:
                logger.info(f"‚úÖ Successfully cloned repository to {target_dir}")
                return True
            else:
                logger.error(f"‚ùå Failed to clone repository: {result.stderr}")
                return False
        except Exception as e:
            logger.error(f"‚ùå Error cloning repository: {e}")
            return False
    
    def pull_latest(self, repo_dir: str) -> bool:
        """Pull latest changes from remote repository"""
        cmd = ["git", "-C", repo_dir, "pull"]
        
        # Add SSH options if provided
        if self.ssh_options:
            ssh_cmd = " ".join(self.ssh_options)
            cmd.extend(["-c", f"core.sshCommand=ssh {ssh_cmd}"])
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=False)
            if result.returncode == 0:
                logger.info("‚úÖ Successfully pulled latest changes")
                return True
            else:
                logger.error(f"‚ùå Failed to pull latest changes: {result.stderr}")
                return False
        except Exception as e:
            logger.error(f"‚ùå Error pulling latest changes: {e}")
            return False
--- FILE: .github/scripts/modules/vps/__init__.py ---
"""
VPS module for remote server operations
"""
--- FILE: .github/scripts/modules/vps/db_manager.py ---
"""
Database Manager for VPS - Handles remote database operations
"""

import logging

logger = logging.getLogger(__name__)


class VPSDatabaseManager:
    """Handles remote database operations on VPS"""
    
    def __init__(self, config: dict):
        """
        Initialize VPSDatabaseManager with configuration
        
        Args:
            config: Dictionary containing VPS configuration
        """
        self.vps_user = config.get('vps_user', '')
        self.vps_host = config.get('vps_host', '')
        self.remote_dir = config.get('remote_dir', '')
    
    def check_remote_database(self) -> bool:
        """Check if remote database exists and is accessible"""
        logger.info("Checking remote database...")
        # Implementation would go here
        return True
    
    def backup_remote_database(self, backup_path: str) -> bool:
        """Backup remote database"""
        logger.info(f"Backing up remote database to {backup_path}")
        # Implementation would go here
        return True
--- FILE: .github/scripts/modules/vps/rsync_client.py ---
"""
Rsync Client Module - Handles file transfers using Rsync
"""

import os
import subprocess
import shutil
import time
import logging
from pathlib import Path
from typing import List

logger = logging.getLogger(__name__)


class RsyncClient:
    """Handles Rsync file transfers and remote operations"""
    
    def __init__(self, config: dict):
        """
        Initialize RsyncClient with configuration
        
        Args:
            config: Dictionary containing:
                - vps_user: VPS username
                - vps_host: VPS hostname
                - remote_dir: Remote directory on VPS
                - ssh_options: SSH options list
                - repo_name: Repository name
        """
        self.vps_user = config['vps_user']
        self.vps_host = config['vps_host']
        self.remote_dir = config['remote_dir']
        self.ssh_options = config.get('ssh_options', [])
        self.repo_name = config.get('repo_name', '')
    
    def mirror_remote_packages(self, mirror_temp_dir: Path, output_dir: Path) -> bool:
        """
        Download ALL remote package files to local directory
        
        Returns:
            True if successful, False otherwise
        """
        print("\n" + "=" * 60)
        print("MANDATORY STEP: Mirroring remote packages locally")
        print("=" * 60)
        
        # Ensure remote directory exists first
        # Note: This requires SSHClient, will be called from PackageBuilder
        
        # Create a temporary local repository directory
        if mirror_temp_dir.exists():
            shutil.rmtree(mirror_temp_dir, ignore_errors=True)
        mirror_temp_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Created local mirror directory: {mirror_temp_dir}")
        
        # Use rsync to download ALL package files from server
        print("üì• Downloading ALL remote package files to local mirror...")
        
        rsync_cmd = f"""
        rsync -avz \
          --progress \
          --stats \
          -e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=60" \
          '{self.vps_user}@{self.vps_host}:{self.remote_dir}/*.pkg.tar.*' \
          '{mirror_temp_dir}/' 2>/dev/null || true
        """
        
        logger.info(f"RUNNING RSYNC MIRROR COMMAND:")
        logger.info(rsync_cmd.strip())
        
        start_time = time.time()
        
        try:
            result = subprocess.run(
                rsync_cmd,
                shell=True,
                capture_output=True,
                text=True,
                check=False
            )
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            logger.info(f"EXIT CODE: {result.returncode}")
            if result.stdout:
                for line in result.stdout.splitlines()[-20:]:
                    if line.strip():
                        logger.info(f"RSYNC MIRROR: {line}")
            if result.stderr:
                for line in result.stderr.splitlines():
                    if line.strip() and "No such file or directory" not in line:
                        logger.error(f"RSYNC MIRROR ERR: {line}")
            
            # List downloaded files
            downloaded_files = list(mirror_temp_dir.glob("*.pkg.tar.*"))
            file_count = len(downloaded_files)
            
            if file_count > 0:
                logger.info(f"‚úÖ Successfully mirrored {file_count} package files ({duration} seconds)")
                logger.info(f"Sample mirrored files: {[f.name for f in downloaded_files[:5]]}")
                
                # Verify file integrity and copy to output directory
                valid_files = []
                for pkg_file in downloaded_files:
                    if pkg_file.stat().st_size > 0:
                        valid_files.append(pkg_file)
                    else:
                        logger.warning(f"‚ö†Ô∏è Empty file: {pkg_file.name}")
                
                logger.info(f"Valid mirrored packages: {len(valid_files)}/{file_count}")
                
                # Copy mirrored packages to output directory
                print(f"üìã Copying {len(valid_files)} mirrored packages to output directory...")
                copied_count = 0
                for pkg_file in valid_files:
                    dest = output_dir / pkg_file.name
                    if not dest.exists():  # Don't overwrite newly built packages
                        shutil.copy2(pkg_file, dest)
                        copied_count += 1
                
                logger.info(f"Copied {copied_count} mirrored packages to output directory")
                
                # Clean up mirror directory
                shutil.rmtree(mirror_temp_dir, ignore_errors=True)
                
                return True
            else:
                logger.info("‚ÑπÔ∏è No package files were mirrored (repository is empty or permission issue)")
                shutil.rmtree(mirror_temp_dir, ignore_errors=True)
                return True
                
        except Exception as e:
            logger.error(f"RSYNC mirror execution error: {e}")
            if mirror_temp_dir.exists():
                shutil.rmtree(mirror_temp_dir, ignore_errors=True)
            return False
    
    def upload_files(self, files_to_upload: List[str], output_dir: Path) -> bool:
        """
        Upload files to server using RSYNC WITHOUT --delete flag
        
        Returns:
            True if successful, False otherwise
        """
        # Ensure remote directory exists first
        # Note: This requires SSHClient, will be called from PackageBuilder
        
        if not files_to_upload:
            logger.warning("No files to upload")
            return False
        
        # Log files to upload (safe - only filenames, not paths)
        logger.info(f"Files to upload ({len(files_to_upload)}):")
        for f in files_to_upload:
            try:
                size_mb = os.path.getsize(f) / (1024 * 1024)
                filename = os.path.basename(f)
                file_type = "PACKAGE"
                if self.repo_name in filename:
                    file_type = "DATABASE" if not f.endswith('.sig') else "SIGNATURE"
                logger.info(f"  - {filename} ({size_mb:.1f}MB) [{file_type}]")
            except Exception:
                logger.info(f"  - {os.path.basename(f)} [UNKNOWN SIZE]")
        
        # Build RSYNC command WITHOUT --delete
        rsync_cmd = f"""
        rsync -avz \
          --progress \
          --stats \
          {" ".join(f"'{f}'" for f in files_to_upload)} \
          '{self.vps_user}@{self.vps_host}:{self.remote_dir}/'
        """
        
        logger.info(f"RUNNING RSYNC COMMAND WITHOUT --delete:")
        logger.info(rsync_cmd.strip())
        
        # FIRST ATTEMPT
        start_time = time.time()
        
        try:
            result = subprocess.run(
                rsync_cmd,
                shell=True,
                capture_output=True,
                text=True,
                check=False
            )
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            logger.info(f"EXIT CODE (attempt 1): {result.returncode}")
            if result.stdout:
                for line in result.stdout.splitlines():
                    if line.strip():
                        logger.info(f"RSYNC: {line}")
            if result.stderr:
                for line in result.stderr.splitlines():
                    if line.strip() and "No such file or directory" not in line:
                        logger.error(f"RSYNC ERR: {line}")
            
            if result.returncode == 0:
                logger.info(f"‚úÖ RSYNC upload successful! ({duration} seconds)")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è First RSYNC attempt failed (code: {result.returncode})")
                
        except Exception as e:
            logger.error(f"RSYNC execution error: {e}")
        
        # SECOND ATTEMPT (with different SSH options)
        logger.info("‚ö†Ô∏è Retrying with different SSH options...")
        time.sleep(5)
        
        rsync_cmd_retry = f"""
        rsync -avz \
          --progress \
          --stats \
          -e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=60 -o ServerAliveInterval=30 -o ServerAliveCountMax=3" \
          {" ".join(f"'{f}'" for f in files_to_upload)} \
          '{self.vps_user}@{self.vps_host}:{self.remote_dir}/'
        """
        
        logger.info(f"RUNNING RSYNC RETRY COMMAND WITHOUT --delete:")
        logger.info(rsync_cmd_retry.strip())
        
        start_time = time.time()
        
        try:
            result = subprocess.run(
                rsync_cmd_retry,
                shell=True,
                capture_output=True,
                text=True,
                check=False
            )
            
            end_time = time.time()
            duration = int(end_time - start_time)
            
            logger.info(f"EXIT CODE (attempt 2): {result.returncode}")
            if result.stdout:
                for line in result.stdout.splitlines():
                    if line.strip():
                        logger.info(f"RSYNC RETRY: {line}")
            if result.stderr:
                for line in result.stderr.splitlines():
                    if line.strip() and "No such file or directory" not in line:
                        logger.error(f"RSYNC RETRY ERR: {line}")
            
            if result.returncode == 0:
                logger.info(f"‚úÖ RSYNC upload successful on retry! ({duration} seconds)")
                return True
            else:
                logger.error(f"‚ùå RSYNC upload failed on both attempts!")
                return False
                
        except Exception as e:
            logger.error(f"RSYNC retry execution error: {e}")
            return False
--- FILE: .github/scripts/modules/vps/ssh_client.py ---
"""
SSH Client Module - Handles SSH connections and remote operations
"""

import os
import subprocess
import shutil
import logging
from pathlib import Path
from typing import List, Tuple, Optional

logger = logging.getLogger(__name__)


class SSHClient:
    """Handles SSH connections and remote VPS operations"""
    
    def __init__(self, config: dict):
        """
        Initialize SSHClient with configuration
        
        Args:
            config: Dictionary containing:
                - vps_user: VPS username
                - vps_host: VPS hostname
                - remote_dir: Remote directory on VPS
                - ssh_options: SSH options list
                - repo_name: Repository name
        """
        self.vps_user = config['vps_user']
        self.vps_host = config['vps_host']
        self.remote_dir = config['remote_dir']
        self.ssh_options = config.get('ssh_options', [])
        self.repo_name = config.get('repo_name', '')
        
    def setup_ssh_config(self, ssh_key: Optional[str] = None):
        """Setup SSH config file for builder user - container invariant"""
        ssh_dir = Path("/home/builder/.ssh")
        ssh_dir.mkdir(exist_ok=True, mode=0o700)
        
        # Write SSH config file using environment variables
        config_content = f"""Host {self.vps_host}
  HostName {self.vps_host}
  User {self.vps_user}
  IdentityFile ~/.ssh/id_ed25519
  StrictHostKeyChecking no
  ConnectTimeout 30
  ServerAliveInterval 15
  ServerAliveCountMax 3
"""
        
        config_file = ssh_dir / "config"
        with open(config_file, "w") as f:
            f.write(config_content)
        
        config_file.chmod(0o600)
        
        # Ensure SSH key exists and has correct permissions
        ssh_key_path = ssh_dir / "id_ed25519"
        if not ssh_key_path.exists() and ssh_key:
            with open(ssh_key_path, "w") as f:
                f.write(ssh_key)
            ssh_key_path.chmod(0o600)
        
        # Set ownership to builder
        try:
            shutil.chown(ssh_dir, "builder", "builder")
            for item in ssh_dir.iterdir():
                shutil.chown(item, "builder", "builder")
        except Exception as e:
            logger.warning(f"Could not change SSH dir ownership: {e}")
    
    def test_ssh_connection(self) -> bool:
        """Test SSH connection to VPS"""
        print("\nüîç Testing SSH connection to VPS...")
        
        ssh_test_cmd = [
            "ssh",
            f"{self.vps_user}@{self.vps_host}",
            "echo SSH_TEST_SUCCESS"
        ]
        
        result = subprocess.run(ssh_test_cmd, capture_output=True, text=True, check=False)
        if result and result.returncode == 0 and "SSH_TEST_SUCCESS" in result.stdout:
            print("‚úÖ SSH connection successful")
            return True
        else:
            print(f"‚ö†Ô∏è SSH connection failed: {result.stderr[:100] if result and result.stderr else 'No output'}")
            return False
    
    def ensure_remote_directory(self):
        """Ensure remote directory exists and has correct permissions"""
        print("\nüîß Ensuring remote directory exists...")
        
        remote_cmd = f"""
        # Check if directory exists
        if [ ! -d "{self.remote_dir}" ]; then
            echo "Creating directory {self.remote_dir}"
            sudo mkdir -p "{self.remote_dir}"
            sudo chown -R {self.vps_user}:www-data "{self.remote_dir}"
            sudo chmod -R 755 "{self.remote_dir}"
            echo "‚úÖ Directory created and permissions set"
        else
            echo "‚úÖ Directory exists"
            # Ensure correct permissions
            sudo chown -R {self.vps_user}:www-data "{self.remote_dir}"
            sudo chmod -R 755 "{self.remote_dir}"
            echo "‚úÖ Permissions verified"
        fi
        """
        
        ssh_cmd = ["ssh", *self.ssh_options, f"{self.vps_user}@{self.vps_host}", remote_cmd]
        
        try:
            result = subprocess.run(
                ssh_cmd,
                capture_output=True,
                text=True,
                check=False
            )
            
            if result.returncode == 0:
                logger.info("‚úÖ Remote directory verified")
                for line in result.stdout.splitlines():
                    if line.strip():
                        logger.info(f"REMOTE DIR: {line}")
            else:
                logger.warning(f"‚ö†Ô∏è Could not ensure remote directory: {result.stderr[:200]}")
                
        except Exception as e:
            logger.warning(f"Could not ensure remote directory: {e}")
    
    def check_repository_exists_on_vps(self) -> Tuple[bool, bool]:
        """Check if repository exists on VPS via SSH"""
        print("\nüîç Checking if repository exists on VPS...")
        
        remote_cmd = f"""
        # Check for package files
        if find "{self.remote_dir}" -name "*.pkg.tar.*" -type f 2>/dev/null | head -1 >/dev/null; then
            echo "REPO_EXISTS_WITH_PACKAGES"
        # Check for database files
        elif [ -f "{self.remote_dir}/{self.repo_name}.db.tar.gz" ] || [ -f "{self.remote_dir}/{self.repo_name}.db" ]; then
            echo "REPO_EXISTS_WITH_DB"
        else
            echo "REPO_NOT_FOUND"
        fi
        """
        
        ssh_cmd = ["ssh", f"{self.vps_user}@{self.vps_host}", remote_cmd]
        
        try:
            result = subprocess.run(
                ssh_cmd,
                capture_output=True,
                text=True,
                check=False,
                timeout=30
            )
            
            if result.returncode == 0:
                if "REPO_EXISTS_WITH_PACKAGES" in result.stdout:
                    logger.info("‚úÖ Repository exists on VPS (has package files)")
                    return True, True
                elif "REPO_EXISTS_WITH_DB" in result.stdout:
                    logger.info("‚úÖ Repository exists on VPS (has database)")
                    return True, False
                else:
                    logger.info("‚ÑπÔ∏è Repository does not exist on VPS (first run)")
                    return False, False
            else:
                logger.warning(f"‚ö†Ô∏è Could not check repository existence: {result.stderr[:200]}")
                return False, False
                
        except subprocess.TimeoutExpired:
            logger.error("‚ùå SSH timeout checking repository existence")
            return False, False
        except Exception as e:
            logger.error(f"‚ùå Error checking repository: {e}")
            return False, False
    
    def list_remote_packages(self) -> List[str]:
        """List all *.pkg.tar.zst files in the remote repository directory"""
        print("\n" + "=" * 60)
        print("STEP 1: Listing remote repository packages (SSH find)")
        print("=" * 60)
        
        ssh_key_path = "/home/builder/.ssh/id_ed25519"
        if not os.path.exists(ssh_key_path):
            logger.error(f"SSH key not found at {ssh_key_path}")
            return []
        
        ssh_cmd = [
            "ssh",
            f"{self.vps_user}@{self.vps_host}",
            f"find {self.remote_dir} -maxdepth 1 -type f \\( -name '*.pkg.tar.zst' -o -name '*.pkg.tar.xz' \\) 2>/dev/null || echo 'NO_FILES'"
        ]
        
        logger.info(f"RUNNING SSH COMMAND: {' '.join(ssh_cmd)}")
        
        try:
            result = subprocess.run(
                ssh_cmd,
                capture_output=True,
                text=True,
                check=False
            )
            
            logger.info(f"EXIT CODE: {result.returncode}")
            if result.stdout:
                logger.info(f"STDOUT (first 1000 chars): {result.stdout[:1000]}")
            if result.stderr:
                logger.info(f"STDERR: {result.stderr[:500]}")
            
            if result.returncode == 0:
                files = [f.strip() for f in result.stdout.split('\n') if f.strip() and f.strip() != 'NO_FILES']
                file_count = len(files)
                logger.info(f"‚úÖ SSH find returned {file_count} package files")
                if file_count > 0:
                    print(f"Sample files: {files[:5]}")
                else:
                    logger.info("‚ÑπÔ∏è No package files found on remote server")
                return files
            else:
                logger.warning(f"‚ö†Ô∏è SSH find returned error: {result.stderr[:200]}")
                return []
                
        except Exception as e:
            logger.error(f"SSH command failed: {e}")
            return []
--- FILE: .github/scripts/modules/__init__.py ---
"""
Package Builder Modules
"""
--- FILE: .github/scripts/config.py ---
"""
Configuration file for Manjaro Package Builder
"""

import os

# Repository configuration
REPO_DB_NAME = "manjaro-awesome"  # Default repository name
OUTPUT_DIR = "built_packages"     # Local output directory
BUILD_TRACKING_DIR = ".buildtracking"  # Build tracking directory

# PACKAGER identity from environment variable (secure via GitHub Secrets)
PACKAGER_ID = os.getenv("PACKAGER_ENV", "Maintainer <no-reply@gshoots.hu>")

# SSH and Git configuration
SSH_REPO_URL = "git@github.com:megvadulthangya/manjaro-awesome.git"
SSH_OPTIONS = [
    "-o", "StrictHostKeyChecking=no",
    "-o", "ConnectTimeout=30",
    "-o", "BatchMode=yes"
]

# Build timeouts (seconds)
MAKEPKG_TIMEOUT = {
    "default": 3600,        # 1 hour for normal packages
    "large_packages": 7200, # 2 hours for large packages (gtk, qt, chromium)
    "simplescreenrecorder": 5400,  # 1.5 hours
}

# Special dependency mappings
SPECIAL_DEPENDENCIES = {
    "gtk2": ["gtk-doc", "docbook-xsl", "libxslt", "gobject-introspection"],
    "awesome-git": ["lua", "lgi", "imagemagick", "asciidoc"],
    "awesome-freedesktop-git": ["lua", "lgi", "imagemagick", "asciidoc"],
    "lain-git": ["lua", "lgi", "imagemagick", "asciidoc"],
    "simplescreenrecorder": ["jack2"],  # Convert jack to jack2
}

# Build tool checks (will be installed if missing)
REQUIRED_BUILD_TOOLS = [
    "make", "gcc", "pkg-config", "autoconf", "automake", 
    "libtool", "cmake", "meson", "ninja", "patch"
]

# Temporary directories (runtime-required, /tmp is POSIX invariant)
MIRROR_TEMP_DIR = "/tmp/repo_mirror"
SYNC_CLONE_DIR = "/tmp/manjaro-awesome-gitclone"

# AUR configuration
AUR_URLS = [
    "https://aur.archlinux.org/{pkg_name}.git",
    "git://aur.archlinux.org/{pkg_name}.git"
]

# Build directory names
AUR_BUILD_DIR = "build_aur"

# GitHub repository for synchronization
GITHUB_REPO = "megvadulthangya/manjaro-awesome.git"

# Debug mode configuration - when True, bypass logger for critical build output
DEBUG_MODE = True

# VPS configuration (from environment variables)
VPS_USER = os.getenv("VPS_USER")
VPS_HOST = os.getenv("VPS_HOST")
VPS_SSH_KEY = os.getenv("VPS_SSH_KEY")
REMOTE_DIR = os.getenv("REMOTE_DIR")
REPO_NAME = os.getenv("REPO_NAME")
REPO_SERVER_URL = os.getenv("REPO_SERVER_URL")

# GPG configuration
GPG_KEY_ID = os.getenv("GPG_KEY_ID")
GPG_PRIVATE_KEY = os.getenv("GPG_PRIVATE_KEY")
--- FILE: .github/scripts/packages.py ---
"""
Package definitions for Manjaro Package Builder
"""

# LOCAL packages (from our repository)
LOCAL_PACKAGES = [
    "gghelper",
    "gtk2",
    "awesome-freedesktop-git",
    "lain-git",
    "awesome-rofi",
    "awesome-git",
    "awesome-welcome",
    "tilix-git",
    "nordic-backgrounds",
    "awesome-copycats-manjaro",
    "i3lock-fancy-git",
    "ttf-font-awesome-5",
    "nvidia-driver-assistant",
    "grayjay-bin"
]

# AUR packages (from Arch User Repository)
AUR_PACKAGES = [
    "libinput-gestures",
    "gtkd",
    "qt5-styleplugins",
    "urxvt-resize-font-git",
    "i3lock-color",
    "raw-thumbnailer",
    "gsconnect",
    "tamzen-font",
    "betterlockscreen",
    "nordic-theme",
    "nordic-darker-theme",
    "geany-nord-theme",
    "nordzy-icon-theme",
    "oh-my-posh-bin",
    "fish-done",
    "find-the-command",
    "p7zip-gui",
    "qownnotes",
    "xorg-font-utils",
    "xnviewmp",
    "simplescreenrecorder",
    "gtkhash-thunar",
    "a4tech-bloody-driver-git",
    "nordic-bluish-accent-theme",
    "nordic-bluish-accent-standard-buttons-theme",
    "nordic-polar-standard-buttons-theme",
    "nordic-standard-buttons-theme",
    "nordic-darker-standard-buttons-theme"
]

# Optionally, you can also define package groups or categories
PACKAGE_CATEGORIES = {
    "desktop": ["awesome-freedesktop-git", "lain-git", "awesome-rofi"],
    "themes": ["nordic-theme", "nordic-darker-theme", "nordic-bluish-accent-theme"],
    "fonts": ["tamzen-font", "ttf-font-awesome-5"],
    "tools": ["libinput-gestures", "betterlockscreen", "simplescreenrecorder"],
    "drivers": ["nvidia-driver-assistant", "a4tech-bloody-driver-git"]
}
--- FILE: .github/scripts/checker.py ---
"""
Package Checker Module - Validates packages and repository state
"""

import logging

logger = logging.getLogger(__name__)


class PackageChecker:
    """Validates packages and repository state"""
    
    def __init__(self):
        pass
    
    def check_package_integrity(self, package_path: str) -> bool:
        """Check if a package file is valid"""
        logger.info(f"Checking package integrity: {package_path}")
        # Implementation would go here
        return True
    
    def verify_repository_state(self, repo_path: str) -> bool:
        """Verify repository state and consistency"""
        logger.info(f"Verifying repository state: {repo_path}")
        # Implementation would go here
        return True
--- FILE: .github/scripts/builder.py ---
#!/usr/bin/env python3
"""
Manjaro Package Builder - Refactored Modular Architecture with Zero-Residue Policy
Main orchestrator that coordinates between modules
"""

print(">>> DEBUG: Script started")

import os
import sys

# Add the script directory to sys.path for imports
script_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, script_dir)

print(f">>> DEBUG: Script directory: {script_dir}")
print(f">>> DEBUG: sys.path: {sys.path}")

# Import our modules
try:
    # Adjust imports to work from the script directory
    from modules.orchestrator.package_builder import PackageBuilder
    MODULES_LOADED = True
    print(">>> DEBUG: PackageBuilder imported successfully")
except ImportError as e:
    print(f"‚ùå CRITICAL: Failed to import modules: {e}")
    print(f"‚ùå sys.path: {sys.path}")
    print(f"‚ùå Current directory: {os.getcwd()}")
    print(f"‚ùå Script directory: {script_dir}")
    MODULES_LOADED = False
    sys.exit(1)

if __name__ == "__main__":
    # Run the builder
    print(">>> DEBUG: Starting PackageBuilder...")
    sys.exit(PackageBuilder().run())